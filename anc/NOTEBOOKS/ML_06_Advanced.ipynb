{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Topics (...tying up some ends...)\n",
        "\n",
        "This unit discusses some useful-to-know concepts that round off the course."
      ],
      "metadata": {
        "id": "dIsvu7du8eE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extending Keras\n",
        "\n",
        "So far, we have encountered TensorFlow in two different guises:\n",
        "\n",
        " * As a RM-AD tool for GPU-enhanced computation of fast good-quality\n",
        "   gradients (such as: for optimization).\n",
        " * As a ML toolkit to quickly wire up some DNN architectures.\n",
        "\n",
        "What we have not seen is how these things fit together, specifically: how can we wrap up some of our own (possibly quite sophisticated) function designs as a Keras layer?\n",
        "\n",
        "Let us explore this by means of an example. We start with a simple convolutional architecture for CIFAR-10 one-out-of-10 image classification."
      ],
      "metadata": {
        "id": "o9Mw8GVR7OqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Let us load the CIFAR-10 dataset rather than MNIST\n",
        "# (10 classes, 32x32 pixels, RGB).\n",
        "# Details: http://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "(ds_train_raw, ds_validation_raw, ds_test_raw), ds_info = tfds.load(\n",
        "    'cifar10',\n",
        "    split=['train[:75%]', 'train[75%:]', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True)\n",
        "\n",
        "def normalize_image(image, label):\n",
        "  \"\"\"Normalizes images.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "\n",
        "ds_train = (\n",
        "    ds_train_raw\n",
        "    # Details: see https://www.tensorflow.org/guide/data_performance\n",
        "    .map(normalize_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .cache()\n",
        "    .shuffle(ds_info.splits['train'].num_examples)\n",
        "    .batch(32)\n",
        "    .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "ds_validation = (\n",
        "    ds_validation_raw\n",
        "    .map(normalize_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "ds_test = (\n",
        "    ds_test_raw\n",
        "    .map(normalize_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE))\n",
        "\n"
      ],
      "metadata": {
        "id": "gunHzeVk3WLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### The model.\n",
        "\n",
        "cnn_model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Input(shape=(32, 32, 3)),\n",
        "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(50, activation='relu'),\n",
        "        tf.keras.layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "cnn_model.fit(\n",
        "    ds_train,\n",
        "    epochs=25,\n",
        "    validation_data=ds_validation)\n",
        "\n"
      ],
      "metadata": {
        "id": "p0mbOzNh31SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen, modern DNN architectures tend to develop calibration problems more readily than earlier architectures.\n",
        "\n",
        "One plausible hypothesis here might be that earlier architectures used rather simple features weakly indicative of the target class, which then also can be regarded as reasonably independent - and we rarely would encounter a situation where we accumulated \"really strong evidence\" where the \"independency of (Bayesian) votes\" would have been violated badly. With DNNs, we can have a situation where hierarchical extraction of features might lead to \"tell-tale sign\" type of evidence which (by optimization) gets attributed some large logit-value(s), but is not independent of other such evidence, which we do however implicitly assume by summing.\n",
        "\n",
        "So, one could have the idea that, perhaps, large accumulated evidence (as input to softmax) should generally be distrusted, and the model is driven to use large evidence-contributions in some cases since, overall, this still improves predictions. So, how about introducing a layer that uniformly \"punishes\" large evidence before we feed it into softmax? Rather than trying to come up with a principled way to do this, let us here try an ad-hoc approach and attenuate all \"large evidence\" the same way with a nonlinear function that involves two parameters that are learnable - one determining a \"length scale\" below which behavior is mostly linear, and one for overall rescaling. We will go with the function `f(x) = A * asinh(B * x)`, with learnable `A` and `B`.\n",
        "\n",
        "Also, for the sake of this example, we will ignore the question how to perhaps achieve this via creative use of some existing Keras layers (specifically, a \"1D\" convolution across a hidden feature-vector with an $1\\times 1$ kernel and nonlinear activation). Rather, we want to implement our own layer.\n",
        "\n",
        "For this, we need to do a little bit of OO programming, which we so far did not discuss in this course - implementing a subclass of the `tf.keras.layers.Layer` class. This will have a tiny amount of state - just two parameters - but show all the relevant bits and pieces.\n",
        "\n",
        "Also mostly for illustration purposes, we give our layer an extra tuning parameter which allows selecting a nonlinearity, with (here) two possible choices.\n",
        "\n",
        "We mostly follow the [Google/Alphabet Python style guide](https://g3doc.corp.google.com/eng/doc/devguide/py/style/index.md?cl=head) for this code.\n"
      ],
      "metadata": {
        "id": "DfRHGmPL7jR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introducing a new type of Keras layer.\n",
        "# Base class documentation:\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\n",
        "\n",
        "\n",
        "# Here, we are moving the computational body to separate @tf.function-s.\n",
        "# For this particular example, the body is so simple that this makes\n",
        "# little sense - we could just have done the computation in-place.\n",
        "#\n",
        "# In general, it is useful to have nontrivial transformations which we\n",
        "# put into Keras layers also available directly as @tf.function functions\n",
        "# that do not depend on Keras. A typical design would then use one module\n",
        "# with such @tf.function definitions that can be used independently, plus\n",
        "# a Keras wrapper that defines a layer using these functions on top of that.\n",
        "\n",
        "@tf.function\n",
        "def squash_evidence_asinh(t_evidence, t_param_a, t_param_b):\n",
        "  \"\"\"Squashes evidence `E` to `a * asinh(b * E)`.\"\"\"\n",
        "  return t_param_a * tf.math.asinh(t_param_b * t_evidence)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def squash_evidence_atan(t_evidence, t_param_a, t_param_b):\n",
        "  \"\"\"Squashes evidence `E` to `a * atan(b * E)`.\"\"\"\n",
        "  return t_param_a * tf.math.atan(t_param_b * t_evidence)\n",
        "\n",
        "\n",
        "class EvidenceTweakingLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Layer for uniform nonlinear total-evidence-adjustment.\n",
        "\n",
        "  Based on the hypothesis that seeing large accumulated evidence may generally\n",
        "  have violated the Bayesian \"independence\" assumption beyond what we would\n",
        "  be comfortable with.\n",
        "  \"\"\"\n",
        "\n",
        "  # Class attributes.\n",
        "  _NONLINEARITY_BY_TAG = dict(asinh=squash_evidence_asinh,\n",
        "                              atan=squash_evidence_atan)\n",
        "\n",
        "  def __init__(self, *, nonlinearity='asinh', **kwargs):\n",
        "    \"\"\"Initializes the instance.\"\"\"\n",
        "    # Forward parent-class keyword args to parent-class __init__, so that\n",
        "    # `name=...` etc. args work as for a generic Keras `Layer`.\n",
        "    super().__init__(**kwargs)\n",
        "    nonlinearity_func = self._NONLINEARITY_BY_TAG.get(nonlinearity)\n",
        "    if nonlinearity_func is None:\n",
        "      raise ValueError(f'Unknown nonlinearity: {nonlinearity!r} - '\n",
        "                       f'known: {set(self._NONLINEARITY_BY_TAG)!r}')\n",
        "    # We need to store the __init__() parameters for .get_config(), so that\n",
        "    # serialization and deserialization of the layer works.\n",
        "    config = dict(nonlinearity=nonlinearity)\n",
        "    config.update(**kwargs)\n",
        "    self._config = config\n",
        "    self._nonlinearity_func = nonlinearity_func\n",
        "    # It is generally good practice to make sure that inspection of the\n",
        "    # __init__ method's body gives clarity about all (public and private)\n",
        "    # instance attributes.\n",
        "    self._param_ab = None\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"Sets up layer-state.\"\"\"\n",
        "    del input_shape  # Unused by this layer.\n",
        "    self._param_ab = self.add_weight(shape=(2,),\n",
        "                                     initializer='random_normal')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Evaluates the layer.\"\"\"\n",
        "    # Note that `inputs` may in general have batch-indices. The code in this\n",
        "    # method needs to be able to handle data in such a form.\n",
        "    # Here, the calculation is rather simple.\n",
        "    return self._nonlinearity_func(inputs, self._param_ab[0], self._param_ab[1])\n",
        "\n",
        "  def get_config(self):\n",
        "    \"\"\"Returns the layer's configuration.\"\"\"\n",
        "    # This method is important to ensure that saving and loading a layer\n",
        "    # (such as: as part of a trained model) can re-create the given layer\n",
        "    # in the expected form. Here, we need this since we have\n",
        "    # instantiation-time tweaking parameters.\n",
        "    #\n",
        "    # The default .from_config() classmethod works for our use case,\n",
        "    # since all our config is JSON-serializable.\n",
        "    return self._config\n"
      ],
      "metadata": {
        "id": "b9fs_hTlAC2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see if adding our new layer improves things."
      ],
      "metadata": {
        "id": "wf3bup79GGwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model_tweaked = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Input(shape=(32, 32, 3)),\n",
        "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(50, activation='relu'),\n",
        "        tf.keras.layers.Dense(10),\n",
        "        EvidenceTweakingLayer(nonlinearity='asinh'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cnn_model_tweaked.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "cnn_model_tweaked.fit(\n",
        "    ds_train,\n",
        "    epochs=25,\n",
        "    validation_data=ds_validation)\n",
        "\n"
      ],
      "metadata": {
        "id": "hNGecZX6GJom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try saving and loading our model."
      ],
      "metadata": {
        "id": "5eb4zEr_HDc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model_tweaked.save('cnn_model_tweaked.h5')\n",
        "reloaded_cnn_model_tweaked = tf.keras.models.load_model(\n",
        "    'cnn_model_tweaked.h5',\n",
        "    custom_objects={'EvidenceTweakingLayer': EvidenceTweakingLayer})\n",
        "\n",
        "\n",
        "print('Test set accuracy (orig): '\n",
        "      f'{cnn_model_tweaked.evaluate(ds_test)[1] * 100:.2f}%')\n",
        "print('Test set accuracy (reloaded): '\n",
        "      f'{reloaded_cnn_model_tweaked.evaluate(ds_test)[1] * 100:.2f}%')"
      ],
      "metadata": {
        "id": "5mDrtnwKD2QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e427cc3a-6314-416b-8402-62768c1d446f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 1.1486 - sparse_categorical_accuracy: 0.6017\n",
            "Test set accuracy (orig): 60.17%\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1486 - sparse_categorical_accuracy: 0.6017\n",
            "Test set accuracy (reloaded): 60.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the classifier performance impact of this little idea was unconvincing - but this is often how things turn out when exploring ideas. Two things matter:\n",
        "\n",
        "1. Having developed some useful intuition about what might work\n",
        "   (via experimenting).\n",
        "1. Being able to quickly try out ideas with little effort.\n"
      ],
      "metadata": {
        "id": "WnCzLhcFIHPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow and JAX\n",
        "\n",
        "TensorFlow is Google's ML flagship library. This however does not mean that everybody at Google would do ML exclusively with TensorFlow.\n",
        "\n",
        "One particularly interesting \"not officially supported, currently-under-research\" tool which Google also open sourced is [JAX](https://github.com/google/jax).\n",
        "\n",
        "Tony Hoare is claimed to have said that \"inside every large program is a small program struggling to get out\", and JAX can be thought of as being such a \"small program\": On its github page, it describes itself as \"Autograd and XLA\" (where \"[XLA](https://www.tensorflow.org/xla)\" is the \"accelerated linear algebra\" also powering TensorFlow). JAX looks a lot like \"numpy with automatic differentiation capabilities\".\n",
        "\n",
        "The Google colab kernel has JAX pre-installed, so let us explore it a bit."
      ],
      "metadata": {
        "id": "LzdzR-ysIq3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import numpy as jnp\n",
        "\n",
        "\n",
        "def box_volume(sides):\n",
        "  return jnp.prod(sides)\n",
        "\n",
        "print(box_volume(jnp.array([2, 3, 4, 5], dtype=jnp.float32)))\n",
        "\n",
        "grad_vol = jax.jit(jax.grad(box_volume))\n",
        "\n",
        "print('Grad(volume) at [2, 4, 8]:',\n",
        "      grad_vol(jnp.array([2, 4, 8], dtype=jnp.float32)))"
      ],
      "metadata": {
        "id": "0OWixb7KLBTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237856ec-bf1b-42b0-979f-e7a8c2ced80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120.0\n",
            "Grad(volume) at [2, 4, 8]: [32. 16.  8.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this certainly looks useful, please note the JAX documentation (at the time of this writing) says:\n",
        "\n",
        "```\n",
        "This is a research project, not an official Google product.\n",
        "Expect bugs and sharp edges.\n",
        "Please help by trying it out, reporting bugs, and letting us know what you think!\n",
        "```"
      ],
      "metadata": {
        "id": "JLlb7T97Miwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap\n",
        "\n",
        "1. We looked into the structure of the Python language.\n",
        "\n",
        "   * This was a lot of material.\n",
        "   * Further on, we used nested functions a lot, used numpy\n",
        "     vectorization left and right, and having a good model\n",
        "     of evaluation semantics was relevant for\n",
        "     understanding how `@tf.function` sees code differently.\n",
        "\n",
        "1. We looked into the basic principles underlying fast gradients - and their use in numerical optimization.\n",
        "\n",
        "   * This is the basis for many ML and ML-related frameworks\n",
        "     (other than TensorFlow and JAX).\n",
        "   * This also explains the design limitations one would face\n",
        "     when trying to implement similar such ML-supporting\n",
        "     infrastructure on top of a framework such as Matlab,\n",
        "     Octave, Mathematica, Maple, etc.\n",
        "   * ML frameworks have their limitations, such as having\n",
        "     little reason to support better-than-float64 numerical\n",
        "     accuracy. So, when we cannot use them, it is good to see how\n",
        "     we can do this on our own. Also, a solid understanding\n",
        "     of sensitivity backpropagation opens up new perspectives\n",
        "     on other ideas, including Hamiltonian mechanics.\n",
        "   * We have seen that \"solving non-malicious 1000-parameter\n",
        "     optimization problems numerically\" is generally \"easy\"\n",
        "     these days.\n",
        "\n",
        "1. We formulated a first ML problem (\"digit-8-recognizer\") purely\n",
        "   as a high-dimensional optimization problem, and from there went\n",
        "   on to ML.\n",
        "\n",
        "   * Mental model of supervised ML: \"infinite-examples limit\n",
        "     of k-Nearest-Neighbors classifiers\".\n",
        "   * Major concepts we encountered: \"Estimating gradients on\n",
        "     batches\" (\"stochastic gradient descent\"), some\n",
        "     \"standard constructions\" (loss functions, softmax, embeddings),\n",
        "     generalization-performance-enhancing tricks (early stopping,\n",
        "     L2 regularization, dropout).\n",
        "   * We looked into (often entropy-based) explanations of the design\n",
        "     of some standard constructions.\n",
        "   * We discussed what generally happens if inference is done\n",
        "     on examples drawn on a different distribution than\n",
        "     the one used to get the training set.\n",
        "   * We briefly discussed \"vanishing gradients\" and how\n",
        "     information propagation in a DNN can be seen as a\n",
        "     percolation problem (via MFA).\n",
        "   * We observed that TensorFlow can indeed make our life\n",
        "     much easier as long as we are within the confines of\n",
        "     what we can build with commonly used \"lego bricks\".\n",
        "\n",
        "1. We took a deeper look at TensorFlow\n",
        "   * We explored a bit of the inner mechanics.\n",
        "   * We saw how to use it to conveniently formulate high\n",
        "     dimensional optimization problems in a device-agnostic way\n",
        "     (and do physics with that).\n",
        "   * We saw how to wrap up \"unusual\" computations to make\n",
        "     them available as Keras layers.\n",
        "\n",
        "[Author's note: Overall, I may occasionally have talked a bit of nonsense, but this was less than 10% of the time, and physics course material rarely is right more than 90% of the time anyhow. This was the 1st iteration of teaching this material. We might want to refine this for future iterations.\n",
        "\n",
        "Thanks for participating. And a standing offer from my side: If you need help wiring something up with TensorFlow, you can always ask me via email to take a look. Getting some initial help when starting on some own physics project may be very useful.]\n"
      ],
      "metadata": {
        "id": "ps0wZ1cmef8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addendum: Connecting Mathematica and TensorFlow\n",
        "\n",
        "Given the large popularity of symbolic algebra packages such as in particular Mathematica in theoretical physics, it makes sense to address some obvious questions around ML and Mathematica.\n",
        "\n",
        "As we have seen, the changes required to make a major programming language support backpropagation go rather deep, and with both TensorFlow and JAX, there are still some sharp edges that are related to the attempt to blend object-language (for tensor-arithmetic computations - in TensorFlow1, these enter the scene very explicitly as computational graphs) and meta-language (for setting up and manipulating object-language entities - so, Python). Design-wise, it is perhaps debateable if this is the best possible design approach. While a clearer separation between these roles would be conceptually more elegant, there is also a large human element here. In any case, it is clear that bringing backpropagation to any major language is most feasible if the language is designed with simplicity and minimality in mind (such as the Lisp dialect Scheme, where this has indeed been accomplished), or started out with the design idea of properly supporting backpropagation. Retrofitting this into some existent design of a large language is hard. Also, compiling linear algebra to machine code that can execute fast on a range of very different hardware architectures (CPUs and also GPUs) requires major effort to build.\n",
        "\n",
        "As such, it is not clear when - or even if at all - data manipulation packages that are popular with physicists will implement advanced numerical tensor backpropagation capabilities roughly on par with TensorFlow or JAX. Clearly, major symbolic algebra packages these days do support some form of ML - but typically not in the general-purpose way that would allow us to design and shape rather freely what our models look like. Still, one clearly would want to be able to *utilize* what such advanced capabilities have to offer in a setting where one would not want the ML library to dictate the programming language to use. This raises the question: if we might not get backpropagation with fast compiled linear algebra in the near future in popular symbolic algebra packages, is there perhaps at least a way to utilize such functionality in such a way that we can still use these packages as we are used to, but have them delegate parts of a problem to some other code behind-the-scenes? So, to the user of a Mathematica notebook, it looks as if there simply were a few functions to perform specialized data analysis, but behind the scenes, mostly invisible to the user, these functions exchange data with some other component running TensorFlow code.\n",
        "\n",
        "This is indeed feasible - and we will look in detail into one concrete possible realization here. Before we do that, we should briefly ponder the solution landscape. There are efforts to introduce file format standards for serialized computation graphs, and modern versions of Mathematica have experimental support for loading and then running inference with models saved in the [ONNX format](https://onnx.ai/about.html) - we produced such a serialized model earlier for classifying MNIST digits. Since this file format is rather new and new versions are currently introduced in short succession that expand the set of supported basic numerical operators, it is quite possible that the dust has to settle a bit first before this file format can be used widely without hassle. If this is available, this might be the best option.\n",
        "\n",
        "More generally, symbolic algebra packages, like just about every programming language, usually come with a Foreign Function Interface (FFI) that allows code authors to have the runtime kernel utilize other libraries - perhaps call functions from C or Fortran libraries. In principle, it might be possible to integrate TensorFlow(-Lite) into Mathematica at this level. The advantage would be very efficient data exchange between these components. However, given that both projects are large and complex and have their own unique approaches to handling some deep technical problems, it may well be that this causes major friction. Another idea might be to go for less tight coupling and have Mathematica exchange data with another process (perhaps on the same machine) that runs TensorFlow. In Mathematica, connecting to an external process either for one-off or session-based evaluation is supported via the [External Interpreted Languages Interfaces](https://reference.wolfram.com/language/guide/ExternalInterpretedLanguageInterfaces.html). Using this - or also a more bare-bones approach such as one based on Mathematica's [RunThrough](https://reference.wolfram.com/language/ref/RunThrough.html) command - would typically require first serializing data into some textual form, and then communicating requests and responses back-and-forth. Overall, this requires both data-conversion and interprocess communication, so is perhaps less efficient and also more brittle (with two running processes) than a more tightly integrated solution, but also - in principle - would be much easier to build.\n",
        "\n",
        "Since setting up a ML model is major computational effort that typically then is amortized over all the queries to that model, it makes little sense to use an approach where every query would start a new process and set up the model from saved form first - we want to go with a background server that loads the model at start time and then can be queried as needed.\n",
        "\n",
        "A general possible concern when starting processes from Mathematica, such as via `RunThrough[]`, is that these processes generally would inherit the Mathematica kernel's process environment, which may well have adjustments to e.g. `LD_LIBRARY_PATH` (on Unix systems), making launched processes by default use a very different collection of common libraries (such as for example `libz.so`) than the system-default - and this might clash with the needs libraries needed by other components in such a set-up. Here, an extra step (such as one more indirection) would likely be needed to switch over to using a less customized library environment.\n",
        "\n",
        "One basic fallback approach that should nowadays always be available, irrespective of which symbolic algebra system (and version) one uses, is to wire up a simple web server that allows querying ML models via HTTP requests. This is spelled out in detail in the following - and may be a useful basis for trying very similar approaches for other combinations than Mathematica and TFLite."
      ],
      "metadata": {
        "id": "ao4x0szboUJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Having Mathematica use an external HTTP server to access ML capabilities\n",
        "\n",
        "Let us look into a basic solution for having Mathematica handle ML tasks via calling a dedicated function that delegates work to a webserver, via a HTTP request. Here, we want to consider running the webserver and the Mathematica process(es) on the same computer, which we assume to be a Unix system - such as a researcher's personal workstation. We would still want to have at least some protection against unrelated users simply accessing the webserver and submitting their own requests without any form of authorization. If the computer is the researcher's private machine, no one else can log in to it, and also all programs running on that computer can be trusted, a lightweight approach would be to simply bind the webserver port only to the loopback interface - making it unreachable from the wider network. If there are other assumed-unprivileged users (i.e. users who cannot read all files or sniff network traffic), they still could in principle access the web server socket (since web services in general cannot be run over Unix domain sockets, only TCP sockets). Here, a very simple authentication mechanism that is similar to what is used by the X Window System might make sense. The idea is that every request needs to include a \"secret\" which can only be learned by accessing a file on the filesystem. This way, filesystem-based access control can be transmuted into access control for services offered by a server to which every user on the local machine can connect. For X11, the `MIT-MAGIC-COOKIE-1` mechanism shall be our guideline - whatever program can read the `$HOME/.XAuthority` file (or whatever the `XAUTHORITY` environment variable has been set to) and extract the secret within can access and interact with the X display of the logged in user.\n",
        "\n",
        "We will do something very similar with our own secrets-file:\n",
        "We want our webserver to write a secret to an agreed-upon location in the user's home directory at start up time, and the Mathematica kernel (which needs to be started afterwards) read that secret file and communicate the secret alongside every HTTP request to our \"ML Server\". For actually exchanging data, we cannot use the `HTTP GET` method for multiple reasons, an important one being URL length restrictions. Instead, we want to use `HTTP POST` requests that then transport a payload. Let us implement a bare-bones server on the basis of Python's `http` module. We want this server to - at start up time - load one (or multiple) ML models and use them to process requests, differentiating models by URL. The following code-cell is not intended to be run in a colab notebook, but is a copy-pasteable complete executable basic web server implemented in Python that can use `tflite`. We want to serve the `MNIST` tflite model we trained earlier on this course. The code comments indicate some little tricks here and there that were required to make this work despite some software packages having minor issues.\n"
      ],
      "metadata": {
        "id": "wMX7PT2YxoYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3.10\n",
        "\n",
        "import ast\n",
        "import base64\n",
        "import contextlib\n",
        "import http.server\n",
        "import threading\n",
        "\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Hack: at the time of this writing, tflite was only available for\n",
        "# Python3.10, which however could not use Python3.11's numpy, so I did a:\n",
        "# python3.10 -m pip install numpy -t ~/TensorFlow-ML/python3.10/site-packages\n",
        "# ...and we adjust the path here.\n",
        "# Note: This is for demo purposes only, and an unsound technique.\n",
        "# Done properly, this would use a Python \"virtual environment\" (venv).\n",
        "sys.path.insert(\n",
        "    0,\n",
        "    os.path.join(os.getenv('HOME'),\n",
        "                 'TensorFlow-ML/python3.10/site-packages'))\n",
        "\n",
        "import numpy\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "\n",
        "class ServerError(Exception):\n",
        "  \"\"\"Generic ML HTTP Server error.\"\"\"\n",
        "\n",
        "\n",
        "# This is somewhat slow and perhaps overkill for simply-structured data.\n",
        "def parse_mathematica(data):\n",
        "  \"\"\"Parse mathematica via sympy.parsing.\"\"\"\n",
        "  # These imports only do heavy lifting upon first evaluation of the body;\n",
        "  # subsequent evaluations (which however we do not have here) could\n",
        "  # re-use this. If we use the below \"fast\" alternative, this also\n",
        "  # avoids the `sympy` dependency.\n",
        "  import sympy\n",
        "  from sympy.parsing import mathematica\n",
        "  return sympy.parsing.mathematica.parse_mathematica(data)\n",
        "\n",
        "\n",
        "def parse_mathematica_simple_fast(data):\n",
        "  \"\"\"Ad-hoc parse simple Mathematica data (fast, lightweight).\"\"\"\n",
        "  return ast.literal_eval(data.translate(str.maketrans('{}', '[]', '\\n\\r')))\n",
        "\n",
        "\n",
        "def get_mnist_tflite_predictor(model_path):\n",
        "  interpreter = tflite.Interpreter(model_path=model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  interpreter_lock = threading.Lock()\n",
        "  #\n",
        "  def fn_predict(in_data, verbose=False):\n",
        "    # Note that this is setting interpreter-state, making\n",
        "    # this function non-reentrant unless we ensure that\n",
        "    # interpreter.invoke() calls cannot get mangled by\n",
        "    # interwoven concurrent set-up / read-out operations.\n",
        "    # This would matter a lot if we were to use e.g.\n",
        "    # a http.server.ThreadingHTTPServer - but let's make this\n",
        "    # robust.\n",
        "    try:\n",
        "      interpreter_lock.acquire()\n",
        "      # Here, we are extra-permissive:\n",
        "      # Any numerical data matrix/vector that comes in gets zero-padded\n",
        "      # to at least 28x28 elements, then trimmed to 28x28 elements,\n",
        "      # then reshaped. This allows us to easily hand-feed data such as\n",
        "      # {1,2,3} for debugging. No harm in trying to predict\n",
        "      # from bad-size data here.\n",
        "      in_array = numpy.pad(\n",
        "          numpy.asarray(in_data, dtype=numpy.float32).ravel(),\n",
        "          ((0, 28*28),))[:28*28].reshape(1, 28, 28)\n",
        "      interpreter.set_tensor(\n",
        "          input_details[0]['index'], in_array)\n",
        "      interpreter.invoke()\n",
        "      output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "      return output_data[0, ...].tolist()\n",
        "    finally:\n",
        "      interpreter_lock.release()\n",
        "  #\n",
        "  return fn_predict\n",
        "\n",
        "\n",
        "class ML_HTTPServer(http.server.HTTPServer):\n",
        "  \"\"\"Server subclass that has access to a ML model.\n",
        "\n",
        "  Attributes:\n",
        "    (base class attributes plus...):\n",
        "    ml_fn_predict_by_urlpath: Mapping of url-path to predictor-function,\n",
        "      as documented in `__init__`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, server_address, request_handler_class, *,\n",
        "               fn_predict_by_urlpath=(),\n",
        "               token_file='.ml_server_token'):\n",
        "    \"\"\"Initializes the instance.\n",
        "\n",
        "    Args:\n",
        "      server_address: The server address, passed on to base class `__init__`.\n",
        "      request_handler_class: The request handler class, passed on to\n",
        "        base class `__init__`.\n",
        "      fn_predict_by_urlpath: data which when passed to `dict()` produces\n",
        "        a key-value dictionary that maps a URL-path key to a predictor-function;\n",
        "        Each predictor function is expected to map a single numpy.ndarray-like\n",
        "        ML-data argument to numerical output data.\n",
        "      token_file: Path (implicitly-relative to web-user's $HOME env-var)\n",
        "        to a file where upon webserver startup the server writes a random\n",
        "        secret. POST web requests must include the secret as the 1st line of\n",
        "        the payload, proving that the requestor could read the token-file.\n",
        "        This loosely resembles MIT-MAGIC-COOKIE-1 X Window Authorization and\n",
        "        protects against independent users on the same machine (who see the\n",
        "        TCP port bound to the loopback interface) connecting to the webserver\n",
        "        and issuing their own requests. Does not protect against local\n",
        "        users with elevated privileges, such as root access to files or\n",
        "        packet sniffing. If web requests are to be routed from a different\n",
        "        machine, TLS encryption should be used additionally.\n",
        "    \"\"\"\n",
        "    # Secret handling goes first. Overall, this is a primitive mechanism\n",
        "    # that one might want to replace with something more advanced,\n",
        "    # such as HMAC, but the client also needs to support this.\n",
        "    secret = base64.b64encode(os.getrandom(32)).rstrip(b'=')\n",
        "    token_path = os.path.join(os.getenv('HOME'), token_file)\n",
        "    with open(token_path, 'wb') as h_token:\n",
        "      os.fchmod(h_token.fileno(), 0o700)\n",
        "      h_token.write(secret)\n",
        "    # Check if writing the secret succeeded.\n",
        "    with open(token_path, 'rb') as h_token_re:\n",
        "      re_secret = h_token_re.read()\n",
        "    if not re_secret == secret:\n",
        "      raise ServerError(\n",
        "          'Could not align on-filesystem secret '\n",
        "          f'with internal secret - path: {token_path}')\n",
        "    # Initialize base class instance. We have not yet touched instance-state.\n",
        "    super().__init__(server_address, request_handler_class)\n",
        "    self.ml_fn_predict_by_urlpath = dict(fn_predict_by_urlpath)\n",
        "    self._secret = secret\n",
        "\n",
        "  def check_secret(self, user_provided):\n",
        "    \"\"\"Checks if the user-provided string equals the secret.\"\"\"\n",
        "    return self._secret == user_provided\n",
        "\n",
        "\n",
        "class ML_HTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n",
        "  \"\"\"HTTP Request handler that delegates POST to specialist functions.\n",
        "\n",
        "  Also performs secret-checking on the server.\n",
        "  \"\"\"\n",
        "\n",
        "  def _send_text_plain_response(self, status_code, response):\n",
        "    self.send_response(status_code)\n",
        "    self.send_header('Content-Type', 'text/plain')\n",
        "    self.send_header('Content-Length', str(len(response)))\n",
        "    self.end_headers()\n",
        "    self.wfile.write(response)\n",
        "\n",
        "  def do_POST(self):\n",
        "    req_urlpath = self.path\n",
        "    content_length = int(self.headers.get('content-length', 0))\n",
        "    request_data = self.rfile.read(content_length)\n",
        "    try:\n",
        "      # If any of these operations fail, such as due to `req_urlpath`\n",
        "      # not being in the mapping, the payload not having a b'\\n', etc.,\n",
        "      # the violation of the implicit code-expectation raises an exception,\n",
        "      # and the `except`-section just returns a `Not Implemented` response.\n",
        "      user_provided_secret, payload = request_data.split(b'\\n', 1)\n",
        "      if not self.server.check_secret(user_provided_secret):\n",
        "          # Handled right below.\n",
        "          # We provide a message since we stderr-output text.\n",
        "          raise ServerError('Bad secret.')\n",
        "      parsed_payload = parse_mathematica_simple_fast(payload.decode('utf-8'))\n",
        "      fn_predict = self.server.ml_fn_predict_by_urlpath[req_urlpath]\n",
        "      prediction = fn_predict(parsed_payload)\n",
        "      response = repr(prediction).translate(\n",
        "          str.maketrans('[]', '{}')).encode('utf-8')\n",
        "      self._send_text_plain_response(http.HTTPStatus.OK, response)\n",
        "    except Exception as exn:\n",
        "      print('ERROR:', repr(exn), file=sys.stderr)\n",
        "      self._send_text_plain_response(http.HTTPStatus.NOT_IMPLEMENTED,\n",
        "                                     b'501 - Not Implemented')\n",
        "\n",
        "\n",
        "def run_server(server_address=('localhost', 8000),\n",
        "               mnist_model='mnist_model.tflite'):\n",
        "  fn_predict_mnist = get_mnist_tflite_predictor(mnist_model)\n",
        "  httpd = ML_HTTPServer(server_address,\n",
        "                        ML_HTTPRequestHandler,\n",
        "                        fn_predict_by_urlpath={\n",
        "                            '/mnist': fn_predict_mnist,\n",
        "                            })\n",
        "  httpd.serve_forever()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  run_server()\n"
      ],
      "metadata": {
        "id": "BdyVOYGj1NgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we start this webserver, the following Mathematica code illustrates how to then wrap up delegation of a ML inference task to this external server.\n",
        "\n",
        "```\n",
        "(* Server URL Authentication Secret *)\n",
        "mlServerURL := \"http://localhost:8000/mnist\";\n",
        "mlServerTokenPath:=FileNameJoin[{$HomeDirectory,\".ml_server_token\"}];\n",
        "mlServerAuthSecret = Import[mlServerTokenPath, \"Text\"];\n",
        "mlServerAuthSecretLength := {\"Secret Length\", StringLength[mlServerAuthSecret]};\n",
        "Print[mlServerAuthSecretLength];\n",
        "\n",
        "\n",
        "(* Running external classification *)\n",
        "TFClassifyMNIST[numdata_]:=Module[{body, result},\n",
        "body=mlServerAuthSecret<>\"\\n\"<>ExportString[numdata,\"String\"];\n",
        "result=URLFetch[mlServerURL,\"Method\"->\"POST\",\"Body\"->body];\n",
        "result]\n",
        "\n",
        "\n",
        "(* Demo - Example Input *)\n",
        "\n",
        "(* For illustration: Create 28x28 input from 7x7 text. *)\n",
        "\n",
        "as28x28[textImage7x7_]:=KroneckerProduct[\n",
        "ToExpression[#/.{\"#\"->1.0,\".\"->0.0}]&/@Characters[textImage7x7],\n",
        "ConstantArray[1, {4,4}]]\n",
        "\n",
        "demoDigit := as28x28[{\n",
        "\".......\",\n",
        "\"..####.\",\n",
        "\"..#..#.\",\n",
        "\"..####.\",\n",
        "\"..#..#.\",\n",
        "\"..####.\",\n",
        "\".......\"}]\n",
        "\n",
        "(* Running Classification *)\n",
        "\n",
        "MLResponse:=TFClassifyMNIST[demoDigit];\n",
        "Print[MLResponse]\n",
        "(* Produces:\n",
        "{-0.12768815457820892, -3.9151949882507324,\n",
        " -1.174180269241333, -2.3871653079986572,\n",
        " 0.34610337018966675, 3.6387715339660645,\n",
        " 2.232551097869873, -4.550597667694092,\n",
        " 4.761049270629883, 0.7428076267242432}\n",
        "*)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "s2uSZdw02KI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This basic template can (perhaps with a bit of help from a Unix wizard) be adjusted to other, similar tasks."
      ],
      "metadata": {
        "id": "SaSjiNJK25Rf"
      }
    }
  ]
}