\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{ML\_01\_Python}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
\begin{titlepage}

\begin{center}
{\bf \Large An Introduction to Machine Learning and ML tool basics for Physicists}

\vskip6ex

{\bf Thomas Fischbacher   \\ }
\bigskip
Google Research\\
Brandschenkestrasse 110, 8002 Z\"urich, Switzerland
\vskip 5mm
\bigskip
\tt{tfish@google.com} \\
\end{center}

\bigskip
\bigskip

\begin{abstract}

\noindent
These lecture notes contain of a set of Interactive Python (Google
Colab) notebooks that were the basis for a one-week ML course at the
Albert Einstein Institute in Potsdam, taught by the author in June
2022. They give a hands-on introduction to not only Machine Learning
Fundamentals and underlying concepts and ideas (in some places
presented in a form that aligns with concepts known to theoretical
physicists that many machine learning practitioners would not be
familiar with), but also explain bottom-up some of the design goals of
modern machine learning frameworks, and how these can be harnessed to
solve a broad range of numerical problems that readily arise in
theoretical physics - even when not employing any "learning" per
se. This point is illustrated with examples such as a basic general
relativity ray tracer implemented on top of Google's TensorFlow. The
notebooks have been typeset with some minor expansions, fixes, and
modernizations. It is expected that they might be useful both for
self-study and also as a source of material for educators who want to
design similar courses.

\end{abstract}

\vfill

\end{titlepage}

\tableofcontents

\hypertarget{preface}{%
\section{Preface}\label{preface}}

These lecture notes give an introduction to Machine Learning
(henceforth, ``ML'') basics that can be conveniently covered in a
one-week compact course for a target audience of participants with some
background in physics. It was first taught in June 2022 (virtually) at
the Albert Einstein Institute in Potsdam, as part of the International
Max Planck Research School, and video recordings of the lectures are
available on the AEI web page. This document mostly consists of the
notebooks that were presented during the course, with some fixes,
amendments, and extensions (and this origin explains the somewhat
messed-up structure of the table-of-contents). The arXiv web page for
these lecture notes also have these notebooks as standalone
\texttt{.ipynb} interactive Python notebook documents, which interested
readers can download and study - and in case they have a
\texttt{@gmail.com} email account - hence a Google account - upload to
their Google Drive (\url{http://drive.google.com/}) and then execute as
Google Colab notebooks. This approach gives a convenient way to learn
about Machine Learning without having to go through the motions of
installing any software locally - since all code will run on a virtual
machine back-end in the Google cloud that has the relevant ML libraries
installed.

Given that there are many ML tutorials available nowadays, the question
seem valid why to introduce yet another one. The two primary reasons
behind this are that, first, there are some fundamental constructions in
ML that are interesting to discuss but much easier to explain to
participants with some background on differential equations, and second,
that the big public attention focus on the current ML megatrend can lead
to overlooking major gold veins in the mine. In particular, just as ML
is benefitting a lot from the computer gaming industry having made
powerful numerics hardware very affordable, physics can benefit a lot
from not only ML itself, but also the ML having made high-dimensional
numerical optimization much easier to handle than in the past. While it
has been possible in principle for many years to write fast gradient
code even for complicated calculations by hand (as the present author
has e.g.~demonstrated back in 2008 in
\href{https://arxiv.org/abs/0811.1915}{https://arxiv.org/abs/0811.1915 -
The many vacua of gauged extended supergravities} \cite{fischbacher2009many}), pragmatically
speaking, this was effectively out of reach for many practitioners in
physics due to the high time commitment required to both master the
technique and then write the actual code. A major thesis of this course
is that with ML technology available, just about every physicist should
be empowered to readily tackle many a non-malicious numerical
optimization problem in 1000-dimensional parameter space - and the
course material discusses in detail how to.

Apart from these primary reasons, there also are secondary reasons for
presenting this material in the form chosen here. One of them obviously
is to showcase the general usefulness of Jupyter notebooks (here in
their incarnation as Google Colab documents). On the educational side,
this course emphasizes the importance of forming good mental models for
basic principles and ideas, empowering the participant to correctly and
proficiently reason about behavior - to the largest extent possible.
This empowerment especially is an explicit articulated course objective
here since there is a very concerning trend in recent years for
technical documentation to move in the opposite direction and merely
provide do-this-to-use-it instructions; ``the opposite of education is
training''.

One obvious obstacles to this important objective of always enabling the
student to correctly reason about behavior is of course limited
available time. Clearly, the course needs - for example - to contain a
quick introduction to the Python programming language, but cannot
possibly cover all the relevant detail in about a single day.

Whenever this problem arises, the course at least tries to provide
references to the authoritative documentation, which sometimes may
itself have shortcomings, but nevertheless generally provides the
best-available answers.

Another important obstacle to ``enabling students to reason about
behavior'' is that we generally cannot reason that well about what is
going on deep inside ML models - especially in comparison to algorithmic
code. Overall, this is not at all surprising: once we consider employing
ML to handle a problem, this typically means we already have conceded
defeat on the algorithmic front - that is, given up on the idea that we
could come up with some precise algorithmic definition that could be
inspected and proven to satisfy the expected objectives via stringent
analytical reasoning. Given that we are still discovering and learning
to understand a lot about in particular Deep Learning, we might be able
to explain some things we still struggle with these days better in the
future - but in general, we should perhaps not be too surprised to find
that trying to reason about the behavior of ML systems can run into
similar problems as trying to reason about biological systems.

    
    
    
    

    
    \hypertarget{a-whirlwind-tour-of-python}{%
\section{A Whirlwind Tour of Python}\label{a-whirlwind-tour-of-python}}

\hypertarget{why}{%
\subsection{Why?}\label{why}}

\begin{itemize}
\tightlist
\item
  Many ML frameworks in use today are Python-based.
\item
  In general, tasks around ``ML model definition'' are usually done in
  Python.
\item
  When a trained ML model gets deployed in a product, Python will in
  general not be involved.
\end{itemize}

\hypertarget{objective-of-this-lecture}{%
\subsection{Objective of this Lecture}\label{objective-of-this-lecture}}

\begin{itemize}
\tightlist
\item
  Give a reasonably solid overview over the skeletal structure of the
  Python programming language.

  \begin{itemize}
  \tightlist
  \item
    ``These are the basic ideas and principles''
  \item
    ``Authoritative documentation is over there''
  \item
    ``Extension Modules/Libraries are documented here''
  \item
    General principle: To the extent we can do this given constraints,
    we will always reference the authoritative sources of truth for
    information about Python (rather than merely presenting examples).
  \end{itemize}
\item
  Explain basic design decisions underlying the language.
\item
  Gain some familiarity with one particular Python execution environment
  that is useful for experimenting - Google Colab Notebooks.
\end{itemize}

    \hypertarget{colab-notebooks}{%
\subsection{Colab Notebooks}\label{colab-notebooks}}

This document is a Google colab notebook. Much of this course will be
notebook-based, since such notebooks are a UI concept many physicists
(and data analysts) are already familiar with.

What are we seeing here? The current file is a document on Google Drive
cloud storage (linked to a @gmail.com or @google.com account) that is an
``interactive Python notebook'' - it can be downloaded as such, using
the menu (and re-uploaded again). It can also be downloaded as a .py
Python file. This loses some information about text cells and their
formatting. Also, Colab notebooks have some limited (single-file)
version tracking capabilities, to explore questions such as ``what did I
change over the past 24 hours in this file?''

A Colab Notebook can be connected to a ``Colab Runtime''. Basically,
this is an (emulated) Linux machine in the cloud that can execute
``Interactive Python'' (IPython) commands. The IPython interpreter will
forward lines of code that start with an exclamation mark (!) to the
shell - so we can indeed run shell code on the runtime Virtual Machine
(as a root-user!).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{err}{!}\PY{n}{head} \PY{o}{\PYZhy{}}\PY{n}{n} \PY{l+m+mi}{20} \PY{o}{\PYZlt{}} \PY{o}{/}\PY{n}{proc}\PY{o}{/}\PY{n}{cpuinfo}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
processor       : 0
vendor\_id       : AuthenticAMD
cpu family      : 23
model           : 49
model name      : AMD EPYC 7B12
stepping        : 0
microcode       : 0xffffffff
cpu MHz         : 2249.998
cache size      : 512 KB
physical id     : 0
siblings        : 2
core id         : 0
cpu cores       : 1
apicid          : 0
initial apicid  : 0
fpu             : yes
fpu\_exception   : yes
cpuid level     : 13
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov
pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr\_opt pdpe1gb rdtscp
lm constant\_tsc rep\_good nopl nonstop\_tsc cpuid extd\_apicid tsc\_known\_freq pni
pclmulqdq ssse3 fma cx16 sse4\_1 sse4\_2 movbe popcnt aes xsave avx f16c rdrand
hypervisor lahf\_lm cmp\_legacy cr8\_legacy abm sse4a misalignsse 3dnowprefetch
osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc\_adjust bmi1 avx2 smep
bmi2 rdseed adx smap clflushopt clwb sha\_ni xsaveopt xsavec xgetbv1 clzero
xsaveerptr arat npt nrip\_save umip rdpid
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{err}{!}\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{} Disk Usage \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}
\PY{err}{!}\PY{n}{du} \PY{o}{\PYZhy{}}\PY{n}{h} \PY{o}{.}
\PY{err}{!}\PY{n}{df} \PY{o}{\PYZhy{}}\PY{n}{h}
\PY{err}{!}\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{} Linux System RAM \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}
\PY{err}{!}\PY{n}{head} \PY{o}{\PYZhy{}}\PY{n}{n} \PY{l+m+mi}{10} \PY{o}{\PYZlt{}} \PY{o}{/}\PY{n}{proc}\PY{o}{/}\PY{n}{meminfo}
\PY{c+c1}{\PYZsh{} The UI also shows ram/disk information about the connected runtime system}
\PY{c+c1}{\PYZsh{} in the top right corner.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\#\#\# Disk Usage \#\#\#
72K     ./.config/logs/2023.08.14
76K     ./.config/logs
8.0K    ./.config/configurations
120K    ./.config
55M     ./sample\_data
55M     .
Filesystem      Size  Used Avail Use\% Mounted on
overlay         108G   27G   82G  25\% /
tmpfs            64M     0   64M   0\% /dev
shm             5.8G     0  5.8G   0\% /dev/shm
/dev/root       2.0G  1.1G  887M  55\% /usr/sbin/docker-init
tmpfs           6.4G  360K  6.4G   1\% /var/colab
/dev/sda1        44G   28G   16G  65\% /etc/hosts
tmpfs           6.4G     0  6.4G   0\% /proc/acpi
tmpfs           6.4G     0  6.4G   0\% /proc/scsi
tmpfs           6.4G     0  6.4G   0\% /sys/firmware
\#\#\# Linux System RAM \#\#\#
MemTotal:       13294252 kB
MemFree:         8461152 kB
MemAvailable:   12071020 kB
Buffers:           98472 kB
Cached:          3672952 kB
SwapCached:            0 kB
Active:           910260 kB
Inactive:        3608388 kB
Active(anon):       1324 kB
Inactive(anon):   747552 kB
    \end{Verbatim}

    The underlying runtime is a (modified) Debian GNU/Linux system.

We can install extra packages if we want to.

Runtime systems are allocated when we start a colab and get de-allocated
if they remain idle for too long.

There of course are some ``terms and conditions'' around what colabs can
and can not be used for. These can be found at:
\url{https://colab.research.google.com/pro/terms/v1}.

Many relevant packages are pre-installed here, but let us install some
extra Debian packages and also some extra Python packages which we will
need further down in this specific notebook.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{err}{!}\PY{n}{apt} \PY{n}{install} \PY{n}{clisp} \PY{c+c1}{\PYZsh{} A Debian package}
\PY{err}{!}\PY{n}{pip} \PY{n}{install} \PY{n}{lark} \PY{c+c1}{\PYZsh{} A Python package}
\PY{c+c1}{\PYZsh{} (Output below is from a 2nd run; these install commands are idempotent}
\PY{c+c1}{\PYZsh{}  apart from producing lengthy output when installing a package.)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Reading package lists{\ldots} Done
Building dependency tree{\ldots} Done
Reading state information{\ldots} Done
clisp is already the newest version (1:2.49.20210628.gitde01f0f-2).
0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.
Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages
(1.1.7)
    \end{Verbatim}

    \hypertarget{python---some-history}{%
\subsection{Python - Some History}\label{python---some-history}}

    \begin{itemize}
\tightlist
\item
  Crude but useful model: There are two complementary approaches to
  ``computing'': machine-oriented vs.~maths-oriented.

  \begin{itemize}
  \tightlist
  \item
    Machine-oriented school: Focus on Turing machines, Knuth's ``MIX
    Assembly'', etc.
  \item
    Maths-oriented: ``What primitives do we need to meaningfully talk
    about algorithms?''

    \begin{itemize}
    \item
      ``Term evaluation'' and ``function application'' can take us very
      far.
    \item
      (This needs a suitable notion of ``function'' \textasciitilde{}
      computation specification.)
    \item
      ``More recently'' also of importance (\textasciitilde since
      1970+): Concepts from category theory.

      Especially: those related to ``algebraic data types'' and
      ``composition operations''.
    \end{itemize}
  \end{itemize}
\item
  Modern ML is closely related to AI.

  \begin{itemize}
  \tightlist
  \item
    Historically, much of AI-related work since the 1960s has been done
    in LISP.
  \item
    LISP follows the ``maths-oriented'' approach to computation.
  \item
    Looking at the core language (sans libraries), many LISP systems are
    still more powerful than Python. Python became popular due to
    ``being more accessible''.
  \item
    LISP's blessing and curse is its simplicity:

    \begin{itemize}
    \item
      One uniform approach to express structured data:

      ``Structure is generally built out of pairs'' (called `cons
      cells').
    \item
      Programs are also directly written down as syntax trees!
    \item
      A minimal LISP-like system:

      \begin{itemize}
      \item
        Key atoms: ``symbols'' (``name tags'' that provide ``identity'')
        and numbers.
      \item
        ``Pairs'', written as \texttt{(left\ .\ right)} with extra rule
        ``if a dot precedes a `(', we need not write the dot and the
        parentheses around the expression to its right''.
      \item
        ``Evaluation rules'' for ``evaluating an expression specified as
        a tree''.
      \item
        Some built-in function definitions, like the functions
        \texttt{abs}, \texttt{+}, etc.
      \item
        Most important non-function elements: ``if''-conditional and
        ``define-a-function-that-evaluates-this-term''.
      \end{itemize}
    \item
      Lisp code, like sheet music, ``looks alien to the un-initiated''.

      \begin{itemize}
      \tightlist
      \item
        Entry barrier!
      \item
        Requires discipline!
      \item
        (\ldots but great experience once mastered, due to structural
        simplicity bringing enormous flexibility).
      \end{itemize}
    \item
      Two of the three currently most popular programming languages
      conceptually are effectively LISP systems ``with some syntax sugar
      on top to lower the entry barrier'':
      \href{https://thenewstack.io/brendan-eich-on-creating-javascript-in-10-days-and-what-hed-do-differently-today/}{JavaScript}
      and \href{https://norvig.com/python-lisp.html\#sample}{Python}.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{python-vs.-lisp}{%
\subsubsection{Python vs.~Lisp}\label{python-vs.-lisp}}

We will have to say more about Python-vs-Lisp when we discuss the
TensorFlow2 evaluation model. Here, we only have time for a quick - but
important - first glimpse.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This Python notebook Code cell}
\PY{c+c1}{\PYZsh{} \PYZhy{} defines a multi\PYZhy{}line string that contains some LISP code.}
\PY{c+c1}{\PYZsh{} \PYZhy{} writes that LISP code to a file.}
\PY{c+c1}{\PYZsh{} \PYZhy{} executes a LISP interpreter on that code.}
\PY{c+c1}{\PYZsh{} \PYZhy{} Also contains a Python function that closely corresponds to the}
\PY{c+c1}{\PYZsh{}   LISP function in this file, for syntax comparison.}

\PY{n}{lisp\PYZus{}code\PYZus{}1} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+s2}{(defun print\PYZhy{}multiplication\PYZhy{}table (n)}
\PY{l+s+s2}{  (loop for i from 1 to 10}
\PY{l+s+s2}{   do (format t }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZti{}2d x \PYZti{}2d = \PYZti{}2d\PYZti{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ n i (* n i))))}
\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}

\PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/tmp/mt.lisp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h\PYZus{}out}\PY{p}{:}
  \PY{n}{h\PYZus{}out}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{lisp\PYZus{}code\PYZus{}1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== LISP ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{err}{!}\PY{n}{clisp} \PY{o}{\PYZhy{}}\PY{n}{q} \PY{o}{\PYZhy{}}\PY{n}{x} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(load }\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{/tmp/mt.lisp}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZhy{}}\PY{n}{x} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(print\PYZhy{}multiplication\PYZhy{}table 7)}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}

\PY{k}{def} \PY{n+nf}{print\PYZus{}multiplication\PYZus{}table}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}2d}\PY{l+s+s1}{ x }\PY{l+s+si}{\PYZpc{}2d}\PY{l+s+s1}{ = }\PY{l+s+si}{\PYZpc{}2d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{n} \PY{o}{*} \PY{n}{i}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== Python ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{print\PYZus{}multiplication\PYZus{}table}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
=== LISP ===
;; Loading file /tmp/mt.lisp {\ldots}
;; Loaded file /tmp/mt.lisp
\#P"/tmp/mt.lisp"
 7 x  1 =  7
 7 x  2 = 14
 7 x  3 = 21
 7 x  4 = 28
 7 x  5 = 35
 7 x  6 = 42
 7 x  7 = 49
 7 x  8 = 56
 7 x  9 = 63
 7 x 10 = 70
NIL
=== Python ===
 7 x  1 =  7
 7 x  2 = 14
 7 x  3 = 21
 7 x  4 = 28
 7 x  5 = 35
 7 x  6 = 42
 7 x  7 = 49
 7 x  8 = 56
 7 x  9 = 63
 7 x 10 = 70
    \end{Verbatim}

    The similarities between Python's ``def print\_multiplication\_table''
and the LISP ``defun print-multiplication-table'' are quite evident.

However, the LISP code is merely a tree of symbols (and numbers and a
string) that could equivalently be written as:

\begin{verbatim}
(defun . (print-multiplication-table . ((n . ()) . {...})
\end{verbatim}

Here, indentation is purely arbitrary, and we used a single simple(!)
rule to elide ``spurious'' parentheses.

For the Python code however, indentation carries meaning (like in
Haskell). While LISP code generally is described as ``having many
parentheses'', every LISP hacker reads and writes code by-indentation,
and Python merely makes indentation carry actual meaning.

\emph{Participant Exercise}: How many parentheses do we count in the
Python and LISP definitions of the multiplication-table function?

    \hypertarget{understanding-python}{%
\section{Understanding Python}\label{understanding-python}}

As for just about every programming language, understanding its
structure and interpretation requires a solid understanding of three
core aspects:

\begin{itemize}
\tightlist
\item
  The Syntax
\item
  The ``Data Model''
\item
  The ``Execution Model''
\end{itemize}

    \hypertarget{python-syntax}{%
\subsection{Python Syntax}\label{python-syntax}}

    While in LISP, one directly specifies an arithmetic expression as a
tree, such as \texttt{(+\ 2\ (*\ 3\ 4))} (``the sum of 2 and (the
product of 3 and 4)''), Python uses a dedicated software component that
translates source code to an internal ``syntax tree representation''.
This is the job of a ``Parser''.

Generally speaking, almost all programming languages use a ``parser'' to
process source code, but such parsers are basically never written by
hand. Rather, there are programs that take as input a
grammar-specification (itself ``a kind of program'') and produce the
code for a parser that processes input following this grammar.

\hypertarget{mathematical-term-grammar}{%
\subsubsection{Mathematical term
grammar}\label{mathematical-term-grammar}}

Common mathematical notation also has a grammar - and the grammar of
most programming languages mostly is an extension of that grammar. This
term-grammar is generally introduced/discussed in 5th/6th grade
mathematics, with a typical exercise of the form:

\begin{verbatim}
Translate the term `5+(7+9)/2` into structured form.
\end{verbatim}

The expected answer being:

\begin{verbatim}
  * This term is a sum.
    * The 1st summand is the number 5
    * The 2nd summand is a quotient.
      * The numerator is a parenthesized term.
        * This term is a sum.
          * The 1st summand is the number 7.
          * The 2nd summand is the number 9.
      * The denominator is the number 2.
\end{verbatim}

This ``structured form'' is, of course, a tree (\ldots which we could
write in LISP tree notation). Let us actually write a parser for such a
simplified term grammar, using a grammar and a parser-generator (the one
provided by the ``lark'' Python module, which we installed and imported
earlier).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{lark}

\PY{c+c1}{\PYZsh{} EBNF (Extended Backus\PYZhy{}Naur Form) grammar, see:}
\PY{c+c1}{\PYZsh{} https://en.wikipedia.org/wiki/Extended\PYZus{}Backus\PYZpc{}E2\PYZpc{}80\PYZpc{}93Naur\PYZus{}form}
\PY{c+c1}{\PYZsh{} https://lark\PYZhy{}parser.readthedocs.io/\PYZus{}/downloads/en/latest/pdf}
\PY{n}{grammar} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+s2}{?start: sum}

\PY{l+s+s2}{?sum: product}
\PY{l+s+s2}{    | sum }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ product  \PYZhy{}\PYZgt{} add}
\PY{l+s+s2}{    | sum }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ product  \PYZhy{}\PYZgt{} sub}

\PY{l+s+s2}{?product: factor}
\PY{l+s+s2}{    | product }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ factor \PYZhy{}\PYZgt{} mul}
\PY{l+s+s2}{    | product }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ factor \PYZhy{}\PYZgt{} div}

\PY{l+s+s2}{?factor: NUMBER \PYZhy{}\PYZgt{} num}
\PY{l+s+s2}{    | }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ sum }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}

\PY{l+s+s2}{NUMBER: /[+\PYZhy{}]?[0\PYZhy{}9]+/}
\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}

\PY{n}{parser} \PY{o}{=} \PY{n}{lark}\PY{o}{.}\PY{n}{Lark}\PY{p}{(}\PY{n}{grammar}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{parser}\PY{o}{.}\PY{n}{parse}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{7}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Prints:}
\PY{c+c1}{\PYZsh{} Tree(\PYZsq{}num\PYZsq{}, [Token(\PYZsq{}NUMBER\PYZsq{}, \PYZsq{}7\PYZsq{})])}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{======}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{parser}\PY{o}{.}\PY{n}{parse}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1+2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Prints:}
\PY{c+c1}{\PYZsh{} Tree(\PYZsq{}add\PYZsq{}, [Tree(\PYZsq{}num\PYZsq{}, [Token(\PYZsq{}NUMBER\PYZsq{}, \PYZsq{}1\PYZsq{})]), Tree(\PYZsq{}num\PYZsq{}, [Token(\PYZsq{}NUMBER\PYZsq{}, \PYZsq{}2\PYZsq{})])])}

\PY{c+c1}{\PYZsh{} There also is pretty\PYZhy{}printing for parsed trees:}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(pretty\PYZhy{}printed form)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{parser}\PY{o}{.}\PY{n}{parse}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1+2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{pretty}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{show\PYZus{}pretty}\PY{p}{(}\PY{n}{term}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ ===}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{term}\PY{p}{,} \PY{n}{parser}\PY{o}{.}\PY{n}{parse}\PY{p}{(}\PY{n}{term}\PY{p}{)}\PY{o}{.}\PY{n}{pretty}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{======}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{show\PYZus{}pretty}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2+3*(4+5)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Further examples:}
\PY{c+c1}{\PYZsh{} print(show\PYZus{}pretty(\PYZdq{}7*8*9/10\PYZdq{}))}
\PY{c+c1}{\PYZsh{} print(show\PYZus{}pretty(\PYZdq{}2*3\PYZhy{}4*5*6\PYZhy{}7*8\PYZdq{}))}
\PY{c+c1}{\PYZsh{} print(show\PYZus{}pretty(\PYZdq{}1+1/(1+1/(1+1/(1+1)))\PYZdq{}))}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Tree('num', [Token('NUMBER', '7')])
======
Tree('add', [Tree('num', [Token('NUMBER', '1')]), Tree('num', [Token('NUMBER',
'2')])])
(pretty-printed form)
add
  num   1
  num   2

======
=== 2+3*(4+5) ===
add
  num   2
  mul
    num 3
    add
      num       4
      num       5

    \end{Verbatim}

    \hypertarget{pythons-grammar}{%
\subsubsection{Python's Grammar}\label{pythons-grammar}}

Python's (and most other programming languages') grammar can be seen as
a massive expansion of the commonly used grammar for mathematical terms.
Here and in languages such as C, Java, etc that can be (mostly) parsed
with a grammar, the typical extensions are:

\begin{itemize}
\tightlist
\item
  We have many more binary operators beyond the arithmetic ones,
  \texttt{+,\ -,\ *,\ /,\ \{**\ or\ \^{}\}}. Typically, there also is
  \texttt{\textless{}\textless{},\ \textasciitilde{},\ \&,\ \%}, plus
  quite a few more.
\item
  There are both ``expressions'' (which evaluate to a value, so
  ``generalized mathematical terms''), and also ``statements'' which
  ``describe instructions'', and then there also is a notion of a
  ``block of statements'' - which are executed one-after-another.
\item
  In some languages (including Python), ``expressions'' (which have a
  value) are a special kind of statements.
\end{itemize}

The Python language's full grammar specification can be found at:
\url{https://docs.python.org/3/reference/grammar.html}. This then in
particular settles questions such as what the specific role of a pair of
round parentheses is in a piece of code, what operators there are, how
operator precedence works, and what the keywords in the language are.
Naturally, the grammar has some hierarchical structure, there is a
sub-grammar for expressions sitting inside it, for example.

Going forward, we will occasionally come back to the Python grammar and
associated documentation for authoritative answers about how code is
interpreted.

As far as Python is concerned, the Python code we write first gets
transformed into a tree representation. The parser that does this (or
rather, an equivalent one) is also available as a Python module, in the
\texttt{ast} (Abstract Syntax Tree) package. In LISP, we would directly
have written that tree ourselves.

The most important statements we will have to know about are:

\begin{itemize}
\tightlist
\item
  Expressions (= statements that have a value).
\item
  Import statements. These load a ``module'' / ``software library''
  (which must be installed on the (virtual) machine):

  \begin{itemize}
  \tightlist
  \item
    \texttt{import\ numpy}
  \item
    \texttt{import\ tensorflow\ as\ tf}
  \item
    \texttt{from\ scipy\ import\ optimize\ as\ sopt}
  \end{itemize}
\item
  Assignments

  \begin{itemize}
  \tightlist
  \item
    At the ``semantics'' level, we need to discriminate between
    assignments that introduce a new variable and assignments that
    change the value of a variable.
  \item
    Unlike in C, assignments do not have a value, but the grammar
    nevertheless allows us to chain them, as in \texttt{a\ =\ b\ =\ 0}.
  \item
    The thing on the left hand side must designate some place that can
    hold a value. Destructuring is supported. The thing on the right
    must be an expression that has a single value, or an assignment (for
    chaining).
  \item
    Examples

    \begin{itemize}
    \tightlist
    \item
      \texttt{x\ =\ 2\ +\ 3\ +\ f(y)}
    \item
      \texttt{somelist{[}0{]}\ =\ foo.x\ =\ y\ =\ 7}
    \item
      \texttt{(x,\ y,\ z)\ =\ (10,\ 20,\ 30)}
    \item
      (\ldots we will see quite a few more handy things in code examples
      later\ldots)
    \end{itemize}
  \end{itemize}
\item
  Function Definitions

  \begin{itemize}
  \item
    There are both ``named functions'' and ``unnamed functions''.
  \item
    A named function definition is a statement. Example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ squared(x):}
  \ControlFlowTok{return}\NormalTok{ x}\OperatorTok{*}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

    The form is:

\begin{verbatim}
def {name}({...parameters...}):
  """ ... optional docstring ... """
  body_statement
  ... optionally, more body statements ...
\end{verbatim}
  \item
    Reaching a \texttt{return} statement exits the function, returning
    the value after \texttt{return}, if such a value given.
  \item
    Reaching a bare \texttt{return} statement, or reaching end-of-body
    makes the function return the special value \texttt{None}.
  \item
    ``unnamed functions'' / ``lambda functions'' are introduced as
    function-valued expressions of the form
    \texttt{lambda\ \{parameters\}:\ \{expression\}}.

    So, we can do \texttt{squared\ =\ lambda\ x:\ x*x}. The body is an
    expression, not a statement. There is no \texttt{return} here.
  \end{itemize}
\item
  Conditionals and loops.

  \begin{itemize}
  \tightlist
  \item
    \texttt{if/for/while}/\texttt{break/continue}
  \item
    In time, we will see examples.
  \item
    Surprisingly, these are less frequent/relevant in Python than many
    other languages, since ``there often is a better way to do it''.
  \end{itemize}
\item
  Exception handling

  \begin{itemize}
  \tightlist
  \item
    \texttt{try/except/finally} - only play a side role in this course.
  \end{itemize}
\item
  Class definitions

  \begin{itemize}
  \tightlist
  \item
    This basic course will mostly try to circumnavigate having to define
    classes. Pragmatically, object-orientation is much about preventing
    authors from using global definitions for state-management -
    i.e.~making state-management local by having parts of the relevant
    program-state (divided up in a meaningful way) be owned by different
    class instances (``objects'') and manipulated through clearly
    defined pathways (``instance methods''). Often, a superior approach
    to state-management is \emph{avoiding the need to manage state in
    the first place} - such as by focusing on designing programs in
    terms of data-transformations. In maths-heavy use cases (including
    Machine Learning), many relevant objects ``have value semantics'' -
    loosely, ``behave in some sense like numbers'', i.e.~may have
    components akin to how a fraction has a numerator and a denominator,
    but should not be thought of as having any mutable properties - and
    this automatically does away with much of the need for
    state-management. The one place where we will obviously encounter
    stateful objects is in ``trainable'' components of a machine
    learning model. ``Model training'' is all about tweaking their
    internal state-parameters in a way that improves performance.
  \end{itemize}
\end{itemize}

    \hypertarget{pythons-data-model}{%
\subsection{Python's ``Data Model''}\label{pythons-data-model}}

    Python's Data Model in many ways resembles Common Lisp's, but its
implementation resembles Perl more than any LISP system.

\hypertarget{key-aspects}{%
\subsubsection{Key Aspects}\label{key-aspects}}

\begin{itemize}
\item
  Variables are names that reference an in-memory value (or ``object'').
  We must distinguish between the \texttt{name} (variable) and the value
  that is being referred to by a name. The variable is in itself not
  really an object, and there is only one situation where a variable
  shows up as a stateful (``container-like'') entity.
\item
  Every value in Python is an object in memory and has a memory address.

  These addresses are generally ``hidden behind the scenes'', we
  normally do not have to concern ourselves with these.

  The CPython interpreter actually exposes these addresses via the
  \texttt{id(.)} function. For every \texttt{x}, the value of
  \texttt{id(x)} is an integer such that \texttt{id(x)\ ==\ id(y)} holds
  if and only if the variables \texttt{x} and \texttt{y} name the same
  in-memory-object. (If an object gets reclaimed after becoming
  provably-unreachable, a different object can have the same \texttt{id}
  later.)
\item
  Every in-memory value/object has a clearly defined type. This type is
  a value/object itself, a ``class object''. The function
  \texttt{type(.)} maps an object to its type(-object).
\item
  Terminology:

  \begin{itemize}
  \tightlist
  \item
    A \texttt{type} is the same as a \texttt{class}.
  \item
    A class can be a ``subclass'' of other classes defined earlier.
  \item
    ``Is subclass of'' is generally considered to be a reflexive(!) and
    transitive property, so, the term ``subclasses of X'' generally
    refers to the entire class hierarchy tree rooted at and including X.
  \item
    If the type of an object \texttt{o} is \texttt{t}, then ``\texttt{o}
    is an instance (a''class instance'') of \texttt{t}``, and also, if
    \texttt{t} is a subclass of \texttt{t0}, then \texttt{o} also is an
    instance of \texttt{t0}.
  \item
    The \texttt{issubclass(t,\ t0)} function checks whether \texttt{t}
    is a (not necessarily proper) subclass of \texttt{t0}. Hence,
    \texttt{issubclass(type(x),\ type(x))} always evaluates to
    \texttt{True}, for every object \texttt{x}.
  \end{itemize}
\end{itemize}

\hypertarget{important-typesclasses}{%
\subsubsection{Important Types/Classes}\label{important-typesclasses}}

Here, we are takling exclusively about the ``dynamic types'', the ones
obtained by \texttt{type(.)}.

There also are ``static type annotations'', but we do not talk about
these anywhere in this course.

\begin{itemize}
\tightlist
\item
  The type \texttt{int}.

  \begin{itemize}
  \tightlist
  \item
    Arbitrary-size integers (since Python3).
  \item
    Literals (examples): \texttt{-5}, \texttt{100},
    \texttt{1\_000\_000}, \texttt{0xff}.
  \end{itemize}
\item
  The type \texttt{bool}.

  \begin{itemize}
  \tightlist
  \item
    Subclass of \texttt{int}.
  \item
    Literals: \texttt{True}, \texttt{False}.
  \item
    Cannot be subclassed. This guarantees that
    \texttt{issubclass(type(x),\ bool)} implies
    \texttt{x\ is\ True\ or\ x\ is\ False} - there can only be these
    exact two in-memory objects of this type.
  \item
    Used as an integer, \texttt{True} is equivalent to 1, and
    \texttt{False} is equivalent to 0, so (for example)
    \texttt{True\ +\ True\ =\ 2}.
  \end{itemize}
\item
  The type \texttt{float}.

  \begin{itemize}
  \tightlist
  \item
    IEEE-754 ``binary64'' floats.
  \item
    Literals (examples): \texttt{1.23}, \texttt{-4.567e-12}.
  \item
    ``As everywhere else'': 64-bit signed floating point numbers with
    just shy of 16 valid decimal digits. Valid exponents in decimal
    include the range (-300..300) and extend a little bit beyond that.
    There is minus-zero, positive-infinity, negative-infinity, and
    not-a-number. not-a-number is not equal to itself.
  \item
    Note: \texttt{1.0/0.0} does not evaluate to float-infinity
    \texttt{float("+inf")}. Rather, evaluating this expression raises a
    \texttt{ZeroDivisionError}.
  \item
    Related type: \texttt{complex}: pair of binary64 floats representing
    real and imaginary part of a complex number.
  \end{itemize}
\item
  The type \texttt{NoneType}.

  \begin{itemize}
  \tightlist
  \item
    There is only one in-memory object of this type.
  \item
    Literal: \texttt{None}.
  \item
    Cannot be subclassed.
  \item
    Generally used in many Python APIs as a ``poor man's substitute to
    express `value can be missing'\,'', but ``category-theory-wise
    botched''.
  \item
    Terminology: in documentation, ``an optional int'' means: ``this is
    an int instance, or the value \texttt{None}''. (Likewise: ``an
    optional string'', etc.) Design problem: There is no sound notion of
    ``an optional X'' for arbitrary X, since ``an optional optional
    int'' does not work.
  \end{itemize}
\item
  The types \texttt{str} and \texttt{bytes}.

  \begin{itemize}
  \tightlist
  \item
    Informally, \texttt{str} instances represent text (such as: Unicode
    text), while \texttt{bytes} instances represent sequences of 8-bit
    bytes.
  \item
    Unlike C/Java/etc, there is no separate \texttt{character} type
    \texttt{char}. In Python APIs, characters are represented by
    length-1 strings.
  \item
    Literals

    \begin{itemize}
    \tightlist
    \item
      str: \texttt{"abc"},
      \texttt{\textquotesingle{}def\textquotesingle{}},
      \texttt{\textquotesingle{}\textquotesingle{}\textquotesingle{}ghijk\textquotesingle{}\textquotesingle{}\textquotesingle{}},
      \texttt{"""xyz"""}
    \item
      bytes: \texttt{b"abc"},
      \texttt{b\textquotesingle{}def\textquotesingle{}}, etc.
    \item
      There are special-form literals such as raw-string literals for
      which the rules about how to interpret embedded backslashes are
      different.
    \item
      Adjacent literals ``get merged into one literal at parse time''
      (like in C or C++).
    \item
      Indexable, and so technically ``sequences''. Iterating over a
      string iterates over its 1-letter-long substrings.
    \item
      Closely related to \texttt{bytes}: \texttt{bytearray} - which we
      will not discuss here.
    \end{itemize}
  \end{itemize}
\item
  The type \texttt{tuple}.

  \begin{itemize}
  \item
    Tuples are ordered sequences holding a finite number of
    (arbitrary-type) objects.
  \item
    If \texttt{type(x)\ is\ tuple}, then \texttt{len(x)} is the number
    of elements of the tuple, and \texttt{x{[}0{]}} \ldots{}
    \texttt{x{[}len(x)-1{]}} refer to the elements of the tuple.
    \emph{Index counting always starts at zero}.
  \item
    Literals (examples): \texttt{()}, \texttt{(7,)},
    \texttt{(1,\ True)}, \texttt{(1,\ (2,\ 5))},
    \texttt{(1,\ -2.0,\ 7,)},

    \texttt{(True,\ None,\ "foo",\ (2,\ 3.0),\ b"bar",\ bool)}.
  \item
    There can be trailing commas.

    Syntax wart: a 1-tuple \emph{must} be written with a trailing dot,
    since \texttt{(7)} would be a parenthesized expression that
    evaluates to the \texttt{int} instance \texttt{7}.
  \item
    ``Immutable'' in the sense that they do not permit changing
    object-state by assigning to slots: \texttt{mytuple{[}0{]}\ =\ 7}
    will raise a \texttt{TypeError} exception.
  \item
    ``Addition'' is overloaded on tuples to mean ``concatenation''
    (similary for strings), so:
    \texttt{(1,\ 2)\ +\ (3,\ 4)\ =\textgreater{}\ (1,\ 2,\ 3,\ 4)}.
  \end{itemize}
\item
  The type \texttt{list}.

  \begin{itemize}
  \item
    Like a tuple, a \texttt{list} is an ordered sequence holding a
    finite number of objects (which in principle can be arbitrary-type,
    but quite often will be homogeneous).
  \item
    Lists ``have internal state'', we can change the contents of a
    list-object:

    \begin{itemize}
    \item
      Clearing the list: \texttt{mylist.clear()}.
    \item
      Replacing the k-th element: \texttt{mylist{[}5{]}\ =\ 7}.
    \item
      Appending an element at the end: \texttt{mylist.append(10)}.

      (This uses ``slack space'' allocated in geometric progression,
      like C++'s \texttt{std::vector\textless{}T\textgreater{}}, so is
      amortized-O(1).)
    \item
      (There are more object-state-mutating operations.)
    \end{itemize}
  \item
    These are not `lists' in the LISP sense: Producing a new list from a
    given one that has one extra element at the front is an expensive
    operation with Python lists, but accessing the N-th element requires
    \emph{constant} effort, rather than ``O(N) effort''.
  \item
    Literals (examples): \texttt{{[}{]}}, \texttt{{[}1,\ 2,\ 3{]}},
    \texttt{{[}(1,\ True),\ (2,\ False){]}},

    \texttt{{[}None,\ {[}1,\ 2,\ 3{]},\ None,\ {[}4,\ 5{]},\ None{]}}.
  \item
    Since we can assign to lists, we can make a list ``contain itself'':
    \texttt{xs\ =\ {[}None,\ 7{]};\ xs{[}0{]}\ =\ xs}.

    General principle: Having objects for which traversing their
    components can get us back to the original object is technically
    possible in Python, but really really should always be avoided!
    (LISP does not have much of an issue here.)
  \end{itemize}
\item
  The type \texttt{range}.

  \begin{itemize}
  \item
    Represents a range of integers that has a length, a range, a
    start-value, and a step size.
  \item
    Conceptually, behaves a lot like a length-k tuple of integers with
    constant spacing, but is more efficient to represent in memory,
    iterate over, etc.
  \item
    Example: \texttt{range(0,\ 10,\ 1)} is the range of integers
    starting at 0 (inclusive), ending at 10 (exclusive), with step size
    1.

    The common short-hand here is \texttt{range(10)}. (Common convention
    throughout: start-indices are inclusive, end-indices exclusive.)
  \end{itemize}
\item
  The types \texttt{set} and \texttt{frozenset}.

  \begin{itemize}
  \item
    Both set- and frozenset-instances represent unordered collections of
    different objects.
  \item
    Instances of \texttt{set} have mutable state, so can be changed
    (adding/deleting elements). Instances of \texttt{frozenset} are
    immutable.
  \item
    Constraint: Elements of \texttt{set} and also \texttt{frozenset}
    must not be mutable, unless they ``define equality in terms of
    identity''. This course: no sets/frozensets with mutable-state
    elements!
  \item
    Set literals (examples): \texttt{\{1,\ 2\}},
    \texttt{\{(5,\ 6),\ (7,\ 8)\}}.
  \item
    Syntax wart: This is not an empty set, but an empty dict(ionary):
    \texttt{\{\}}. As for other types, we can get an
    ``empty/zero/neutral value'' by calling the type-object with no
    parameters: \texttt{set()}.
  \item
    Frozenset examples: No literals, but can create frozensets by
    calling \texttt{frozenset} with an arbitrary collection as a
    parameter: \texttt{frozenset({[}1,\ 2,\ 3{]})},
    \texttt{frozenset(\{"abc",\ "ghi"\})}, \texttt{frozenset(range(7))}
  \item
    Caution: This does not work (\texttt{1} is not a collection):
    \texttt{frozenset(1)}.

    This might do something unexpected:
    \texttt{frozenset(\textquotesingle{}cat\textquotesingle{})}
    (evaluates the same way as:
    \texttt{frozenset({[}\textquotesingle{}a\textquotesingle{},\ \textquotesingle{}c\textquotesingle{},\ \textquotesingle{}t\textquotesingle{}{]})}).
  \end{itemize}
\item
  The type \texttt{dict}.

  \begin{itemize}
  \item
    Table of key-value pairs, ``with fast access'', mostly corresponds
    to C++ \texttt{std::unordered\_map\textless{}K,\ V\textgreater{}},
    or Java \texttt{HashMap\textless{}K,\ V\textgreater{}}, but
    technically, neither keys nor values need be type-homogeneous.
  \item
    Mutable, has state, we can add and delete entries.
  \item
    For keys, the same rules apply as for set-elements. Unlike
    JavaScript and Perl, keys need not be strings, can be complex
    objects.
  \item
    When used as a collection, mostly treated like the collection of its
    keys.
  \item
    Literals (examples):

    \texttt{\{"A":\ 1,\ "a":\ 1,\ "B":\ 2,\ "b":\ 2\}},

    \texttt{\{3:\ 9,\ -3:\ 9\}},

    \texttt{\{(1,\ 2,\ 3):\ (3,\ 2,\ 1),\ (5,\ 6):\ (6,\ 5)\}}.
  \end{itemize}
\item
  The type \texttt{function} - and the abstract base class
  \texttt{Callable}.

  \begin{itemize}
  \tightlist
  \item
    Functions that map input-parameters to output-values.
  \item
    There are different types of function-like objects:

    \begin{itemize}
    \tightlist
    \item
      built-in functions
    \item
      user-defined functions
    \item
      class instance methods
    \item
      class methods
    \item
      instances of classes implementing a \texttt{\_\_call\_\_} method
    \end{itemize}

    These all are instances of the abstract base type
    \texttt{collections.abc.Callable}.
  \item
    More about them later.
  \end{itemize}
\item
  The type \texttt{type}.

  \begin{itemize}
  \item
    Instances of the type \texttt{type} are type-objects, such as
    \texttt{int}, \texttt{str}, \texttt{tuple}, \texttt{type}.
  \item
    (See above): For many such type-objects, using them as callables
    with zero arguments evaluates to an `empty' instance:

    \texttt{str()\ -\textgreater{}\ ""},
    \texttt{int()\ -\textgreater{}\ 0},
    \texttt{list()\ -\textgreater{}\ {[}{]}}, etc.
  \item
    Generally, we can introduce `derivatives' of most such types, even
    \texttt{int}. (Some restrictions for types that only admit a fixed
    set of literals, like \texttt{bool}.)
  \end{itemize}
\item
  The \texttt{numpy} (numerical Python) module's \texttt{numpy.ndarray}
  type.

  \begin{itemize}
  \item
    Not a core Python datatype, but something introduced and owned by a
    (prominent, but still: entirely optional) library(/``module'').

    Likewise, Python has no built-in ``variable in a symbolic
    expression'' data type, but modules like \texttt{sympy} provide
    that. (LISP would use a \texttt{symbol} here.)
  \item
    The reference that describes the contracts in detail is:
    \url{https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html}
  \item
    Name means ``n-dimensional array'', but as usual, we can save
    ourselves a lot of confusion if we consistently avoid language like
    ``this 1-dimensional ndarray holds a 20-dimensional vector'' or
    ``this 2d array describes a 3d rotation''. Much better: ``this
    1-index ndarray holds a 20-dimensional vector'', respectively ``this
    2-index array describes a 3d rotation''. (This convention
    unfortunately has not been adopted widely yet.)
  \item
    TensorFlow's (and JAX's) tensors are closely modeled after
    \texttt{numpy.ndarray}.
  \item
    General principle: Python (at least the CPython implementation) is a
    slow bytecode-interpreter. NumPy operations are executed by fast
    compiled and optimized numerical code. So, emulating something (like
    matrix multiplication) that can be done with NumPy operations
    through Python is generally not a good idea.
  \item
    A useful basic and mostly-correct mental model:

    \begin{itemize}
    \item
      instances `under the hood' hold on to some block of memory with
      data (typically numerical data) in it, and provide extra
      bookkeeping which allows us to address this block of data in
      various ways, typically via indexing, using zero or more indices.
    \item
      So, this naturally contains arrays with zero indices (``scalars'',
      holding a single number), one index (``vectors''), two indices
      (``matrices''), etc.
    \item
      Each such \texttt{ndarray} has a \texttt{size} (the number of
      elements), a ``number of dimensions'' \texttt{ndim} (misnomer -
      means: number of indices), and a \texttt{shape} (describing the
      range for each index), as well as a numerical data-type
      (\texttt{dtype}), and, of course, \texttt{data}.
    \item
      numpy \texttt{ndarray}s are mutable: We can change data by
      assigning. (Not so for the ``tensors'' in ML frameworks - we will
      see why).
    \item
      Example: we would naturally want to represent the 4x4x4x4
      numerical entries of the Riemann curvature tensor (ignoring all
      symmetry considerations) at a given spacetime event as a float
      {[}4, 4, 4, 4{]}-ndarray.
    \item
      Example: a 800x600 RGB image might be represented as a {[}600,
      800, 3{]}-ndarray with dtype=uint8 (1-byte integers in the range
      0..255). If \texttt{imgdata} is such a \texttt{ndarray}, we would
      have:

      \begin{itemize}
      \tightlist
      \item
        \texttt{len(imgdata)\ ==\ 600}
      \item
        \texttt{imgdata.size\ ==\ 600\ *\ 800\ *\ 3}
      \item
        \texttt{imgdata.dtype\ ==\ numpy.uint8}
      \item
        \texttt{imgdata.shape\ ==\ (600,\ 800,\ 3)}
      \item
        \texttt{imgdata.ndim\ ==\ 3}
      \end{itemize}
    \item
      Wart: if \texttt{a} is a numpy-ndarray, then it depends on whether
      \texttt{a} is a 0-index array or not whether we can iterate over
      it. For this reason, \texttt{ndarray} instances technically are
      not Python sequences (like lists or tuples), but nevertheless in
      many ways behave sequence-like.
    \item
      Important operations with arrays include (we will encounter many
      examples later):

      \begin{itemize}
      \tightlist
      \item
        Indexing readjustments: slicing, striding, reshaping, etc.
      \item
        ``broadcasting'' (generalization of ``adding a vector to every
        row in a matrix'').
      \item
        ``generalized Einstein summation'' type operations.
      \item
        Padding, tiling.
      \item
        Nonlinear element manipulations (such as: clipping outlier
        values).
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  ``Generator-Expressions'' / ``Iterators''.

  \begin{itemize}
  \item
    Just like \texttt{function} in Python is a subtype of
    \texttt{Callable}, these are all subtypes of a
    never-encountered-directly ``abstract type''
    \texttt{collections.abc.Iterator}.
  \item
    There are different specific such types, including
    e.g.~dictionary-key-iterators.
  \item
    These objects have internal state and behave like a tissue
    dispenser, only that they dispense Python objects: We can keep
    pulling objects from them, one after another, until they are
    exhausted. In particular, we can map and iterate over them.
  \item
    Very important for doing computations on-demand.
  \end{itemize}
\end{itemize}

Overall, generator expressions tend to show up all over the place with
modern Python code. An important aspect to keep in mind is that they are
stateful, so consuming a generator/iterator involves side effects. It is
important to ensure that in Python code, reading the code could not ever
lead to mistaken mental models about what side effects occur at
execution time. This normally means that all side effects are expected
to be documented. the state-changes that occur when consuming an
iterator are a borderline-exception in the sense that these are
typically not documented when it is obvious that items will be pulled
from an iterator. However, this also generally means that if a
generator/iterator gets passed to a callable that does not precisely
specify how much the iterator will be advanced, it is not OK to keep
using the same iterator-object post-call, for example by passing it on
to another callable. An important callable that allows re-using its
iterator-argument post-call is - obviously - the \texttt{next()}
built-in function, which retrieves the next element and advances the
iterator by one. This example shows different generator-``dispensers''
for squares.

\begin{verbatim}
>>> squares = (x*x for x in range(1, 11))
>>> next(squares)
1
>>> next(squares)
4
>>> next(squares)
9
>>> type(squares)
<class 'generator'>
>>> list(squares)
[16, 25, 36, 49, 64, 81, 100]
>>> squares2 = (x*x for x in range(1, 11))
>>> list(squares2)
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
>>> all_squares = map(lambda x: x*x, itertools.count(1))
>>> next(all_squares)
1
>>> next(all_squares)
4
# ... could go on "forever" ...
# This is an equivalent form that uses generator-expression syntax:
>>> all_squares2 = (x*x for x in itertools.count(1))
\end{verbatim}

\hypertarget{other-types}{%
\subsubsection{Other types}\label{other-types}}

Python has many more built-in types, including somewhat esoteric ones
such as as \texttt{module}, \texttt{NotImplementedType},
\texttt{ellipsis}, various file-handle types, etc, but we will not
concern ourselves with most of these here.

One type that is somewhat relevant still is the type \texttt{object}.
Every value is an instance of \texttt{object}, and every type is a
subtype of \texttt{object}. (So, the \texttt{object}-type-value is an
instance of the type \texttt{object}.)

\hypertarget{exceptions}{%
\subsubsection{Exceptions}\label{exceptions}}

One important subclass hierarchy to be aware of however is rooted at the
\texttt{Exception} class/type. Instances are typically used as an
argument to a \texttt{raise} statement, which early-terminates a
function in such a way that, rather than the function returning a value,
in-process nested function calls will be aborted until a call frame is
reached that handles the (error-like) exception.

A typical example is this code:

\begin{verbatim}
def is_valid_probability(p):
  """Returns whether `p` is a valid probability."""
  return 0 <= p <= 1  # chained-comparison, equivalent to: 0 <= p and p <= 1.


def bayesian_rule(p1, p2):
  """Combines independent probablities via Bayes's rule."""
  if not (is_valid_probability(p1) and is_valid_probability(p2)):
    raise ValueError('Both p1 and p2 need to be valid probabilities. '
                     f'Got: p1={p1!r}, p2={p2!r}')
  p12 = p1 * p2
  return p12 / (p12 + (1 - p1) * (1 - p2))  
\end{verbatim}

    \hypertarget{pythons-execution-model}{%
\subsection{Python's ``Execution
Model''}\label{pythons-execution-model}}

    A key principle for every programming language is ``scoping'', both in
the concrete technical sense (see:
\url{https://en.wikipedia.org/wiki/Scope_(computer_science)}, and in a
generalized sense.

A basic problem is ``Which context tells us what a particular named
entity is referring to''? Unsurprisingly, this problem also exists in
mathematics - and there are commonly accepted conventions that resolve
all related ambiguities.

In mathematics, let us consider the following definition of the function
\({\rm hypotenuse}(\cdot, \cdot)\):

\[
{\rm hypotenuse}(x, y)=\sqrt{x^2+y^2}
\]

Now, if we defined a function
\(g(x):={\rm hypotenuse}(x, 1)+{\rm hypotenuse}(1, x)\), we would all
agree on the meaning of that definition, despite this definition of
\(g\) using a variable name (namely \(x\)) that also occurs in the
definition of \({\rm hypotenuse}\). Properly formalizing the rules of
such variable-substitutions and variable-binding is - however - not
completely trivial.

Clearly, if we want to have ``components supporting synthesis'', then
having some such rules that govern name-scoping is essential for
building larger and more complicated units out of small units. (Just as
in mathematics.)

There is basically one programming language where a conscious design
decision was made to ignore such ``synthesizability'' considerations and
go with a single flat namespace - ``there is just one thing around that
can be called \(x\)''. This then naturally makes it increasingly hard to
keep track of things the larger programs get - typically reaching
un-manageability at about 1000 lines. That language is BASIC
(specifically, the classic form of BASIC, Dartmouth BASIC pre-v6).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

With code, there are basically three situations where such questions
about ``where is this named thing coming from'' are relevant:

\begin{itemize}
\item
  Lexical scope - meaning of variables inside functions.

  Example: This is the translation-operator (written with spurious
  parentheses for more clarity):

  \texttt{translated\ =\ lambda\ a:\ (lambda\ f:\ (lambda\ x:\ f(x-a)))}

  This is a specific translation: \texttt{shifted5\ =\ translated(5)}.

  Questions such as ``what provides the translation-parameter \texttt{a}
  in this \texttt{shifted5} function'' are about ``lexical scope''.
\item
  Dynamic scope - meaning of context-parameters while some particular
  function is being evaluated. This is typically about resource
  management or some other implicit context we are referring to. ``While
  we are still processing this function's body (and we might have
  stepped into evaluating some inner function), the term `X' shall
  mean\ldots{}''

  \begin{itemize}
  \item
    In Python, dynamic scope (more commonly called ``dynamic context'')
    is typically handled by ``context-manager objects'' that ``know how
    to `enter' and `exit' some context'', and sometimes via
    \texttt{try/finally} blocks. The general idea is: ``we need to
    close/drop/remove something as control flow leaves a block, no
    matter how'' and ``as long as the block is being processed, the
    to-be-closed/removed thing is available.''
  \item
    A common case is ``writing data to a file'':

\begin{verbatim}
def write_record(filehandle, record):
  # ... writing data ...
  # (while this function-body here is being processed,
  # the file opened by the caller is open, and will
  # have to be closed even if an error arises here that
  # aborts further processing of the caller's function-body.)

def write_all_records(filename, records):
  with open(filename, 'wt') as h_out:
    for record in records:
      write_record(h_out, record)
\end{verbatim}
  \item
    With TensorFlow1, there is (for example) a `session-context' that
    provides the association with some hardware that can do a numerical
    calculation. With TensorFlow2, a ``gradient-tape'' is generally
    handled by such dynamic context.
  \end{itemize}
\item
  ``Attribute resolution'' - ``where does a particular property of an
  object come from''?

  \begin{itemize}
  \item
    Example: The \texttt{Counter} class in the \texttt{collections}
    module is a subclass of \texttt{dict}.
    \texttt{collections.Counter({[}1,\ 1,\ 1,\ 2,\ 2{]}).items()} uses
    the \texttt{items} attribute provided by the \texttt{dict}
    superclass, but calling the \texttt{.most\_common()} method uses the
    attribute provided by the \texttt{Counter} class.
  \item
    Object-oriented programming is a lot about using class instances
    (``objects'') as a systematic approach to state-management.
  \item
    As explained earlier, for maths-oriented code, where the fundamental
    notions are related to ``properties of value-to-value
    transformations'', we can get quite far not concerning ourselves
    with OO at all - ``since there is basically no state in need of
    management''. (This will be mostly our perspective for this course.)
  \end{itemize}
\end{itemize}

\hypertarget{how-python-code-gets-executed}{%
\subsubsection{How Python code gets
executed}\label{how-python-code-gets-executed}}

An early ``caveat'' is in order here: a TensorFlow2
\texttt{@tf.function} decorated function looks like Python code but
actually is something very different. We will come back to that.

A basic working model of Python code execution is:

\begin{itemize}
\item
  A \texttt{def\ xyz(...):} statement introduces a variable \texttt{xyz}
  whose value is a callable. The part in parentheses specifies
  parameters, and the indented lines below are the function's body,
  which may start with a docstring that is handled in a special way
  (made visible to tools).
\item
  If a callable gets called, \texttt{x\ =\ f(p,\ q)}, the parameters are
  first evaluated in left-to-right order. Then, a new ``execution
  frame'' (/ ``stack frame'') gets created, a ``workspace'' where the
  variables specified as parameters in the callable definition come to
  life, get assigned the provided parameters, and stay alive until
  execution of the body completes, either via:

  \begin{itemize}
  \tightlist
  \item
    Processing body-statements reaches the end of the callable's
    body\ldots{}
  \item
    \ldots or an explicit \texttt{return} statement\ldots{}
  \item
    \ldots or a raised `exception' aborts execution of the body.
  \end{itemize}
\item
  ``The execution frame is the in-memory representation of the workspace
  that belongs to this particular call'': It is \emph{not} a property of
  the callable-object, but a property of the call(!).
\item
  Body statements get executed one-after-another. Some statements may
  have one or multiple (indented) blocks of sub-statements. This
  includes: looping statements, conditionals (``if''),
  ``with''-context-statements, etc.
\item
  While a callable's body executes, variables refer to this ``execution
  frame'' - if they are found there. We have to discriminate two cases:

  \begin{itemize}
  \tightlist
  \item
    Assignment-to-a-name will assign to an existing variable with that
    name in the current execution frame, if it already existed, or
    introduce a new name on the current frame.
  \item
    Evaluating-a-name (as an expression) will evaluate to the variable
    with that name in the current execution frame, if it exists.
    Otherwise, it will try to resolve the name against the
    variable-bindings that were in use at the time / in the context the
    current function was defined.
  \item
    There are ways to fine-tune the rules about inclusion of specific
    variables from outer frames, via the \texttt{global} and
    \texttt{nonlocal} keywords.
  \end{itemize}
\item
  Unlike many functional languages (and even TeX), CPython does not
  perform ``tail call optimization''. (Basically, if recursive calls
  only occur in the form
  \texttt{return\ recursive\_call(...parameters...)}, i.e.~we would only
  ever pass through the output from the recursive call to our own
  caller, it is possible to instead transform the code to clear the
  current call-frame and hard-jump into the called function's code,
  making its return our own return. Implementation-wise, it is however
  more appropriate to think of this in terms of ``representing''return''
  as a function-call and passing it into the continuation-function,
  which may or may not be recursing. Python does not do this.)

  Instead, there is actually a (changeable) limit on how deeply function
  calls can be nested - given by \texttt{sys.getrecursionlimit()}
  (typically 1000 or so).
\end{itemize}

In comparison to other languages, Python is a bit unusual in that
variables generally are not declared, but get introduced by simply
assigning to them. This mostly is OK, but there are some strange-looking
cases to be aware of. This code block illustrates the weirdness:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x} \PY{o}{=} \PY{l+m+mi}{10}

\PY{k}{def} \PY{n+nf}{funny1}\PY{p}{(}\PY{n}{a}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n}{x} \PY{o}{+} \PY{n}{a}  \PY{c+c1}{\PYZsh{} x refers to the \PYZdq{}outer x\PYZdq{} above.}


\PY{k}{def} \PY{n+nf}{funny2}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{do\PYZus{}assign}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
  \PY{k}{if} \PY{n}{do\PYZus{}assign}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{b}
  \PY{k}{return} \PY{n}{x} \PY{o}{+} \PY{n}{c}


\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{funny1(7) evaluates to:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{funny1}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{funny2(2, 30, do\PYZus{}assign=True) evaluates to:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{funny2}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{,} \PY{n}{do\PYZus{}assign}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{module\PYZhy{}global x now is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Executing this actually would raise an UnboundLocalError exception (try it!):}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{} funny2(2, 50, do\PYZus{}assign=False)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
funny1(7) evaluates to: 17
funny2(2, 30, do\_assign=True) evaluates to: 32
module-global x now is: 10
    \end{Verbatim}

    If, in the above code, we commented out the
\texttt{if\ do\_assign:\ x\ =\ b} part, then \texttt{x} would refer to
the global \texttt{x}, as in \texttt{funny1}.

Here, however, even in a call with \texttt{do\_assign=False}, which does
not execute the \texttt{x=b} statement, the mere existence of such a
statement somewhere on the function body makes \texttt{x} a name that
belongs to the local frame and is not to be resolved from an outer frame
(this makes x a ``local name''). But then, in the
\texttt{return\ x\ +\ c} line, we try to obtain its value without ever
having set that variable, so it is an ``unbound local''.

    Importantly, Python supports ``lexical closures'', so if a function is
introduced (via a \texttt{def} or a \texttt{lambda}) in such a way that
its body refers to a variable in use on the execution frame on which the
\texttt{def} or \texttt{lambda} occurred, then the so-defined function
retains a tie to the variable in outer scope. So, we can do this:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} These two functions make\PYZus{}adder\PYZus{}v1 and make\PYZus{}adder\PYZus{}v2 are effectively equivalent:}
\PY{k}{def} \PY{n+nf}{make\PYZus{}adder\PYZus{}v1}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x} \PY{o}{+} \PY{n}{k}

\PY{k}{def} \PY{n+nf}{make\PYZus{}adder\PYZus{}v2}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
  \PY{k}{def} \PY{n+nf}{add\PYZus{}k}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{x} \PY{o}{+} \PY{n}{k}
  \PY{k}{return} \PY{n}{add\PYZus{}k}


\PY{c+c1}{\PYZsh{} Examples}
\PY{n}{add7v1} \PY{o}{=} \PY{n}{make\PYZus{}adder\PYZus{}v1}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}
\PY{n}{add7v2} \PY{o}{=} \PY{n}{make\PYZus{}adder\PYZus{}v2}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}

\PY{c+c1}{\PYZsh{} There is a tiny difference in how these function\PYZhy{}objects print:}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{add7v1}\PY{p}{,} \PY{n}{add7v2}\PY{p}{)}

\PY{c+c1}{\PYZsh{} ...but not in their behaviour:}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{add7v1}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{add7v2}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<function make\_adder\_v1.<locals>.<lambda> at 0x7a4d22f39bd0> <function
make\_adder\_v2.<locals>.add\_k at 0x7a4d22f39b40>
17 17
    \end{Verbatim}

    The important thing to remember is: If control flow reaches a
\texttt{lambda} or crosses a \texttt{def}, this creates a
function-object which retains ties to variables it uses that come from
the outer lexical scope - either the immediate frame within which the
function was defined, or some outer lexical frame of that frame.

We have to be careful here - such
functions-referring-to-context-variables are basically the one place
where ``variables show up as having a life as stateful entities''. We
can do this\ldots:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{make\PYZus{}adder\PYZus{}reporter}\PY{p}{(}\PY{n}{initial}\PY{p}{)}\PY{p}{:}
  \PY{n}{cell} \PY{o}{=} \PY{p}{[}\PY{n}{initial}\PY{p}{]}
  \PY{k}{def} \PY{n+nf}{report}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{cell}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{def} \PY{n+nf}{add}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{cell}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{cell}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{x}
  \PY{k}{return} \PY{p}{(}\PY{n}{add}\PY{p}{,} \PY{n}{report}\PY{p}{)}

\PY{p}{(}\PY{n}{the\PYZus{}adder}\PY{p}{,} \PY{n}{the\PYZus{}reporter}\PY{p}{)} \PY{o}{=} \PY{n}{make\PYZus{}adder\PYZus{}reporter}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{the\PYZus{}reporter}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{the\PYZus{}adder}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{the\PYZus{}reporter}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{the\PYZus{}adder}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{the\PYZus{}reporter}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
1000
1010
1015
    \end{Verbatim}

    \ldots and this is an excellent way to get bitten badly:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{make\PYZus{}multipliers}\PY{p}{(}\PY{n}{start}\PY{p}{,} \PY{n}{end}\PY{p}{)}\PY{p}{:}
  \PY{n}{multipliers} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{start}\PY{p}{,} \PY{n}{end}\PY{p}{)}\PY{p}{:}
    \PY{n}{multipliers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{k} \PY{o}{*} \PY{n}{x}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{multipliers}

\PY{n}{the\PYZus{}multipliers} \PY{o}{=} \PY{n}{make\PYZus{}multipliers}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}

\PY{k}{for} \PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{multiplier}\PY{p}{)} \PY{o+ow}{in} \PY{n}{the\PYZus{}multipliers}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{, multiplier(10):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{multiplier}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Let us compare this to...}


\PY{k}{def} \PY{n+nf}{make\PYZus{}multipliers\PYZus{}v2}\PY{p}{(}\PY{n}{start}\PY{p}{,} \PY{n}{end}\PY{p}{)}\PY{p}{:}
  \PY{n}{multipliers} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}multiplier\PYZus{}func}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{k} \PY{o}{*} \PY{n}{x}
  \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{start}\PY{p}{,} \PY{n}{end}\PY{p}{)}\PY{p}{:}
    \PY{n}{multipliers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{get\PYZus{}multiplier\PYZus{}func}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{multipliers}

\PY{n}{the\PYZus{}multipliers2} \PY{o}{=} \PY{n}{make\PYZus{}multipliers\PYZus{}v2}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{for} \PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{multiplier}\PY{p}{)} \PY{o+ow}{in} \PY{n}{the\PYZus{}multipliers2}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{, multiplier(10):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{multiplier}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}


\PY{c+c1}{\PYZsh{} The difference is of course that in the 2nd case, the multiplier\PYZhy{}function}
\PY{c+c1}{\PYZsh{} holds on to a parameter\PYZhy{}variable `k` introduced on the outer execution\PYZhy{}frame}
\PY{c+c1}{\PYZsh{} of a function that received the value of the loop\PYZhy{}iterator variable and}
\PY{c+c1}{\PYZsh{} bound it to its own (owned\PYZhy{}by\PYZhy{}the\PYZhy{}call\PYZhy{}frame!) cell named `k`.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} In the 1st case, every multiplier\PYZhy{}function references the same variable,}
\PY{c+c1}{\PYZsh{} which is the loop\PYZhy{}iteration variable, so it keeps changing its value during}
\PY{c+c1}{\PYZsh{} iteration \PYZhy{} and once iteration is finished, retains the final value.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
k: 5 , multiplier(10): 90
k: 6 , multiplier(10): 90
k: 7 , multiplier(10): 90
k: 8 , multiplier(10): 90
k: 9 , multiplier(10): 90
\#\#\#\#\#\#
k: 5 , multiplier(10): 50
k: 6 , multiplier(10): 60
k: 7 , multiplier(10): 70
k: 8 , multiplier(10): 80
k: 9 , multiplier(10): 90
    \end{Verbatim}

    \hypertarget{useful-things-to-know}{%
\subsection{Useful things to know}\label{useful-things-to-know}}

The above descriptions explained the skeletal structure of Python.

Looking forward to the rest of this course, there are some things that
deserve special attention, since they can save us a lot of work that
otherwise would be very tedious.

    \hypertarget{n-index-arrays-in-their-various-incarnations}{%
\subsubsection{N-index Arrays (in their various
incarnations)}\label{n-index-arrays-in-their-various-incarnations}}

We already touched the \texttt{numpy.ndarray} ``numerical \(n\)-index
array'' data type. The TensorFlow \texttt{tf.Tensor} and also the JAX
\texttt{jax.numpy.ndarray} class are closely modeled after this, but
``come with extra strings attached that make computing fast gradients
simple''.

Indeed, the important characteristics of a ``TensorFlow Tensor object''
are much less about tensor arithmetics than they are about ``strings
attached that associate this object with its role in a calculation,
described by a tensor-arithmetic graph''.

Many ML practitioners try to ``visualize'' 3+ index tensors as
``higher-dimensional coefficient schemes'' that extend the ``0d=scalar -
1d=vector - 2d=matrix'' chain. This is in general mostly a useless
distraction. An overall more useful perspective is to regard any such
``n-index tensor/array'' as a ``table'', which associates a particular
tuple of indices with a value. So, overall, these ``tensors'' are
conceptually much closer to tables in a relational database (such as a
SQL database) than to ``cubes and hypercubes of numbers''. That is
actually also what we do in physics.

The following code cell takes us through a quick tour of some common
\texttt{numpy.ndarray} operations - using some familiar tensors. The
\texttt{tf.Tensor} and \texttt{jax.numpy.ndarray} classes typically
provide very similar functionality, often using the same names.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy}   \PY{c+c1}{\PYZsh{} Loading the `numpy` module.}
\PY{k+kn}{import} \PY{n+nn}{pprint}  \PY{c+c1}{\PYZsh{} Also loading the \PYZdq{}pretty\PYZhy{}printing\PYZdq{} module.}

\PY{n}{pauli\PYZus{}matrices} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}
    \PY{p}{[}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} 1}
     \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}
     \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} sigma\PYZus{}x}
     \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}
     \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{n}{j}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mi}{1}\PY{n}{j}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} sigma\PYZus{}y}
     \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}
     \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} sigma\PYZus{}z}
    \PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{complex}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Rather than defining the gamma matrices in a simple and uniform fashion,}
\PY{c+c1}{\PYZsh{} let us showcase a few different approaches to set these up.}
\PY{c+c1}{\PYZsh{} With numpy.ndarray, the focus sometimes is on mutation, but tf.Tensor}
\PY{c+c1}{\PYZsh{} and jax.numpy.ndarray are immutable, so here we also de\PYZhy{}emphasize mutation}
\PY{c+c1}{\PYZsh{} operations, favoring \PYZdq{}mathematical definitions\PYZdq{}}

\PY{n}{gamma0} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{gamma1} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}  \PY{c+c1}{\PYZsh{} \PYZdq{}reverse\PYZhy{}ordered rows\PYZdq{}.}
\PY{n}{gamma2} \PY{o}{=} \PY{l+m+mi}{1}\PY{n}{j} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
\PY{n}{gamma3} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AB,ab\PYZhy{}\PYZgt{}AaBb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{n}{j} \PY{o}{*} \PY{n}{pauli\PYZus{}matrices}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
                      \PY{n}{pauli\PYZus{}matrices}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}

\PY{n}{gammas} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{gamma0}\PY{p}{,} \PY{n}{gamma1}\PY{p}{,} \PY{n}{gamma2}\PY{p}{,} \PY{n}{gamma3}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{gamma5} \PY{o}{=} \PY{n}{gamma0} \PY{o}{@} \PY{n}{gamma1} \PY{o}{@} \PY{n}{gamma2} \PY{o}{@} \PY{n}{gamma3}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== gammas ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{gammas}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma5:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma5}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{gamma\PYZus{}mu\PYZus{}nu} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{amn,bnp\PYZhy{}\PYZgt{}abmp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gammas}\PY{p}{,} \PY{n}{gammas}\PY{p}{)}
\PY{n}{clifford\PYZus{}rhs} \PY{o}{=} \PY{n}{gamma\PYZus{}mu\PYZus{}nu} \PY{o}{+} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{abmn\PYZhy{}\PYZgt{}bamn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma\PYZus{}mu\PYZus{}nu}\PY{p}{)}
\PY{n}{eta\PYZus{}mu\PYZus{}nu} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{k}{assert} \PY{n}{numpy}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{clifford\PYZus{}rhs}\PY{p}{,}
                      \PY{c+c1}{\PYZsh{} This uses \PYZdq{}broadcasting\PYZdq{} \PYZhy{} indexing with a}
                      \PY{c+c1}{\PYZsh{} `numpy.newaxis` index adds an extra range\PYZhy{}1 index}
                      \PY{c+c1}{\PYZsh{} to a n\PYZhy{}index array in the corresponding place,}
                      \PY{c+c1}{\PYZsh{} and operations such as element\PYZhy{}wise(!) multiplication}
                      \PY{c+c1}{\PYZsh{} of a tensor with another \PYZdq{}repeat along the range\PYZhy{}1}
                      \PY{c+c1}{\PYZsh{} indices\PYZdq{}. The product in the two lines below}
                      \PY{c+c1}{\PYZsh{} is the same as}
                      \PY{c+c1}{\PYZsh{} `einsum(\PYZsq{}ab,cd\PYZhy{}\PYZgt{}abcd\PYZsq{}, 2*eta\PYZus{}mu\PYZus{}nu, eye(4))`, where}
                      \PY{c+c1}{\PYZsh{} `eye(.)` returns an identity matrix.}
                      \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{eta\PYZus{}mu\PYZus{}nu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]} \PY{o}{*}
                      \PY{n}{numpy}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{p}{(}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Clifford algebra has a bug.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{P\PYZus{}L} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{gamma5}\PY{p}{)}
\PY{n}{P\PYZus{}R} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)} \PY{o}{+} \PY{n}{gamma5}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== tr(gamma\PYZus{}mu gamma\PYZus{}nu) ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{abmm\PYZhy{}\PYZgt{}ab}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma\PYZus{}mu\PYZus{}nu}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
=== gammas ===
[[[(1+0j), 0j, 0j, 0j],
  [0j, (1+0j), 0j, 0j],
  [0j, 0j, (-1+0j), 0j],
  [0j, 0j, 0j, (-1+0j)]],
 [[0j, 0j, 0j, (1+0j)],
  [0j, 0j, (1+0j), 0j],
  [0j, (-1+0j), 0j, 0j],
  [(-1+0j), 0j, 0j, 0j]],
 [[0j, 0j, 0j, (-0-1j)],
  [0j, 0j, 1j, 0j],
  [0j, 1j, 0j, 0j],
  [(-0-1j), 0j, 0j, 0j]],
 [[0j, 0j, (-1+0j), 0j],
  [0j, 0j, 0j, (1+0j)],
  [(1+0j), 0j, 0j, 0j],
  [0j, (-1+0j), 0j, 0j]]]
gamma5: [[0j, 0j, 1j, 0j], [0j, 0j, 0j, 1j], [1j, 0j, 0j, 0j], [0j, 1j, 0j, 0j]]
=== tr(gamma\_mu gamma\_nu) ===
[[(4+0j), 0j, 0j, 0j],
 [0j, (-4+0j), 0j, 0j],
 [0j, 0j, (-4+0j), 0j],
 [0j, 0j, 0j, (-4+0j)]]
    \end{Verbatim}

    \hypertarget{python-as-a-query-language}{%
\subsubsection{``Python as a Query
Language''}\label{python-as-a-query-language}}

The ``machine oriented'' approach to programming tends to focus on
questions such as ``where is this value stored?'' and ``what parts of
the program have permission to mutate it?''.

Mutations play a big role, but unfortunately do not nicely align with
how we normally structure analysis of complicated relations in
mathematics - since in maths, there generally is no notion of ``x was
defined as follows, and under these specific circumstances, we
henceforth change its meaning to mean z''.

So, when we have to reason about (perhaps even prove theorems about)
code containing mutations (``imperative programs''), we need some heavy
handed and unwieldy machinery. (A widely used grad course textbook on
the subject is Gougen \& Malcolm,
\href{https://books.google.ch/books/about/Algebraic_Semantics_of_Imperative_Progra.html?id=oeorYZSbVcIC\&source=kp_book_description\&redir_esc=y}{Algebraic
Semantics of Imperative Programs} \cite{goguen1996algebraic}.)

Overall, we are usually in a better position if we align code structure
with a constructive mathematical construction of our objective, written
up the way we usually write up maths. While this here is purely a matter
of improving understandability, the same point also has emerged in
research on compilers and microprocessors. Even down at the
microprocessor level, if - for example - machine code says to store an
intermediate quantity in register R7, but this register is used in
machine code to hold another - completely independent - intermediate
quantity later, calculations that could be done concurrently nominally
cannot because they want to use the same storage location for an
intermediate result. So, modern microprocessors have
\href{https://en.wikipedia.org/wiki/Register_renaming}{``register
renaming''} logic to eliminate such ``false data dependencies''. At the
compiler level, human-written code first gets parsed (we have seen how
that works), and then translated into some intermediate representation
whose `variables' are define-once-read-only and never get reassigned to
-
``\href{https://en.wikipedia.org/wiki/Static_single_assignment_form}{SSA
form}''. Again, the point is that such code is easier to reason about,
in particular for exploring opportunities to apply
performance-increasing transformations.

Overall, list/set/dict comprehensions and also generator expressions are
very powerful Python language features that allow us ``to focus on the
What and not the How''.

Let us look at the following piece of Python code. This ``inverts a
mapping specified by a table'' (which here must be injective).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{inverted\PYZus{}mapping}\PY{p}{(}\PY{n}{table}\PY{p}{)}\PY{p}{:}
  \PY{n}{result} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
  \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o+ow}{in} \PY{n}{table}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{result}\PY{p}{[}\PY{n}{value}\PY{p}{]} \PY{o}{=} \PY{n}{key}
  \PY{k}{return} \PY{n}{result}

\PY{n}{name\PYZus{}by\PYZus{}digit} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{one}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{two}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{three}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{four}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{five}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+m+mi}{6}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{six}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seven}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nine}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{zero}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}

\PY{n}{digit\PYZus{}by\PYZus{}name} \PY{o}{=} \PY{n}{inverted\PYZus{}mapping}\PY{p}{(}\PY{n}{name\PYZus{}by\PYZus{}digit}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{digit\PYZus{}by\PYZus{}name}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7,
'eight': 8, 'nine': 9, 'zero': 0\}
    \end{Verbatim}

    The \texttt{inverted\_mapping} function's body is imperative: ``First,
create a new empty-dictionary object, which will be turned into the
result by mutation operations. Then, loop over the key-value pairs of
the input dictionary, and for every pair, add a new entry to the
result-dictionary, with the role of key and value swapped. Finally,
return the result'' - one command after another.

The alternative forms below use special Python syntax - a ``dictionary
comprehension'' - to express this idea in a way that can be read without
keeping track of intermediate state.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{inverted\PYZus{}mapping\PYZus{}v2} \PY{o}{=} \PY{k}{lambda} \PY{n}{table}\PY{p}{:} \PY{p}{\PYZob{}}\PY{n}{value}\PY{p}{:} \PY{n}{key} \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o+ow}{in} \PY{n}{table}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}


\PY{c+c1}{\PYZsh{} Equivalently, with `def`:}
\PY{k}{def} \PY{n+nf}{inverted\PYZus{}mapping\PYZus{}v3}\PY{p}{(}\PY{n}{table}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{\PYZob{}}\PY{n}{value}\PY{p}{:} \PY{n}{key} \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o+ow}{in} \PY{n}{table}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    There are some extra subtleties here, such as ``what guarantees does
Python give about the order in which \texttt{table.items()} produces the
key-value pairs from the dictionary - and what then are the
corresponding guarantees for the result dictionary?''. However, in those
many situations where we need not concern ourselves with such aspects,
we can simply forget about them and focus on what we need to reason
about - just as it should be!

On the general topic of ``reasoning about the behavior of Python code'',
there unfortunately are some design warts that make this more
complicated than it ought to be, such as
\texttt{sys.getrecursionlimit()} generally spoiling theorems about code
with necessary qualifications such as ``form A is equivalent to form B,
unless evaluating form B breaks the recursion-depth limit''. The author
still has hope that Python can evolve in the direction of making it
easier to reason about code by eliminating such doubtful design
decisions, but this will only happen if the academic community (and also
the computer security community) makes a strong point about how
important this is!

This approach - focusing on the logic of computations rather than the
steps - is known as
``\href{https://en.wikipedia.org/wiki/Declarative_programming}{declarative
programming}''. Python offers quite a few tools that can be used to
great advantage especially for those parts of a program that are not
about ``number-crunching'', including of course ``getting the data'',
and ``wiring up the components of a larger application''.

The code cell below illustrates this with a few examples.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Sum\PYZhy{}of\PYZhy{}divisors of a number (brute\PYZhy{}force) \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{sum\PYZus{}of\PYZus{}divisors}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{d} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)} \PY{k}{if} \PY{n}{n} \PY{o}{\PYZpc{}} \PY{n}{d} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}

\PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{2}\PY{o}{*}\PY{l+m+mi}{248}\PY{p}{,} \PY{l+m+mi}{8128}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{sum\PYZus{}of\PYZus{}divisors}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Sorting version\PYZhy{}number strings by release\PYZhy{}order \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{linux\PYZus{}kernel\PYZus{}versions} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4.20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                         \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5.4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.6.11}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.6.39}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{release\PYZus{}order\PYZus{}sorted\PYZus{}version\PYZus{}strings}\PY{p}{(}\PY{n}{versions}\PY{p}{)}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} When sorting an iterable, we can optionally specify a key\PYZhy{}function.}
  \PY{c+c1}{\PYZsh{} If this is provided, elements are sorted according to monotonically}
  \PY{c+c1}{\PYZsh{} increasing value of the key\PYZhy{}function. Magnitude\PYZhy{}comparison on tuples}
  \PY{c+c1}{\PYZsh{} and also on lists uses lexicographic ordering.}
  \PY{k}{return} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{versions}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{v}\PY{p}{:} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{v}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{release\PYZus{}order\PYZus{}sorted\PYZus{}version\PYZus{}strings}\PY{p}{(}\PY{n}{linux\PYZus{}kernel\PYZus{}versions}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Finding integers \PYZgt{}0 that are not a sum of k\PYZgt{}1 consecutive }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{integers \PYZgt{}0 \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{itertools}

\PY{k}{def} \PY{n+nf}{numbers\PYZus{}that\PYZus{}are\PYZus{}not\PYZus{}a\PYZus{}sum\PYZus{}of\PYZus{}consecutive\PYZus{}numbers}\PY{p}{(}\PY{n}{limit}\PY{p}{)}\PY{p}{:}
  \PY{n}{range\PYZus{}sum} \PY{o}{=} \PY{k}{lambda} \PY{n}{n}\PY{p}{:} \PY{n}{n} \PY{o}{*} \PY{p}{(}\PY{n}{n} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}  \PY{c+c1}{\PYZsh{} `//` is int/int\PYZhy{}\PYZgt{}int division.}
  \PY{n}{sums\PYZus{}of\PYZus{}consecutive\PYZus{}numbers} \PY{o}{=} \PY{p}{\PYZob{}}  \PY{c+c1}{\PYZsh{} set\PYZhy{}comprehension!}
      \PY{n}{range\PYZus{}sum}\PY{p}{(}\PY{n}{upper}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{range\PYZus{}sum}\PY{p}{(}\PY{n}{lower} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
      \PY{c+c1}{\PYZsh{} Note: itertools.combinations((\PYZsq{}a\PYZsq{}, \PYZsq{}b\PYZsq{}, \PYZsq{}c\PYZsq{}), 2) \PYZhy{}\PYZgt{}}
      \PY{c+c1}{\PYZsh{} generator yielding elements: (\PYZsq{}a\PYZsq{}, \PYZsq{}b\PYZsq{}), (\PYZsq{}a\PYZsq{}, \PYZsq{}c\PYZsq{}), (\PYZsq{}b\PYZsq{}, \PYZsq{}c\PYZsq{}).}
      \PY{k}{for} \PY{n}{lower}\PY{p}{,} \PY{n}{upper} \PY{o+ow}{in} \PY{n}{itertools}\PY{o}{.}\PY{n}{combinations}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{limit} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
  \PY{p}{\PYZcb{}}
  \PY{k}{return} \PY{n+nb}{set}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{limit}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{sums\PYZus{}of\PYZus{}consecutive\PYZus{}numbers}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{numbers\PYZus{}that\PYZus{}are\PYZus{}not\PYZus{}a\PYZus{}sum\PYZus{}of\PYZus{}consecutive\PYZus{}numbers}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Brute\PYZhy{}force counting involutions \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{pprint}

\PY{k}{def} \PY{n+nf}{number\PYZus{}of\PYZus{}involutions}\PY{p}{(}\PY{n}{permutation\PYZus{}length}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n+nb}{sum}\PY{p}{(}\PY{n+nb}{all}\PY{p}{(}\PY{n}{p}\PY{p}{[}\PY{n}{p}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{]} \PY{o}{==} \PY{n}{k} \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{permutation\PYZus{}length}\PY{p}{)}\PY{p}{)}
             \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{itertools}\PY{o}{.}\PY{n}{permutations}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{permutation\PYZus{}length}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{n}\PY{p}{:} \PY{n}{number\PYZus{}of\PYZus{}involutions}\PY{p}{(}\PY{n}{n}\PY{p}{)} \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Let\PYZsq{}s try to do something OS related using this approach.}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} All running processes on the current VM that have the same }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{parent process as the current process \PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{glob}\PY{o}{,} \PY{n+nn}{re}\PY{o}{,} \PY{n+nn}{os}

\PY{k}{def} \PY{n+nf}{all\PYZus{}processes\PYZus{}with\PYZus{}this\PYZus{}parent\PYZus{}pid}\PY{p}{(}\PY{n}{wanted\PYZus{}ppid}\PY{p}{)}\PY{p}{:}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}name\PYZus{}and\PYZus{}pid\PYZus{}and\PYZus{}ppid}\PY{p}{(}\PY{n}{pid}\PY{p}{)}\PY{p}{:}
    \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/proc/}\PY{l+s+si}{\PYZob{}}\PY{n}{pid}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{/status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h}\PY{p}{:}
      \PY{n}{match} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(?sm)Name:}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{s*(}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{S*).*?\PYZca{}PPid:}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{s*(}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{S+)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{h}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}\PY{p}{)}
      \PY{k}{return} \PY{p}{(}\PY{n}{match}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pid}\PY{p}{,} \PY{n+nb}{int}\PY{p}{(}\PY{n}{match}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n+nb}{sorted}\PY{p}{(}\PY{p}{(}\PY{n}{name}\PY{p}{,} \PY{n}{pid}\PY{p}{)}
                \PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{pid}\PY{p}{,} \PY{n}{ppid} \PY{o+ow}{in} \PY{n+nb}{map}\PY{p}{(}\PY{n}{get\PYZus{}name\PYZus{}and\PYZus{}pid\PYZus{}and\PYZus{}ppid}\PY{p}{,}
                                           \PY{p}{(}\PY{n}{d} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/proc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                                            \PY{k}{if} \PY{n}{d}\PY{o}{.}\PY{n}{isdigit}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                \PY{k}{if} \PY{n}{ppid} \PY{o}{==} \PY{n}{wanted\PYZus{}ppid}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{all\PYZus{}processes\PYZus{}with\PYZus{}this\PYZus{}parent\PYZus{}pid}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{getppid}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} (It so turns out that the current process has no siblings. Let\PYZsq{}s check.)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{err}{!}\PY{n}{pstree} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{ascii}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

\#\#\# Sum-of-divisors of a number (brute-force) \#\#\#
3 1
4 3
5 1
6 6
7 1
8 7
9 4
10 8
28 28
496 496
8128 8128

\#\#\# Sorting version-number strings by release-order \#\#\#
['2.0', '2.2', '2.4', '2.6', '2.6.11', '2.6.39', '4.3', '4.20', '5.4']

\#\#\# Finding integers >0 that are not a sum of k>1 consecutive integers >0 \#\#\#
\{32, 1, 2, 64, 4, 128, 256, 512, 8, 16\}

\#\#\# Brute-force counting involutions \#\#\#
\{0: 1, 1: 1, 2: 2, 3: 4, 4: 10, 5: 26, 6: 76, 7: 232, 8: 764, 9: 2620\}

\#\#\# All running processes on the current VM that have the same parent process as
the current process \#\#\#
[('python3', '1795')]
---
docker-init-+-node-+-colab-fileshim.
            |      |-dap\_multiplexer---4*[\{dap\_multiplexer\}]
            |      |-jupyter-noteboo-+-python3-+-pstree
            |      |                 |         `-13*[\{python3\}]
            |      |                 `-6*[\{jupyter-noteboo\}]
            |      |-language\_servic-+-node---7*[\{node\}]
            |      |                 `-7*[\{language\_servic\}]
            |      |-oom\_monitor.sh---sleep
            |      |-python3
            |      `-10*[\{node\}]
            |-python3---6*[\{python3\}]
            `-run.sh---kernel\_manager\_---4*[\{kernel\_manager\_\}]
    \end{Verbatim}

    \hypertarget{general-principles-for-good-code-readability-and-maintainability}{%
\subsubsection{General principles for good code readability and
maintainability}\label{general-principles-for-good-code-readability-and-maintainability}}

The principles spelled out below may sound self-evident, but there is a
lot of code around that violates them. By making ``sticking to these
principles a habit'', one can do a lot for one's code quality and
maintainability.

\begin{itemize}
\item
  The documentation of an API (such as: a function to be used by others)
  is an integral part of the API, and considered to be ``a binding
  contract'' - ``if you use this thing as follows, then that is the
  guaranteed behavior.''

  \begin{itemize}
  \tightlist
  \item
    Uses of the API must not violate the contract.
  \item
    It is not OK to rely on accidental implementation details that are
    not part of the documented API contract.
  \item
    A subclass contract can refine but must not override parts of a
    parent class contract.
  \item
    Contracts must not contradict themselves.
  \end{itemize}
\item
  Error-checks must match error messages precisely! No ``Checking one
  thing and reporting something different.''
\item
  Never change global state you do not own. (Such as: re-seeding a
  global number generator, changing floating point rounding mode, etc.)
  Do not use global state in your designs, unless absolutely
  unavoidable.
\item
  Constrain side effects and state-mutation to the narrowest possible
  context - whatever can be described and coded out as a simple
  value-transformation ``that feels mathsy'' should be written that way.
\item
  Code comments should not repeat what the code already says, but are
  very appropriate to explain relevant subtleties.
\item
  Simple questions about the code should have simple answers.
  (Especially: ``What's this variable?'')
\item
  Spend some effort on naming things properly.
\item
  Think about possible sources of concept-confusion for the reader - and
  eliminate them with well-designed terminology.

  (This includes for example: Don't ever call a 20x20 matrix
  ``two-dimensional''!)
\item
  No ``hope and pray'' coding that uses checks/properties which
  generally hold, despite having counterexamples.
\item
  No ``inferring properties from examples only'' - both when writing
  code and when documenting APIs.
\item
  Think about the relevant (``natural'') invariants that the concept
  entities your design is introducing should satisfy.
\item
  Make everything reproducible wherever you can! If you ever are in a
  situation of ``this result that here showed up by chance is very
  remarkable'', and your computer crashes the next minute, you want to
  be able to reproduce that ``chance discovery''!
\item
  Design should be explainable -``everything should be there for a good
  reason'' (and we should at least be able to articulate that reason why
  the design decision makes sense).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Importantly, ``designing with mathematical transformations as key
entities'' and ``keeping state management minimal'' also helps a lot
with making code testable!

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

One of the currently most under-valued and under-appreciated Software
Engineering skills (relative to its importance) is ``defining the
language to talk about the (typically, complicated) problem''. This
``language'' may consist of classes, functions, other code parts, but
also - very importantly - new terms you invented to describe the design
in its documentation. A typical solid Computer Science PhD thesis is
about ``defining a large language that includes natural-language
terminology and typically also code definitions to make a relevant real
world problem manageable''.

When working on a complicated project, it is perfectly natural to start
out with a rough mental model where some ideas are off - there may be
notions of some important concepts and aspects, but their role may be
under-appreciated at first, or ``otherwise a bit off''. (Example: in a
concurrent database, should operations have event-timestamps, or be
associated with time-intervals over which the operation will have
affected database state? Or: Are human-readable error messages
appropriate here? If something else processes these, and then expects
some specific originally-intended-for-humans text, there are issues with
localization (translating to other languages), and also issues with even
just fixing typos. Should the design be adjusted to provide both an
error-message-for-humans and also error-code-for-programs?)

The important competency is to ``make progress by exploring and playing
with the subject matter clay, shaping things in different ways - and
throwing away many design ideas, progressing towards more appropriate
notions that manage to capture the key aspects in a useful way''.

It is natural and expected that, during this ``playing with the clay''
process of language-finding, we explore a lot and throw away a lot. In
that situation, it matters a lot to clearly document the role of every
piece of code - there is a world of a difference between code that
describes itself as ``this is merely for exploring a weird idea, and is
kept here to document how far we got and what problems we ran into'' and
code that describes itself as ``this is a software component that was
diligently engineered to for-use-in-production quality''.

``\textbf{Excellent engineering is mostly language design}'' -
introducing a suitable way to talk about a problem, analyze it, share
insights with other experts, and explain how solutions are built, so
others can understand, use, and adjust them.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Participant Exercise}: What are relevant ``terminology
inventions'' in nuclear engineering that were invented to capture
relevant aspects of how radiation interacts with people, what were the
precursors of some of these modern terms, and what were the drawbacks
that were identified as a reason to replace them? How are product design
decisions tied to that terminology?

    \hypertarget{important-references}{%
\section{Important References}\label{important-references}}

\begin{itemize}
\tightlist
\item
  \href{https://docs.python.org/3/reference/}{Python Reference -
  https://docs.python.org/3/reference/}
\item
  \href{https://docs.python.org/3/reference/grammar.html}{Python Grammar
  - https://docs.python.org/3/reference/grammar.html}
\item
  \href{https://docs.python.org/3/reference/expressions.html\#operator-precedence}{Python
  Operator Precedence table:
  https://docs.python.org/3/reference/expressions.html\#operator-precedence}
\item
  \href{https://docs.python.org/3/library/}{Python Standard Library -
  https://docs.python.org/3/library/}
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{proposed-candidate-python-coding-exercises}{%
\section{Proposed Candidate Python Coding
Exercises}\label{proposed-candidate-python-coding-exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find a square number which is the sum of consecutive square numbers
  1+4+9+\ldots{}
\item
  Find an integer solution \((a, b)\in \mathbb{N}^2\) to
  \(13a^2+1=b^2\).
\item
  Implement a function that computes the number of ways to choose \(k\)
  elements out of \(n\) with replacement.
\item
  Implement a function, which for two pairs of spherical coordinates
  \((\vartheta, \phi)\) computes their angular distance. This should
  never fail, and produce numerically accurate results for small-angle
  distances.
\item
  Implement a function which, when given a collection of \texttt{k}
  (complex-float) matrices of shape \(N\times N\), determines if all
  commutators also lie in the span of these matrices, and if so,
  computes the structure constants \(f_{ab}{}^c\) satisfying
  \([M_a, M_b] = f_{ab}{}^c\, M_c\).
\item
  Implement a function which returns a (fairly sampled) random element
  of \(SO(d, \mathbb{Z})\).
\item
  Implement a function which, when given a
  \(\mathbb{R}^N\to \mathbb{R}^N\) function \(f\) plus its Jacobian, as
  a \(\mathbb{R}^N\to \mathbb{R}^{N\times N}\) function, finds a zero
  \(f\) via multidimensional Newton iteration.
\item
  Implement a function which, when given a (small) graph's adjacency
  matrix, determines the number of connected components.
\item
  Implement a function that maps a list of numbers to a function
  evaluating the polynomial with the given coefficients (using Horner's
  scheme).
\item
  Implement a function that determines whether a tuple of integer
  indices represents a permutation (i.e.~contains every integer from 0
  (included) up to tuple-length (excluded) exactly once).
\end{enumerate}

    \hypertarget{addendum-graph-structure-of-python-in-memory-objects}{%
\subsection{Addendum: Graph structure of Python in-memory
objects}\label{addendum-graph-structure-of-python-in-memory-objects}}

(These deeper details may be useful knowledge in some situations,
especially when dealing with foreign function interfaces, but can be
skipped on a ML-focused course if time is short.)

Every Python value has an underlying in-memory representation as a
\texttt{struct\ PyObject} instance. In general, in bytecode-interpreter
implementations of languages such as Perl or Python, it is not uncommon
to see the corresponding object-pointers to point to a struct-instance
that is the not-first element of a larger struct, so that there can be
extra in-memory bookkeeping information that sits right before the
object-instance. In general, functional languages typically use some
type-tagging scheme for representing values in-memory that avoids having
to go through a memory-reference for small integers, characters, and
other such simple objects, but this is not the case for the CPython
interpreter in version 3.11 or earlier.

Both for Perl and Python, in-memory objects use a reference-counting
approach to memory-management, which means that an object knows the
total number of times it is being referred to by in-memory objects, and
can be reclaimed when that count drops to zero - which means that the
object became provably-unreachable (given that the other code is correct
w.r.t. memory handling). Such a simple reference-counting approach
avoids the problem of requiring some sophisticated Garbage Collection
(as just about every functional programming language and also
e.g.~\texttt{Java}, \texttt{JavaScript}, \texttt{C\#} etc. have it), but
runs into problems when data structures contain circular references -
these can make objects unreclaimable. Since in a non-lazy language like
Python, circular references can only be produced by resorting to
object-mutation and we mostly can get away with data-transformation
focused code structure in ML, we will subsequently not have to concern
ourselves with in-memory representation details. Still, it should be
pointed out that if we were to liberally use object-mutation, this would
come with subtle potential pitfalls. Each of the definitions below
introduces a \texttt{4x4} matrix-like data structure, a list of four
lists of four identical floating point numbers. However, when mutating
the top-left element of each of these matrices by assigning to it, it
shows that in the second and third case, the list of row-lists
references identically the same row-object four times, so making a
mutation of that row-object affects what we see on every row-slot of the
matrix.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{m1} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{]}


\PY{n}{m2} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mf}{7.0}\PY{p}{,} \PY{l+m+mf}{7.0}\PY{p}{,} \PY{l+m+mf}{7.0}\PY{p}{,} \PY{l+m+mf}{7.0}\PY{p}{]}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{4}

\PY{n}{m3} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mf}{8.0}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{4}

\PY{n}{m4} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mf}{9.0}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{4} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]}

\PY{n}{m1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{2.0}
\PY{n}{m2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{2.0}
\PY{n}{m3}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{2.0}
\PY{n}{m4}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{2.0}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{} m1 \PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m1}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{} m2 \PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m2}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{} m3 \PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m3}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{} m4 \PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{m4}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
--- m1 ---
[[2.0, 5.0, 5.0, 5.0], [5.0, 5.0, 5.0, 5.0], [5.0, 5.0, 5.0, 5.0], [5.0, 5.0,
5.0, 5.0]]
--- m2 ---
[[2.0, 7.0, 7.0, 7.0], [2.0, 7.0, 7.0, 7.0], [2.0, 7.0, 7.0, 7.0], [2.0, 7.0,
7.0, 7.0]]
--- m3 ---
[[2.0, 8.0, 8.0, 8.0], [2.0, 8.0, 8.0, 8.0], [2.0, 8.0, 8.0, 8.0], [2.0, 8.0,
8.0, 8.0]]
--- m4 ---
[[2.0, 9.0, 9.0, 9.0], [9.0, 9.0, 9.0, 9.0], [9.0, 9.0, 9.0, 9.0], [9.0, 9.0,
9.0, 9.0]]
    \end{Verbatim}

    The following code cell (which is independent of the rest of this
notebook and can be copy-pasted and adjusted/extended independently)
contains a bare-bones skeleton of a graph-render of the memory object
reference structure of a composite Python value.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{err}{!}\PY{n}{apt} \PY{o}{\PYZhy{}}\PY{n}{qqq} \PY{n}{install} \PY{n}{graphviz}

\PY{k+kn}{import} \PY{n+nn}{io}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{import} \PY{n+nn}{re}
\PY{k+kn}{import} \PY{n+nn}{subprocess}
\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{k+kn}{import} \PY{n+nn}{time}
\PY{k+kn}{import} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display}


\PY{n}{\PYZus{}DOT\PYZus{}HEADER} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+s2}{digraph pyobject }\PY{l+s+s2}{\PYZob{}}
\PY{l+s+s2}{  fontname=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Palatino,Roboto,Helvetica}\PY{l+s+s2}{\PYZdq{}}
\PY{l+s+s2}{  node [fontname=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Palatino,Roboto,Helvetica}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{]}
\PY{l+s+s2}{  edge [fontname=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Palatino,Roboto,Helvetica}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{]}
\PY{l+s+s2}{  graph [}
\PY{l+s+s2}{    rankdir = }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}
\PY{l+s+s2}{  ];}
\PY{l+s+s2}{  node [}
\PY{l+s+s2}{    fontsize = }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{12}\PY{l+s+s2}{\PYZdq{}}
\PY{l+s+s2}{    shape = }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{record}\PY{l+s+s2}{\PYZdq{}}
\PY{l+s+s2}{  ];}
\PY{l+s+s2}{  edge [}
\PY{l+s+s2}{  ];}
\PY{l+s+s2}{  obj [label=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{OBJECT}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{];}
\PY{l+s+s2}{  obj \PYZhy{}\PYZgt{} n0:obj;}
\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}


\PY{k}{def} \PY{n+nf}{\PYZus{}slots\PYZus{}str}\PY{p}{(}\PY{n}{num\PYZus{}slots}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{|}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}s}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZgt{}}\PY{l+s+s1}{\PYZsq{}} \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}slots}\PY{p}{)}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{w ]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{type}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}tuple}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}slots\PYZus{}str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}94eafe}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}list}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}slots\PYZus{}str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}f2e7c0}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}set}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}slots\PYZus{}str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}bbbbee}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}frozenset}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}slots\PYZus{}str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}8899ee}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}dict}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{n}{slots} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{|}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{\PYZlt{}sk}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZgt{}|\PYZlt{}sv}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZgt{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)} \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{slots}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}a5ff9c}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}int}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{obj}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}eebbbb}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}float}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{obj}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}aaaaff}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}str}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} String content might clash with graphviz syntax, so we have to be careful here.}
  \PY{n}{translated} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}\PYZhy{}A\PYZhy{}Za\PYZhy{}z0\PYZhy{}9\PYZus{} .,:;}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{!?@\PYZsh{}\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{[}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{\PYZcb{}]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[?]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{obj}\PY{p}{)}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{translated}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}faffaf}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}none}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|}\PY{l+s+si}{\PYZob{}}\PY{n}{obj}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}aaaaaa}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{\PYZus{}render\PYZus{}unknown}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[shape=record,label=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZlt{}obj\PYZgt{}T:}\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}type\PYZus{}name}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{|(Unknown contents)}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{style=filled,fillcolor=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsh{}88ee88}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{n}{\PYZus{}RENDERER\PYZus{}BY\PYZus{}TYPE} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{n+nb}{int}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}int}\PY{p}{,}
    \PY{n+nb}{float}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}float}\PY{p}{,}
    \PY{n+nb}{str}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}str}\PY{p}{,}
    \PY{n+nb}{tuple}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}tuple}\PY{p}{,}
    \PY{n+nb}{list}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}list}\PY{p}{,}
    \PY{n+nb}{set}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}set}\PY{p}{,}
    \PY{n+nb}{dict}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}dict}\PY{p}{,}
    \PY{n+nb}{frozenset}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}frozenset}\PY{p}{,}
    \PY{n+nb}{type}\PY{p}{(}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:} \PY{n}{\PYZus{}render\PYZus{}none}\PY{p}{,}
\PY{p}{\PYZcb{}}


\PY{k}{def} \PY{n+nf}{as\PYZus{}graphviz}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a .dot string for a graph visualizing the object.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} \PYZsq{}Canonicalized id\PYZsq{} \PYZhy{} does not leak memory layout information.}
  \PY{n}{cid\PYZus{}by\PYZus{}id} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}cid}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{cid\PYZus{}by\PYZus{}id}\PY{o}{.}\PY{n}{setdefault}\PY{p}{(}\PY{n+nb}{id}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{cid\PYZus{}by\PYZus{}id}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{n}{node\PYZus{}by\PYZus{}cid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
  \PY{n}{rendered\PYZus{}by\PYZus{}cid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
  \PY{n}{edges} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{def} \PY{n+nf}{walk}\PY{p}{(}\PY{n}{node}\PY{p}{)}\PY{p}{:}
    \PY{n}{cid} \PY{o}{=} \PY{n}{get\PYZus{}cid}\PY{p}{(}\PY{n}{node}\PY{p}{)}
    \PY{k}{if} \PY{n}{cid} \PY{o+ow}{in} \PY{n}{node\PYZus{}by\PYZus{}cid}\PY{p}{:}
      \PY{c+c1}{\PYZsh{} Properly handling reference cycles.}
      \PY{k}{return}
    \PY{n}{node\PYZus{}by\PYZus{}cid}\PY{p}{[}\PY{n}{cid}\PY{p}{]} \PY{o}{=} \PY{n}{node}
    \PY{n}{node\PYZus{}type} \PY{o}{=} \PY{n+nb}{type}\PY{p}{(}\PY{n}{node}\PY{p}{)}
    \PY{n}{rendered\PYZus{}by\PYZus{}cid}\PY{p}{[}\PY{n}{cid}\PY{p}{]} \PY{o}{=} \PY{n}{\PYZus{}RENDERER\PYZus{}BY\PYZus{}TYPE}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{node\PYZus{}type}\PY{p}{,}
                                                 \PY{n}{\PYZus{}render\PYZus{}unknown}\PY{p}{)}\PY{p}{(}\PY{n}{node}\PY{p}{)}
    \PY{k}{if} \PY{n+nb}{issubclass}\PY{p}{(}\PY{n}{node\PYZus{}type}\PY{p}{,} \PY{p}{(}\PY{n+nb}{list}\PY{p}{,} \PY{n+nb}{tuple}\PY{p}{,} \PY{n+nb}{set}\PY{p}{,} \PY{n+nb}{frozenset}\PY{p}{)}\PY{p}{)}\PY{p}{:}
      \PY{k}{for} \PY{n}{n}\PY{p}{,} \PY{n}{child} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{node}\PY{p}{)}\PY{p}{:}
        \PY{n}{walk}\PY{p}{(}\PY{n}{child}\PY{p}{)}
        \PY{n}{edges}\PY{o}{.}\PY{n}{append}\PY{p}{(}
          \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n}\PY{l+s+si}{\PYZob{}}\PY{n}{cid}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:s}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ \PYZhy{}\PYZgt{} n}\PY{l+s+si}{\PYZob{}}\PY{n}{get\PYZus{}cid}\PY{p}{(}\PY{n}{child}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:obj}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{elif} \PY{n+nb}{issubclass}\PY{p}{(}\PY{n}{node\PYZus{}type}\PY{p}{,} \PY{n+nb}{dict}\PY{p}{)}\PY{p}{:}
      \PY{k}{for} \PY{n}{n}\PY{p}{,} \PY{p}{(}\PY{n}{key}\PY{p}{,} \PY{n}{val}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{node}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{walk}\PY{p}{(}\PY{n}{key}\PY{p}{)}
        \PY{n}{walk}\PY{p}{(}\PY{n}{val}\PY{p}{)}
        \PY{n}{edges}\PY{o}{.}\PY{n}{append}\PY{p}{(}
          \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n}\PY{l+s+si}{\PYZob{}}\PY{n}{get\PYZus{}cid}\PY{p}{(}\PY{n}{key}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:obj \PYZhy{}\PYZgt{} n}\PY{l+s+si}{\PYZob{}}\PY{n}{cid}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:sk}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ [dir=back]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{edges}\PY{o}{.}\PY{n}{append}\PY{p}{(}
          \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n}\PY{l+s+si}{\PYZob{}}\PY{n}{cid}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:sv}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ \PYZhy{}\PYZgt{} n}\PY{l+s+si}{\PYZob{}}\PY{n}{get\PYZus{}cid}\PY{p}{(}\PY{n}{val}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:obj}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{n}{walk}\PY{p}{(}\PY{n}{obj}\PY{p}{)}
  \PY{n}{result} \PY{o}{=} \PY{n}{io}\PY{o}{.}\PY{n}{StringIO}\PY{p}{(}\PY{p}{)}
  \PY{n}{result}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{\PYZus{}DOT\PYZus{}HEADER}\PY{p}{)}
  \PY{k}{for} \PY{n}{cid}\PY{p}{,} \PY{n}{rendered\PYZus{}node} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{rendered\PYZus{}by\PYZus{}cid}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{result}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{  n}\PY{l+s+si}{\PYZob{}}\PY{n}{cid}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{rendered\PYZus{}node}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{;}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{k}{for} \PY{n}{edge} \PY{o+ow}{in} \PY{n}{edges}\PY{p}{:}
    \PY{n}{result}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{  }\PY{l+s+si}{\PYZob{}}\PY{n}{edge}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{;}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{result}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{k}{return} \PY{n}{result}\PY{o}{.}\PY{n}{getvalue}\PY{p}{(}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{pygraph}\PY{p}{(}\PY{n}{obj}\PY{p}{,} \PY{n}{directory}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
  \PY{n}{tag} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{1e9}\PY{p}{)}
  \PY{n}{dot\PYZus{}text} \PY{o}{=} \PY{n}{as\PYZus{}graphviz}\PY{p}{(}\PY{n}{obj}\PY{p}{)}
  \PY{n}{file\PYZus{}stem} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{directory}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pygraph\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{tag}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{file\PYZus{}dot} \PY{o}{=} \PY{n}{file\PYZus{}stem} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.dot}\PY{l+s+s1}{\PYZsq{}}
  \PY{n}{file\PYZus{}png} \PY{o}{=} \PY{n}{file\PYZus{}stem} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.png}\PY{l+s+s1}{\PYZsq{}}
  \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{file\PYZus{}dot}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h}\PY{p}{:}
    \PY{n}{h}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{dot\PYZus{}text}\PY{p}{)}
  \PY{n}{done} \PY{o}{=} \PY{n}{subprocess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}Tpng}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{file\PYZus{}dot}\PY{p}{]}\PY{p}{,} \PY{n}{capture\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
  \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{file\PYZus{}png}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h}\PY{p}{:}
    \PY{n}{h}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{done}\PY{o}{.}\PY{n}{stdout}\PY{p}{)}
  \PY{k}{return} \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{file\PYZus{}png}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    With this we can visualize values such as list-of-lists matrices in a
way that shows the structural difference:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pygraph}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{l+m+mf}{1.5} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}
 
            
\prompt{Out}{outcolor}{7}{}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_01_Python_files/ML_01_Python_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pygraph}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{l+m+mf}{1.5} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}
 
            
\prompt{Out}{outcolor}{8}{}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_01_Python_files/ML_01_Python_45_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    


    % Add a bibliography block to the postdoc
    
    
    

    
    
    
    

    
    \hypertarget{derivatives-numerical-optimization-and-machine-learning}{%
\section{Derivatives, Numerical Optimization, and Machine
Learning}\label{derivatives-numerical-optimization-and-machine-learning}}

\hypertarget{why-we-are-looking-into-this}{%
\subsection{Why we are looking into
this}\label{why-we-are-looking-into-this}}

\begin{itemize}
\tightlist
\item
  ML ``Model training'' generally is a form of numerical optimization -
  ``minimizing the risk of a seriously off answer''.
\item
  Common ML situation: ``very many'' optimization parameters (ballpark
  \textasciitilde{}\(10^4\) to \(10^9\)).
\item
  Being able to do numerical optimization with \textgreater10 parameters
  is very useful in itself.
\item
  Many approaches to numerical optimization of nonlinear functions use
  gradients.
\item
  So, we want to retrace/understand the steps that led to insights on
  how to efficiently compute gradients even for
  \({\mathbb R}^{1\,000\,000}\to{\mathbb R}\) objective functions.
\end{itemize}

But let us start simple with something we know - also to get some Python
practice: Derivatives.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Importing the relevant Python libraries (\PYZdq{}modules\PYZdq{}).}
\PY{k+kn}{import} \PY{n+nn}{math}
\PY{k+kn}{import} \PY{n+nn}{pprint}
\PY{k+kn}{import} \PY{n+nn}{time}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot}
\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}

\PY{c+c1}{\PYZsh{} Larger figures by default.}
\PY{n}{matplotlib}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    First task: Given a simple function (``black box'' - we can evaluate it
at individual points, but not inspect its definition) ``where all
relevant scales (function value, gradient, curvature, etc.) are of
magnitude 1'', let us look into numerically computing the derivative at
a given point.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{derivative\PYZus{}v0}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Estimates f\PYZsq{}(x0) via finite differences.}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    f: Callable, the R\PYZhy{}\PYZgt{}R function we want to compute the derivative of.}
\PY{l+s+sd}{    x0: The position where to compute the derivative.}
\PY{l+s+sd}{    eps: Step size.}

\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    `(f(x0+eps) \PYZhy{} f(x0)) / eps`, as a numerical approximation}
\PY{l+s+sd}{    to the derivative.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{n}{eps}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{eps}


\PY{k}{def} \PY{n+nf}{derivative\PYZus{}v1}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Estimates f\PYZsq{}(x0) via finite differences (symmetric difference).}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    f: Callable, the R\PYZhy{}\PYZgt{}R function we want to compute the derivative of.}
\PY{l+s+sd}{    x0: The position where to compute the derivative:}
\PY{l+s+sd}{    eps: Step size.}

\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    `(f(x0+eps) \PYZhy{} f(x0\PYZhy{}eps)) / (2*eps)`, as a numerical approximation}
\PY{l+s+sd}{    to the derivative.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{n}{eps}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{p}{(}\PY{n}{x0} \PY{o}{\PYZhy{}} \PY{n}{eps}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{eps}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{demo1}\PY{p}{(}\PY{n}{position}\PY{o}{=}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{n}{derivative}\PY{o}{=}\PY{n}{derivative\PYZus{}v0}\PY{p}{)}\PY{p}{:}
  \PY{n}{f} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
  \PY{k}{for} \PY{n}{epsilon} \PY{o+ow}{in} \PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}9}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}10}\PY{p}{)}\PY{p}{:}
    \PY{n}{df\PYZus{}dx} \PY{o}{=} \PY{n}{derivative}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{position}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{n}{epsilon}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps=}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{str}\PY{p}{(}\PY{n}{epsilon}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{6s}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: f}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{(}\PY{l+s+si}{\PYZob{}}\PY{n}{position}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) = approx. }\PY{l+s+si}{\PYZob{}}\PY{n}{df\PYZus{}dx}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{exploring-derivatives}{%
\section{Exploring Derivatives}\label{exploring-derivatives}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{demo1}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
eps=0.01  : f'(2.0) = approx. 12.060099999999707
eps=0.001 : f'(2.0) = approx. 12.006000999997823
eps=0.0001: f'(2.0) = approx. 12.000600010022566
eps=1e-05 : f'(2.0) = approx. 12.000060000261213
eps=1e-06 : f'(2.0) = approx. 12.000006002210739
eps=1e-07 : f'(2.0) = approx. 12.000000584322379
eps=1e-08 : f'(2.0) = approx. 11.999999927070348
eps=1e-09 : f'(2.0) = approx. 12.000000992884452
eps=1e-10 : f'(2.0) = approx. 12.000000992884452
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{demo1}\PY{p}{(}\PY{n}{derivative}\PY{o}{=}\PY{n}{derivative\PYZus{}v1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
eps=0.01  : f'(2.0) = approx. 12.000099999999847
eps=0.001 : f'(2.0) = approx. 12.000000999998317
eps=0.0001: f'(2.0) = approx. 12.00000001000845
eps=1e-05 : f'(2.0) = approx. 12.00000000021184
eps=1e-06 : f'(2.0) = approx. 12.000000000789157
eps=1e-07 : f'(2.0) = approx. 11.99999999368373
eps=1e-08 : f'(2.0) = approx. 11.999999882661427
eps=1e-09 : f'(2.0) = approx. 12.000000992884452
eps=1e-10 : f'(2.0) = approx. 12.000000992884452
    \end{Verbatim}

    For normal-differencing, ``if function-values and derivatives are
roughly on the scale of 1'', we get `best approximation' for eps
\textasciitilde{} 1e-8, which corresponds to about half the available
15-digit accuracy.

With smaller step sizes, we get more valid digits for the
change-in-value, but representing the step-size itself numerically
vs.~the change-in-value becomes more inaccurate, and the best trade-off
is ``if step size splits the available accuracy in the middle''.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For symmetric-differencing, this is a bit trickier to analyze, but as we
would expect, ``larger step size gives us still good values for higher
order approximations.''

The way to reason this out is as follows: symmetric-differencing
actually corresponds to how we would obtain the linear term when fitting
a Taylor polynomial to 2nd order, evaluating the function at: (-eps, 0,
+eps). For the linear term, the weight of f(x0) just happens to be zero.
If values are ``on the scale of 1'', then a 3rd-order output-noise of
1e-15 is created by an input-noise of \textasciitilde1e-5.

    \hypertarget{question-trick-question-what-is-the-best-step-size-for-derivative_v0}{%
\subsection{\texorpdfstring{Question (Trick question?): What is the best
step size for
\texttt{derivative\_v0}?}{Question (Trick question?): What is the best step size for derivative\_v0?}}\label{question-trick-question-what-is-the-best-step-size-for-derivative_v0}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Explore yourself!}
\PY{n}{my\PYZus{}function} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}  \PY{c+c1}{\PYZsh{} Or feel free to define any other function you like.}
\PY{n}{derivative\PYZus{}v0}\PY{p}{(}\PY{n}{my\PYZus{}function}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
3.0301000000000133
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{answer-which-will-lead-us-to-fm-ad-and-rm-ad.}{%
\subsubsection{Answer (\ldots which will lead us to FM-AD and
RM-AD.)}\label{answer-which-will-lead-us-to-fm-ad-and-rm-ad.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{f} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{derivative\PYZus{}v1}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}15j}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Also:}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{derivative\PYZus{}v1}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}100j}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3+0j)
(3+0j)
    \end{Verbatim}

    This may ``look like cheating''. What is going on here?

\begin{itemize}
\item
  Quite a few Python functions are happy to take complex arguments.
\item
  (Python syntax for complex numbers:
  \texttt{\{float\_literal\}{[}+-{]}\{float\_literal\}j}.

  Also: \texttt{complex(2,\ 3)\ =\textgreater{}\ 2+3j}; \texttt{j} as in
  electrical engineering, where \texttt{i} generally is
  current-density.)
\item
  Both for real and imaginary part, we have 64-bit floating point
  accuracy.
\item
  We are basically doing the computation for \texttt{x0+\{fuzz\}}.
\item
  Making the ``fuzz'' imaginary, the imaginary part of the quantities we
  compute tells us by how much the function's value changes as multiples
  of the imaginary fuzz, if we fuzz the input like this.
\item
  Note that \texttt{(k\ *\ 1e-15j)**2\ =\ -(k**2)\ *\ 1e-30}.

  So if \{function value\} ``is on the scale of 1'', and we can
  represent numbers ``to 15 digits behind the point (relative to that
  scale)'', as long as this derivative k will be smaller in magnitude
  than 1e15, we get a ``numerically accurate'' result (= the derivative
  is as good as it gets with numerics, there are no higher order
  distortions).
\item
  (Feel free to try this out with a function with larger derivative,
  such as \texttt{(1\ +\ 100*x)**4} at \texttt{x=1}.)
\item
  We sort-of abused complex numbers (and holomorphicity) here. But
  suppose we had a data type \texttt{complex0} that differed from the
  complex numbers by obeying \(I^2=0\) rather than \(i^2=-1\). These are
  the called ``dual numbers''. With those, we could just evaluate
  \texttt{f(x0+I)} and pick out the ``imaginary part'' to get the
  derivative. No need for this ``1e-15'' scaling.
\item
  Suppose now we had some \({\mathbb R}\to {\mathbb R^{1000}}\) function
  \(f\). Using ``dual numbers'', we can easily compute \(f'(x_0)\)
  alongside the computation of \(f(x_0)\) ``with little extra effort''.
  (``little'' == ``bounded by a small constant effort factor
  vs.~original computation''.) Doing this with a complex-numbers-like
  numeric data type is called ``forward mode automatic differentiation
  (FMAD)''.
\item
  We of course also could do this ``Forward Mode AD'' for 2, 3, etc.
  parameters, using an algebra of numbers \texttt{a+bI+cJ+dK+eL} where
  we have \(I^2=IJ=IK=\ldots=K^2=KL=\ldots=L^2=0\) (``all 2nd order
  quantities can be dropped'').

  Having \(n\) such ``infinitesimals'' basically multiplies the effort
  by \(n\).
\item
  (In undergraduate physics, we typically encounter this idea in the
  form of ``Gaussian error propagation''.)
\item
  For \emph{numerical optimization}, we have the opposite situation:
  \emph{one} objective, but ``thousands of parameters'':
  \({\mathbb R}^{1000}\to{\mathbb R}\), not
  \({\mathbb R}\to{\mathbb R}^{1000}\).
\item
  There is a ``dual trick'' that makes this other case fast. This is
  known as ``reverse mode automatic differentiation'' (``RMAD'', or just
  ``AD''), or also ``sensitivity backpropagation''.
\item
  RMAD was first fully formalized as ``this is an algorithm that
  transforms an algorithm computing \(f\) into an efficient algorithm
  computing \(f'\)'' in
  \href{https://www.osti.gov/servlets/purl/5254402}{Bert Speelpenning's
  1980 PhD thesis} \cite{speelpenning1980compiling}. Unaware of progress in numerical optimization /
  engineering, it was rediscovered in the much narrower context of
  neural networks (so, not as a generic algorithm-transformation) in
  1985, \href{https://www.nature.com/articles/323533a0}{in a paper by
  Rummelhart, Hinton, and Williams} \cite{rumelhart1986learning}. Given that this was rediscovered
  many times in different contexts, it is hard to name an inventor.
  Seppo Linnainmaa might have been the first to put this onto computers
  in the 1970s, but one can demonstrate that the idea is very closely
  related to insights that go back to Lev Pontryagin, and even William
  Rowan Hamilton.
\end{itemize}

    \hypertarget{before-we-move-on-a-python-interlude.}{%
\section{Before we move on\ldots{} a Python
interlude.}\label{before-we-move-on-a-python-interlude.}}

Here is a function that maps some \({\mathbb R}\mapsto{\mathbb R}\)
function \texttt{f} to a function numerically computing
\texttt{f\textquotesingle{}} by finite-differencing:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{numerical\PYZus{}derivative}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{default\PYZus{}eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Maps a R\PYZhy{}\PYZgt{}R function to its finite\PYZhy{}differencing derivative\PYZhy{}function.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{def} \PY{n+nf}{fprime}\PY{p}{(}\PY{n}{x0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Numerically computes the derivative.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} If the optional parameter \PYZsq{}eps\PYZsq{} was provided, we use that as step size,}
    \PY{c+c1}{\PYZsh{} otherwise we use the value of `default\PYZus{}eps` from the (lexically) outer}
    \PY{c+c1}{\PYZsh{} function as it was at the point when code flow encountered the definition}
    \PY{c+c1}{\PYZsh{} of this inner function.}
    \PY{n}{epsilon} \PY{o}{=} \PY{n}{default\PYZus{}eps} \PY{k}{if} \PY{n}{eps} \PY{o+ow}{is} \PY{k+kc}{None} \PY{k}{else} \PY{n}{eps}
    \PY{k}{return} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{n}{epsilon}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{p}{(}\PY{n}{x0} \PY{o}{\PYZhy{}} \PY{n}{epsilon}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{epsilon}\PY{p}{)}
  \PY{k}{return} \PY{n}{fprime}  \PY{c+c1}{\PYZsh{} We return the derivative\PYZhy{}function as a value!}
\end{Verbatim}
\end{tcolorbox}

    Examples below. (Feel free to try out your own!)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{f1} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{math}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\PY{n}{f2} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}

\PY{n}{f1prime} \PY{o}{=} \PY{n}{numerical\PYZus{}derivative}\PY{p}{(}\PY{n}{f1}\PY{p}{)}
\PY{n}{f2prime} \PY{o}{=} \PY{n}{numerical\PYZus{}derivative}\PY{p}{(}\PY{n}{f2}\PY{p}{,} \PY{n}{default\PYZus{}eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f1prime}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{pi}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f1prime}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{pi}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f2prime}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Uses the default eps\PYZhy{}value 1e\PYZhy{}4.}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f2prime}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0.7071067803510189
0.7071067803510189
-0.5773502743211534
-0.5773502692596466
    \end{Verbatim}

    This is a beefed-up variant that uses some Python tricks we have not
discussed yet, but is practically useful for symmetric-differencing a
\(K^{n_1\times n_2\times\ldots}\to K^{m_1\times m_2\times\ldots}\)
tensor-valued function - for \(K={\mathbb R}\) or \(K={\mathbb C}\) - of
coordinates that in themselves may be tensor-valued:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{n\PYZus{}grad}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Maps a tensor\PYZhy{}valued function of tensor\PYZhy{}coordinates to a gradient\PYZhy{}function.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{def} \PY{n+nf}{grad\PYZus{}f}\PY{p}{(}\PY{n}{xs}\PY{p}{)}\PY{p}{:}
    \PY{n}{xs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{xs}\PY{p}{)}
    \PY{n}{xs0} \PY{o}{=} \PY{n}{xs}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
    \PY{n}{dim} \PY{o}{=} \PY{n}{xs}\PY{o}{.}\PY{n}{size}
    \PY{n}{xs1} \PY{o}{=} \PY{n}{xs0}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{f0} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{xs0}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{result} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{f0}\PY{o}{.}\PY{n}{shape} \PY{o}{+} \PY{p}{(}\PY{n}{dim}\PY{p}{,}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{:}
      \PY{n}{xs1}\PY{p}{[}\PY{n}{n}\PY{p}{]} \PY{o}{=} \PY{n}{xs0}\PY{p}{[}\PY{n}{n}\PY{p}{]} \PY{o}{+} \PY{n}{eps}
      \PY{n}{fplus} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{xs1}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}\PY{p}{)}
      \PY{n}{xs1}\PY{p}{[}\PY{n}{n}\PY{p}{]} \PY{o}{=} \PY{n}{xs0}\PY{p}{[}\PY{n}{n}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{eps}
      \PY{n}{fminus} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{xs1}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}\PY{p}{)}
      \PY{n}{xs1}\PY{p}{[}\PY{n}{n}\PY{p}{]} \PY{o}{=} \PY{n}{xs0}\PY{p}{[}\PY{n}{n}\PY{p}{]}
      \PY{n}{result}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{n}{n}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{fplus} \PY{o}{\PYZhy{}} \PY{n}{fminus}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{eps}\PY{p}{)}
    \PY{k}{return} \PY{n}{result}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{f0}\PY{o}{.}\PY{n}{shape} \PY{o}{+} \PY{n}{xs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
  \PY{k}{return} \PY{n}{grad\PYZus{}f}

\PY{c+c1}{\PYZsh{} Example: the gradient of matrix\PYZhy{}squaring evaluated at the 2x2\PYZhy{}identity \PYZhy{}}
\PY{c+c1}{\PYZsh{} which is a [2,2,2,2]\PYZhy{}tensor.}
\PY{n}{grad\PYZus{}matrix\PYZus{}squaring} \PY{o}{=} \PY{n}{n\PYZus{}grad}\PY{p}{(}\PY{k}{lambda} \PY{n}{m}\PY{p}{:} \PY{n}{m} \PY{o}{@} \PY{n}{m}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{grad\PYZus{}matrix\PYZus{}squaring}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[[[2. 0.]
   [0. 0.]]

  [[0. 2.]
   [0. 0.]]]


 [[[0. 0.]
   [2. 0.]]

  [[0. 0.]
   [0. 2.]]]]
    \end{Verbatim}

    \hypertarget{reverse-mode-ad-step-by-step}{%
\section{Reverse Mode AD,
Step-by-Step}\label{reverse-mode-ad-step-by-step}}

    We need a simple-but-not-too-simple toy function to discuss the idea
behind Reverse-mode AD.

Suppose you have a 4d box (3d would lead to ``too simple'' an example)
with edge lengths \(a, b, c, d\). The hypersurface of this box is given
by the following function.

(Note that putting an \(\epsilon\)-thickness layer of paint onto a 4d
such box would require
\({\rm Vol}(a+\epsilon,b+\epsilon,c+\epsilon,d+\epsilon)-{\rm Vol}(a,b,c,d)+{\mathcal O}(\epsilon^2)\)
much paint, and dividing by \(\epsilon\) gives us the surface area, just
as ``summing the surface 3d-boxes'' does.)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Normally, we would instead just do:}
  \PY{c+c1}{\PYZsh{}   return 2 * (abc + abd + acd + bcd)}
  \PY{c+c1}{\PYZsh{} ...but doing smaller steps will be useful for the subsequent discussion.}
  \PY{k}{return} \PY{n}{hypersurface}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Merely ensuring that this computes a plausible value at all for some simple case.}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{box4d\PYZus{}hypersurface}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
8000
    \end{Verbatim}

    Starting from the function definition above, let us manually implement a
function that ``computes the gradient at a given point''.

For some given point \texttt{(a,\ b,\ c,\ d)}, we want to know
\texttt{grad(box4d\_hypersurface)(a,\ b,\ c,\ d)}, i.e.~``by how much
does the hypersurface change if we tweak \texttt{a} by \(\epsilon\)
(relative to \(\epsilon\)), and likewise for \texttt{b}, \texttt{c},
\texttt{d}.

People did this sort of thing (manually implementing gradients) for
quite a while, and gradually realized a few tricks that condensed into
this generally-applicable recipe. Here: for ``straight'' functions
without loops (as this all was understood around 1976 already) - and
subsequently, for code with branches and loops and other control
structure.

\textbf{The Recipe}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Copy the code that computes the function. Efficiently computing the
  gradient will start with computing the forward-function.

  Do make sure the function is written in such a way that intermediate
  results never are overwritten.

  (This already holds for our example.)
\item
  Comment out the final `return \ldots{}' statement.

  Rather than returning that value, we will have to do more work
  afterwards.
\item
  There will (in general) be quite a few intermediate quantities for
  this calculation. For the \(k\)-th such intermediate quantity -
  \(q_k\), we introduce a ``sensitivity accumulator'' \(S_{q_k}\), which
  initially is initialized to zero right at the point where \(q_k\) is
  introduced.
\item
  We also treat the inputs and the output as ``intermediate
  quantities''.
\item
  Right from the point where we could return the result, go through the
  calculation once again, but this time backwards, step-by-step, from
  each intermediate quantity back to earlier ones in terms of which it
  is defined. While doing so, maintain this invariant:

  Each such ``sensitivity accumulator'' \(S_{q_k}\) keeps track of ``by
  how much would the final result of this function change, relative to
  \(\epsilon\), if I intercepted code flow right after this line in the
  calculation that uses the quantity \(q_k\), and made an ad-hoc change
  to \(q_k\), incrementing it by \(\epsilon\)?
\end{enumerate}

Once we know the sensitivities of the final result on the input
parameters, we have our gradient.

Let us apply this recipe step-by-step.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Step 1: Copying the code.}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step1}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Normally, we would instead just do:}
  \PY{c+c1}{\PYZsh{}   return 2 * (abc + abd + acd + bcd)}
  \PY{c+c1}{\PYZsh{} ...but doing smaller steps will be useful for the subsequent discussion.}
  \PY{k}{return} \PY{n}{hypersurface}


\PY{c+c1}{\PYZsh{} Step 2: Removing the `return ...`}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step2}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Normally, we would instead just do:}
  \PY{c+c1}{\PYZsh{}   return 2 * (abc + abd + acd + bcd)}
  \PY{c+c1}{\PYZsh{} ...but doing smaller steps will be useful for the subsequent discussion.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}


\PY{c+c1}{\PYZsh{} Step 3: Introduce sensitivity accumulators.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} Note: Using Semicolons is generally frowned\PYZhy{}upon\PYZhy{}style in Python,}
\PY{c+c1}{\PYZsh{} but here it comes handy for didactic purposes.}
\PY{c+c1}{\PYZsh{} Still: avoid this \PYZdq{}in real code\PYZdq{}.}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step3}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}\PY{p}{;} \PY{n}{s\PYZus{}ab} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}cd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}\PY{p}{;} \PY{n}{s\PYZus{}abc} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}abd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}acd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}bcd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}\PY{p}{;} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Normally, we would instead just do:}
  \PY{c+c1}{\PYZsh{}   return 2 * (abc + abd + acd + bcd)}
  \PY{c+c1}{\PYZsh{} ...but doing smaller steps will be useful for the subsequent discussion.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}

\PY{c+c1}{\PYZsh{} Step 4: Also do this for the inputs and for the result value.}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step4}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{s\PYZus{}a} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}c} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}d} \PY{o}{=} \PY{l+m+mi}{0}   \PY{c+c1}{\PYZsh{} \PYZlt{}== !!!}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}\PY{p}{;} \PY{n}{s\PYZus{}ab} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}cd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}\PY{p}{;} \PY{n}{s\PYZus{}abc} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}abd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}acd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}bcd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}\PY{p}{;} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}
  \PY{c+c1}{\PYZsh{} !!! `hypersurface` is our result value. We already know the final}
  \PY{c+c1}{\PYZsh{} !!!  value of `s\PYZus{}hypersurface`: If we intercepted the calculation after}
  \PY{c+c1}{\PYZsh{} !!! `hypersurface = 2 * ...`, and added a change\PYZhy{}forcing line...:}
  \PY{c+c1}{\PYZsh{} !!!    hypersurface = hypersurface + epsilon}
  \PY{c+c1}{\PYZsh{} !!!  this would (trivially) change the function\PYZsq{}s result by epsilon.}
  \PY{c+c1}{\PYZsh{} !!!  So, s\PYZus{}hypersurface = 1.}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}\PY{p}{;} \PY{n}{s\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{c+c1}{\PYZsh{} Normally, we would instead just do:}
  \PY{c+c1}{\PYZsh{}   return 2 * (abc + abd + acd + bcd)}
  \PY{c+c1}{\PYZsh{} ...but doing smaller steps will be useful for the subsequent discussion.}

\PY{c+c1}{\PYZsh{} Step 5 (\PYZdq{}backpropagation\PYZdq{}) \PYZhy{} we do this one\PYZhy{}by\PYZhy{}one.}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step5a}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{s\PYZus{}a} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}c} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}d} \PY{o}{=} \PY{l+m+mi}{0}   \PY{c+c1}{\PYZsh{} \PYZlt{}== !!!}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}\PY{p}{;} \PY{n}{s\PYZus{}ab} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}cd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}\PY{p}{;} \PY{n}{s\PYZus{}abc} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}abd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}acd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}bcd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}\PY{p}{;} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}\PY{p}{;} \PY{n}{s\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    hypersurface = 2 * half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} If we had added a line right before this that reads:}
  \PY{c+c1}{\PYZsh{}    half\PYZus{}hypersurface = half\PYZus{}hypersurface + eps}
  \PY{c+c1}{\PYZsh{} ...this would change `hypersurface` by 2 * eps, and the end result by}
  \PY{c+c1}{\PYZsh{} 2 * eps * \PYZob{}sensitivity of the end result on `hypersurface`\PYZcb{}}
  \PY{c+c1}{\PYZsh{} (this is the chain rule). So, `s\PYZus{}half\PYZus{}hypersurface` receives a contribution:}
  \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{+} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} (From now on, we will just write this:}
  \PY{c+c1}{\PYZsh{}    s\PYZus{}half\PYZus{}hypersurface += 2 * s\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{}  ...even though one should in general stay away from the += operator}
  \PY{c+c1}{\PYZsh{}  in Python, at least while learning the language.)}

\PY{c+c1}{\PYZsh{} Then...}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step5b}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{s\PYZus{}a} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}c} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}d} \PY{o}{=} \PY{l+m+mi}{0}   \PY{c+c1}{\PYZsh{} \PYZlt{}== !!!}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}\PY{p}{;} \PY{n}{s\PYZus{}ab} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}cd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}\PY{p}{;} \PY{n}{s\PYZus{}abc} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}abd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}acd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}bcd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}\PY{p}{;} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}\PY{p}{;} \PY{n}{s\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    hypersurface = 2 * half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    half\PYZus{}hypersurface = abc + abd + acd + bcd}
  \PY{n}{s\PYZus{}abc} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}abd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}acd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}bcd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}

\PY{c+c1}{\PYZsh{} Then:}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}step5c}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{s\PYZus{}a} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}c} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}d} \PY{o}{=} \PY{l+m+mi}{0}   \PY{c+c1}{\PYZsh{} \PYZlt{}== !!!}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}\PY{p}{;} \PY{n}{s\PYZus{}ab} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}cd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}\PY{p}{;} \PY{n}{s\PYZus{}abc} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}abd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}acd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}bcd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}\PY{p}{;} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}\PY{p}{;} \PY{n}{s\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    hypersurface = 2 * half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    half\PYZus{}hypersurface = abc + abd + acd + bcd}
  \PY{n}{s\PYZus{}abc} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}abd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}acd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}bcd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}     bcd = b * cd}
  \PY{c+c1}{\PYZsh{} For each of the factors, sensitivity = \PYZob{}sensitivity of the}
  \PY{c+c1}{\PYZsh{} end result on the left hand side intermediate quantity, which we at}
  \PY{c+c1}{\PYZsh{} this point fully know, since we reached the definition of this quantity,}
  \PY{c+c1}{\PYZsh{} and all its uses come after the definition\PYZcb{} * \PYZob{}other\PYZhy{}factor, which scales it\PYZcb{}.}
  \PY{c+c1}{\PYZsh{} (This is just the product rule in action).}
  \PY{n}{s\PYZus{}b} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}bcd} \PY{o}{*} \PY{n}{cd}
  \PY{n}{s\PYZus{}cd} \PY{o}{+}\PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{s\PYZus{}bcd}

\PY{c+c1}{\PYZsh{} Proceeding further like this, now doing larger steps \PYZhy{}}
\PY{c+c1}{\PYZsh{} and completing the work.}

\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}and\PYZus{}gradient}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{s\PYZus{}a} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}b} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}c} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}d} \PY{o}{=} \PY{l+m+mi}{0}   \PY{c+c1}{\PYZsh{} \PYZlt{}== !!!}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}\PY{p}{;} \PY{n}{s\PYZus{}ab} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}cd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c}\PY{p}{;} \PY{n}{s\PYZus{}abc} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}\PY{p}{;} \PY{n}{s\PYZus{}abd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}acd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}\PY{p}{;} \PY{n}{s\PYZus{}bcd} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}\PY{p}{;} \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}\PY{p}{;} \PY{n}{s\PYZus{}hypersurface} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Rather than returning the value, proceed to compute the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} return hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    hypersurface = 2 * half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}half\PYZus{}hypersurface} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    half\PYZus{}hypersurface = abc + abd + acd + bcd}
  \PY{n}{s\PYZus{}abc} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}abd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}acd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{n}{s\PYZus{}bcd} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}     bcd = b * cd}
  \PY{n}{s\PYZus{}b} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}bcd} \PY{o}{*} \PY{n}{cd}
  \PY{n}{s\PYZus{}cd} \PY{o}{+}\PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{s\PYZus{}bcd}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}     acd = a * cd}
  \PY{n}{s\PYZus{}a} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}acd} \PY{o}{*} \PY{n}{cd}
  \PY{n}{s\PYZus{}cd} \PY{o}{+}\PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{s\PYZus{}acd}  \PY{c+c1}{\PYZsh{} Note that this 2nd(\PYZhy{}in\PYZhy{}reverse\PYZhy{}order) use of `cd`}
                     \PY{c+c1}{\PYZsh{} introduces a 2nd contribution to `s\PYZus{}cd`!}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}Computation:}
  \PY{c+c1}{\PYZsh{}     abd = ab * d}
  \PY{n}{s\PYZus{}ab} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}abd} \PY{o}{*} \PY{n}{d}
  \PY{n}{s\PYZus{}d} \PY{o}{+}\PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{s\PYZus{}abd}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}     abc = ab * c}
  \PY{n}{s\PYZus{}ab} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}abc} \PY{o}{*} \PY{n}{c}
  \PY{n}{s\PYZus{}c} \PY{o}{+}\PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{s\PYZus{}abc}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}     cd = c * d}
  \PY{n}{s\PYZus{}c} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}cd} \PY{o}{*} \PY{n}{d}
  \PY{n}{s\PYZus{}d} \PY{o}{+}\PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{s\PYZus{}cd}
  \PY{c+c1}{\PYZsh{} Forward\PYZhy{}computation:}
  \PY{c+c1}{\PYZsh{}    ab = a * b}
  \PY{n}{s\PYZus{}a} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}ab} \PY{o}{*} \PY{n}{b}
  \PY{n}{s\PYZus{}b} \PY{o}{+}\PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{s\PYZus{}ab}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} At this point, we know the sensitivities of the result on the vector of}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} input\PYZhy{}coordinates. That is just the gradient.}
  \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} \PYZdq{}Since we can\PYZdq{}, we also return the function\PYZsq{}s value (as it may be useful):}
  \PY{k}{return} \PY{p}{(}\PY{n}{hypersurface}\PY{p}{,} \PY{p}{(}\PY{n}{s\PYZus{}a}\PY{p}{,} \PY{n}{s\PYZus{}b}\PY{p}{,} \PY{n}{s\PYZus{}c}\PY{p}{,} \PY{n}{s\PYZus{}d}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let\PYZsq{}s try it out...:}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{box4d\PYZus{}hypersurface\PYZus{}and\PYZus{}gradient}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{derivative\PYZus{}v1}\PY{p}{(}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{box4d\PYZus{}hypersurface}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}15j}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(1438, (334, 226, 174, 118))
(225.99999999999997+0j)
    \end{Verbatim}

    \hypertarget{computational-complexity}{%
\subsection{Computational Complexity}\label{computational-complexity}}

When reasoning about the effort needed to obtain a good gradient, a key
observation is that ``per use of an intermediate quantity'', the effort
is a fixed multiple of the corresponding effort done on the
forward-calculation.

``The most painful thing that can happen'' is back-propagating through a
product. We need to zero-initialize two sensitivity-accumulators
(somewhere), and to what extent this can be amortized depends e.g.~on
the operating system and code structure. Then, we need to load these
accumulators, add the increments, and store them again. Computing the
increments requires a multiplication each. We can write down effort as a
function of the relative cost of load/add/multiply/store operations, and
then study the range of the \{gradient effort\} / \{forward pass
effort\} function as a function of these costs. Speelpenning's thesis
gives a bound of ``at most 8x effort'', but one can reason out ``at most
6x'', and in practice one often observes ``factor 4-5, rarely around
6''. However, on real computers, cache performance also is an issue
here.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Hand\PYZhy{}backpropagation can be a bit tedious, and one needs to be extremely}
\PY{c+c1}{\PYZsh{} careful to avoid typos, but the good news is: If something is off, we can}
\PY{c+c1}{\PYZsh{} always modify the forward\PYZhy{}calculation \PYZdq{}to allow injecting epsilon changes\PYZdq{}}
\PY{c+c1}{\PYZsh{} and then compare \PYZdq{}measured\PYZdq{} and \PYZdq{}computed\PYZdq{} sensitivities, zeroing in}
\PY{c+c1}{\PYZsh{} on the location of the problem.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} This might roughly look as follows...:}


\PY{k}{def} \PY{n+nf}{box4d\PYZus{}hypersurface\PYZus{}perturbed}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{d}\PY{p}{,} \PY{n}{eps\PYZus{}cd}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{eps\PYZus{}abc}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Hypersurface of a 4d box with edge lengths a, b, c, d.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} We try to maximize sharing of computations, so rather than computing}
  \PY{c+c1}{\PYZsh{} a * b * c + a * b * d, etc., we remember intermediate products.}
  \PY{n}{ab} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{b}
  \PY{n}{cd} \PY{o}{=} \PY{n}{c} \PY{o}{*} \PY{n}{d} \PY{o}{+} \PY{n}{eps\PYZus{}cd}
  \PY{n}{abc} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{c} \PY{o}{+} \PY{n}{eps\PYZus{}abc}
  \PY{n}{abd} \PY{o}{=} \PY{n}{ab} \PY{o}{*} \PY{n}{d}
  \PY{n}{acd} \PY{o}{=} \PY{n}{a} \PY{o}{*} \PY{n}{cd}
  \PY{n}{bcd} \PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{cd}
  \PY{n}{half\PYZus{}hypersurface} \PY{o}{=} \PY{n}{abc} \PY{o}{+} \PY{n}{abd} \PY{o}{+} \PY{n}{acd} \PY{o}{+} \PY{n}{bcd}
  \PY{n}{hypersurface} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{half\PYZus{}hypersurface}
  \PY{c+c1}{\PYZsh{} Normally, we would instead just do:}
  \PY{c+c1}{\PYZsh{}   return 2 * (abc + abd + acd + bcd)}
  \PY{c+c1}{\PYZsh{} ...but doing smaller steps will be useful for the subsequent discussion.}
  \PY{k}{return} \PY{n}{hypersurface}

\PY{n+nb}{print}\PY{p}{(}\PY{p}{(}\PY{n}{box4d\PYZus{}hypersurface\PYZus{}perturbed}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{n}{eps\PYZus{}cd}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)} \PY{o}{\PYZhy{}}
       \PY{n}{box4d\PYZus{}hypersurface\PYZus{}perturbed}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}
\PY{c+c1}{\PYZsh{} ...which we then can compare against the finalized s\PYZus{}cd, respectively s\PYZus{}abc.}

\PY{c+c1}{\PYZsh{} We will see a more complicated semi\PYZhy{}realistic worked out example below which}
\PY{c+c1}{\PYZsh{} still has some of its gradient debug code deliberately left in.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
13.999997463542968
    \end{Verbatim}

    \hypertarget{more-complicated-code-structure}{%
\subsection{More complicated code
structure}\label{more-complicated-code-structure}}

Having seen the basic idea, it is natural to next discuss three
generalizing aspects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``data-parallelism''.
\item
  Conditionals
\item
  Loops
\end{enumerate}

What we have seen so far looks at individual number-additions and
multiplications.

However, the reasoning carries over straightaway to tensor arithmetics.

Let's look at an example for backpropagating through some matrix/tensor
operation.

For pedagogical reasons (and also to practice using einsum()), we will
convert all (multi)linear operations to tensor contractions.

``Once we know how to backprop through einsum(), we understood how to do
this for tensor arithmetics.''

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Backpropagating tensor operations.}

\PY{n}{G\PYZus{}NEWTON} \PY{o}{=} \PY{l+m+mf}{6.672e\PYZhy{}11} \PY{c+c1}{\PYZsh{} m\PYZca{}3 kg\PYZca{}(\PYZhy{}1) s\PYZca{}(\PYZhy{}2)}

\PY{k}{def} \PY{n+nf}{gravitational\PYZus{}potential}\PY{p}{(}\PY{n}{m1}\PY{p}{,} \PY{n}{m2}\PY{p}{,} \PY{n}{pos1}\PY{p}{,} \PY{n}{pos2}\PY{p}{)}\PY{p}{:}
  \PY{n}{v1} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{pos1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{float64}\PY{p}{)}
  \PY{n}{v2} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{pos2}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{float64}\PY{p}{)}
  \PY{n}{v12} \PY{o}{=} \PY{n}{v2} \PY{o}{\PYZhy{}} \PY{n}{v1}
  \PY{n}{dist\PYZus{}squared} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{i,i\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{v12}\PY{p}{,} \PY{n}{v12}\PY{p}{)}
  \PY{n}{dist} \PY{o}{=} \PY{n}{dist\PYZus{}squared}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}
  \PY{n}{m12} \PY{o}{=} \PY{n}{m1} \PY{o}{*} \PY{n}{m2}
  \PY{n}{Gm1m2} \PY{o}{=} \PY{n}{G\PYZus{}NEWTON} \PY{o}{*} \PY{n}{m12}
  \PY{n}{energy} \PY{o}{=} \PY{n}{Gm1m2} \PY{o}{/} \PY{n}{dist}
  \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{energy}

\PY{c+c1}{\PYZsh{} We backpropagate into every parameter here.}
\PY{k}{def} \PY{n+nf}{gravitational\PYZus{}potential\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{m1}\PY{p}{,} \PY{n}{m2}\PY{p}{,} \PY{n}{pos1}\PY{p}{,} \PY{n}{pos2}\PY{p}{)}\PY{p}{:}
  \PY{n}{s\PYZus{}m1} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;} \PY{n}{s\PYZus{}m2} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{v1} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{pos1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{float64}\PY{p}{)} \PY{p}{;} \PY{n}{s\PYZus{}v1} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{v1}\PY{p}{)}
  \PY{n}{v2} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{pos2}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{float64}\PY{p}{)} \PY{p}{;} \PY{n}{s\PYZus{}v2} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{v2}\PY{p}{)}
  \PY{n}{v12} \PY{o}{=} \PY{n}{v2} \PY{o}{\PYZhy{}} \PY{n}{v1}\PY{p}{;} \PY{n}{s\PYZus{}v12} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{v12}\PY{p}{)}
  \PY{n}{dist\PYZus{}squared} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{i,i\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{v12}\PY{p}{,} \PY{n}{v12}\PY{p}{)}\PY{p}{;} \PY{n}{s\PYZus{}dist\PYZus{}squared} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{dist} \PY{o}{=} \PY{n}{dist\PYZus{}squared}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}\PY{p}{;} \PY{n}{s\PYZus{}dist} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{m12} \PY{o}{=} \PY{n}{m1} \PY{o}{*} \PY{n}{m2} \PY{p}{;} \PY{n}{s\PYZus{}m12} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{Gm1m2} \PY{o}{=} \PY{n}{G\PYZus{}NEWTON} \PY{o}{*} \PY{n}{m12}\PY{p}{;} \PY{n}{s\PYZus{}Gm1m2} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{energy} \PY{o}{=} \PY{n}{Gm1m2} \PY{o}{/} \PY{n}{dist}\PY{p}{;} \PY{n}{s\PYZus{}energy} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{c+c1}{\PYZsh{} return \PYZhy{}energy}
  \PY{n}{s\PYZus{}energy} \PY{o}{+}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
  \PY{n}{s\PYZus{}Gm1m2} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}energy} \PY{o}{/} \PY{n}{dist}
  \PY{c+c1}{\PYZsh{} Here, we nomimally would have to use \PYZdq{}inv\PYZus{}dist\PYZdq{} as an intermediate quantity}
  \PY{c+c1}{\PYZsh{} and backprop through f(x)=1/x, but let\PYZsq{}s shorten this.}
  \PY{n}{s\PYZus{}dist} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}energy} \PY{o}{*} \PY{n}{Gm1m2} \PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{/} \PY{n}{dist\PYZus{}squared}\PY{p}{)}
  \PY{n}{s\PYZus{}m12} \PY{o}{=} \PY{n}{G\PYZus{}NEWTON} \PY{o}{*} \PY{n}{s\PYZus{}Gm1m2}
  \PY{n}{s\PYZus{}m1} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}m12} \PY{o}{*} \PY{n}{m2}
  \PY{n}{s\PYZus{}m2} \PY{o}{+}\PY{o}{=} \PY{n}{m1} \PY{o}{*} \PY{n}{s\PYZus{}m12}
  \PY{n}{s\PYZus{}dist\PYZus{}squared} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}dist} \PY{o}{*} \PY{l+m+mf}{0.5} \PY{o}{/} \PY{n}{dist}
  \PY{c+c1}{\PYZsh{} These two could of course be combined into a simple}
  \PY{c+c1}{\PYZsh{} s\PYZus{}v12 += 2 * s\PYZus{}dist\PYZus{}squared * v12}
  \PY{n}{s\PYZus{}v12} \PY{o}{+}\PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,i\PYZhy{}\PYZgt{}i}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s\PYZus{}dist\PYZus{}squared}\PY{p}{,} \PY{n}{v12}\PY{p}{)}
  \PY{n}{s\PYZus{}v12} \PY{o}{+}\PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{i,\PYZhy{}\PYZgt{}i}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{v12}\PY{p}{,} \PY{n}{s\PYZus{}dist\PYZus{}squared}\PY{p}{)}
  \PY{n}{s\PYZus{}v2} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}v12}
  \PY{n}{s\PYZus{}v1} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}v12}  \PY{c+c1}{\PYZsh{} Note \PYZhy{}= !}
  \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{energy}\PY{p}{,} \PY{p}{(}\PY{n}{s\PYZus{}m1}\PY{p}{,} \PY{n}{s\PYZus{}m2}\PY{p}{,} \PY{n}{s\PYZus{}v1}\PY{p}{,} \PY{n}{s\PYZus{}v2}\PY{p}{)}



\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{gravitational\PYZus{}potential\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{l+m+mf}{2e7}\PY{p}{,} \PY{l+m+mf}{3e7}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{k}{def} \PY{n+nf}{debug\PYZus{}potential\PYZus{}1parameter}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n}{gravitational\PYZus{}potential}\PY{p}{(}\PY{l+m+mf}{2e7}\PY{p}{,} \PY{l+m+mf}{3e7}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{15} \PY{o}{+} \PY{n}{x}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{derivative\PYZus{}v1}\PY{p}{(}\PY{n}{debug\PYZus{}potential\PYZus{}1parameter}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(-2220.5758255288374,
 (-0.00011102879127644187,
  -7.401919418429458e-05,
  array([   0.        ,  -68.32541002, -102.48811502]),
  array([  0.        ,  68.32541002, 102.48811502])))
===
-102.48811577184824
    \end{Verbatim}

    So, not breaking up tensor arithmetics into individual additions and
multiplications and then doing backpropagation on this makes less sense
than keeping the tensor arithmetics intact.

Next, let us look into some nontrivial control structure. We will use
Heron's method for root finding by iteratively averaging
\{candidate-root\} and \{square\}/\{candidate-root\}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{heron\PYZus{}root}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes sqrt(x) by iterative averaging.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
    \PY{n}{x\PYZus{}div\PYZus{}y} \PY{o}{=} \PY{n}{x} \PY{o}{/} \PY{n}{y}
    \PY{n}{mid} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{y} \PY{o}{+} \PY{n}{x\PYZus{}div\PYZus{}y}\PY{p}{)}
    \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{x\PYZus{}div\PYZus{}y} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{max\PYZus{}distance}\PY{p}{:}
      \PY{k}{return} \PY{n}{mid}
    \PY{k}{else}\PY{p}{:}
      \PY{n}{y} \PY{o}{=} \PY{n}{mid}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{heron\PYZus{}root}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
1.414213562373095
    \end{Verbatim}

    Our first task is to convert this simple function to a form that
``remembers all intermediate quantities, and never overwrites
anything''.

For the sake of clarity, this form deliberately ignores some
optimization opportunities. Rather, we emphasize ``understandability''
here (and also get to see some useful new Python).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{dataclasses}

\PY{c+c1}{\PYZsh{} Nominally, we are introducing a class here,}
\PY{c+c1}{\PYZsh{} but there is no actual state\PYZhy{}management: The `HeronState`}
\PY{c+c1}{\PYZsh{} instance is effectively just a fancy tuple with named slots.}
\PY{n+nd}{@dataclasses}\PY{o}{.}\PY{n}{dataclass}\PY{p}{(}\PY{n}{frozen}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{k}{class} \PY{n+nc}{HeronState}\PY{p}{:}
  \PY{n}{y} \PY{p}{:} \PY{n+nb}{float}
  \PY{n}{x\PYZus{}div\PYZus{}y} \PY{p}{:} \PY{n+nb}{float}
  \PY{n}{mid} \PY{p}{:} \PY{n+nb}{float}
  \PY{n}{delta} \PY{p}{:} \PY{n+nb}{float}

\PY{k}{def} \PY{n+nf}{heron\PYZus{}root\PYZus{}v1}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes sqrt(x) by iterative averaging.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{states} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
    \PY{n}{x\PYZus{}div\PYZus{}y} \PY{o}{=} \PY{n}{x} \PY{o}{/} \PY{n}{y}
    \PY{n}{mid} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{y} \PY{o}{+} \PY{n}{x\PYZus{}div\PYZus{}y}\PY{p}{)}
    \PY{n}{delta} \PY{o}{=} \PY{n}{x\PYZus{}div\PYZus{}y} \PY{o}{\PYZhy{}} \PY{n}{y}
    \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{delta}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{max\PYZus{}distance}\PY{p}{:}
      \PY{k}{return} \PY{n}{mid}
    \PY{k}{else}\PY{p}{:}
      \PY{n}{y} \PY{o}{=} \PY{n}{mid}
\end{Verbatim}
\end{tcolorbox}

    Next, we can think about backpropagating this.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{heron\PYZus{}root\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes sqrt(x) and by iterative averaging \PYZhy{} plus its gradient.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{states} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
    \PY{n}{x\PYZus{}div\PYZus{}y} \PY{o}{=} \PY{n}{x} \PY{o}{/} \PY{n}{y}
    \PY{n}{mid} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{y} \PY{o}{+} \PY{n}{x\PYZus{}div\PYZus{}y}\PY{p}{)}
    \PY{n}{delta} \PY{o}{=} \PY{n}{x\PYZus{}div\PYZus{}y} \PY{o}{\PYZhy{}} \PY{n}{y}
    \PY{n}{states}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{HeronState}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{x\PYZus{}div\PYZus{}y}\PY{o}{=}\PY{n}{x\PYZus{}div\PYZus{}y}\PY{p}{,} \PY{n}{mid}\PY{o}{=}\PY{n}{mid}\PY{p}{,} \PY{n}{delta}\PY{o}{=}\PY{n}{delta}\PY{p}{)}\PY{p}{)}
    \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{delta}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{max\PYZus{}distance}\PY{p}{:}
      \PY{c+c1}{\PYZsh{} Rather than returning to the caller, we need to exit to after\PYZhy{}the\PYZhy{}loop.}
      \PY{c+c1}{\PYZsh{} return mid}
      \PY{k}{break}
    \PY{k}{else}\PY{p}{:}
      \PY{n}{y} \PY{o}{=} \PY{n}{mid}
  \PY{c+c1}{\PYZsh{} At this point,}
  \PY{c+c1}{\PYZsh{} \PYZhy{} The result we would have returned is in `mid`.}
  \PY{c+c1}{\PYZsh{} \PYZhy{} List entry `states[n]` contains intermediate quantites right at the end}
  \PY{c+c1}{\PYZsh{}   of iteration n, before we updated `y`.}
  \PY{n}{result} \PY{o}{=} \PY{n}{mid}
  \PY{n}{s\PYZus{}x} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{n}{s\PYZus{}mid} \PY{o}{=} \PY{l+m+mi}{1}
  \PY{k}{for} \PY{n}{state} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n}{states}\PY{p}{)}\PY{p}{:}
    \PY{n}{s\PYZus{}y} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{s\PYZus{}mid}
    \PY{n}{s\PYZus{}x\PYZus{}div\PYZus{}y} \PY{o}{=} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{s\PYZus{}mid}
    \PY{n}{s\PYZus{}x} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}x\PYZus{}div\PYZus{}y} \PY{o}{/} \PY{n}{state}\PY{o}{.}\PY{n}{y}
    \PY{n}{s\PYZus{}y} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}x\PYZus{}div\PYZus{}y} \PY{o}{*} \PY{n}{x} \PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{n}{state}\PY{o}{.}\PY{n}{y} \PY{o}{*} \PY{n}{state}\PY{o}{.}\PY{n}{y}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} At the end of the previous loop, s\PYZus{}y was assigned from `mid`.}
    \PY{n}{s\PYZus{}mid} \PY{o}{=} \PY{n}{s\PYZus{}y}
  \PY{c+c1}{\PYZsh{} Here, it turns out that we actually would not have had to remember `delta`,}
  \PY{c+c1}{\PYZsh{} since this was not used as an intermediate quantity, but only to make}
  \PY{c+c1}{\PYZsh{} some continue\PYZhy{}or\PYZhy{}not decisions that became retraceable via the bookkeeping}
  \PY{c+c1}{\PYZsh{} we do with `states`.}
  \PY{c+c1}{\PYZsh{} Let us return some more detailed information.}
  \PY{k}{return} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{result}\PY{o}{=}\PY{n}{result}\PY{p}{,}
              \PY{n}{num\PYZus{}iterations}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{states}\PY{p}{)}\PY{p}{,}
              \PY{n}{grad}\PY{o}{=}\PY{n}{s\PYZus{}x}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{heron\PYZus{}root\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{l+m+mf}{100.0}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Given that this method is reasonably efficient, we have to pick really large}
\PY{c+c1}{\PYZsh{} max\PYZus{}distance values to make it \PYZdq{}use too few iterations\PYZdq{}.}
\PY{c+c1}{\PYZsh{} The question here is: Do these cases then produce good gradients?}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{heron\PYZus{}root\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{l+m+mf}{100.0}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{derivative\PYZus{}v1}\PY{p}{(}
    \PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{heron\PYZus{}root}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}\PY{p}{,}
    \PY{l+m+mf}{100.0}\PY{p}{,}
    \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{heron\PYZus{}root\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{l+m+mf}{100.0}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{derivative\PYZus{}v1}\PY{p}{(}
    \PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{heron\PYZus{}root}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{max\PYZus{}distance}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}\PY{p}{)}\PY{p}{,}
    \PY{l+m+mf}{100.0}\PY{p}{,}
    \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'result': 10.0, 'num\_iterations': 8, 'grad': 0.05\}
\#\#\#
\{'result': 10.840434673026925, 'num\_iterations': 4, 'grad': 0.06835572803193413\}
0.06835572108343513
\#\#\#
\{'result': 26.24009900990099, 'num\_iterations': 2, 'grad': 0.2500980296049407\}
0.25009802229192246
    \end{Verbatim}

    So, backpropagating through loops is generally a bit more messy, but
actually doable - even if the number of iterations is data-dependent.

The basic principle is: When computing the gradient, as we are retracing
the computational steps, we make every computational flow decision
(which branch to take, when to leave a loop) as on the forward
computation.

    \hypertarget{actually-doing-optimization}{%
\section{Actually doing
optimization}\label{actually-doing-optimization}}

Now that we have a way to efficiently get good gradients of even
high-dimensional objective functions, let us actually explore what we
can do with this. Numerical optimization obviously is an important topic
in itself.

For the time being, let us focus on simple non-ML problems to practice
gradient-based numerical optimization with \texttt{\textless{}=1000}
parameters.

The short story here is: For nonlinear minimization, if the function can
be approximated by some quadric near the minimum in some reasonable
sense, and if it does not have too weird `hole on the golf course'
minima, the BFGS-family of algorithms (Broyden-Fletcher-Goldfarb-Shanno)
generally are a very potent tool one should know about. The most general
one that is available in the \texttt{scipy.optimize} module is L-BFGS-B.

The basic idea is: We compute gradients to find out in which direction
to move in order to minimize the function. From successive gradient
evaluations, we estimate a (\texttt{NxN}) approximate Hessian and use
this to turn the gradient (a ``covector''!) into a step(-vector).

Let us consider the following problem: We have two wire circles that
extend in x-y direction, have their centers at \texttt{(0,\ 0,\ z1)},
respectively \texttt{(0,\ 0,\ z2)}, with radii \texttt{r1} and
\texttt{r2}, and want to find the shape of some soap film that stretches
between these. There are two ways to see the physics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``Local law'': Mean (extrinsic) curvature \textasciitilde{} pressure
  difference on both sides of the soap film = 0.
\item
  ``Variational principle'': Total surface tension energy
  \textasciitilde{} surface area, is minimal.
\end{enumerate}

Let us use the latter principle - and also make some basic assumptions
here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We can get a good approximation to the geometry by finding
  area-minimizing vertices of a triangle mesh.
\item
  We can simplify this further by meshing a conical frustum and allowing
  every vertex a single degree of freedom - its distance from the
  symmetry axis.
\end{enumerate}

(Via 2., we ``fix gauge symmetry'' related to being able to move the
discretization points along the surface.)

Here, we will go for a slightly unusual approach and compute triangle
areas from side-lengths, using (again) a formula by Heron.

(Admittedly, this first example is slightly silly, as we are not
exploiting the symmetry of the problem. The first exercise will break
that symmetry.)

\textbf{Participant Exercise}: Replace Heron's formula with code that
computes triangle area via the cross product of two sides (and also
backpropagate this to obtain its gradient).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{heron\PYZus{}area}\PY{p}{(}\PY{n}{d1}\PY{p}{,} \PY{n}{d2}\PY{p}{,} \PY{n}{d3}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes the area of a triangle with sides `d1, d2, d3`. Vectorized.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{s} \PY{o}{=} \PY{p}{(}\PY{n}{d1} \PY{o}{+} \PY{n}{d2} \PY{o}{+} \PY{n}{d3}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.5}
  \PY{k}{return} \PY{n}{numpy}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{s} \PY{o}{*} \PY{p}{(}\PY{n}{s} \PY{o}{\PYZhy{}} \PY{n}{d1}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{s} \PY{o}{\PYZhy{}} \PY{n}{d2}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{s} \PY{o}{\PYZhy{}} \PY{n}{d3}\PY{p}{)}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{vertices\PYZus{}and\PYZus{}total\PYZus{}mesh\PYZus{}area}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,}
                                 \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{,}
                                 \PY{c+c1}{\PYZsh{} This is debugging code deliberately left}
                                 \PY{c+c1}{\PYZsh{} in this example for illustration \PYZhy{} how to}
                                 \PY{c+c1}{\PYZsh{} track down gradient discrepancies.}
                                 \PY{n}{ddd\PYZus{}delta\PYZus{}xyzs\PYZus{}wrapped}\PY{o}{=}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                                 \PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes vertex\PYZhy{}positions and total mesh area.}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    z1: float, z\PYZhy{}height of the first ring.}
\PY{l+s+sd}{    z2: float, z\PYZhy{}height of the 2nd ring.}
\PY{l+s+sd}{    r1: float, radius of the first ring.}
\PY{l+s+sd}{    r2: float, radius of the 2nd ring.}
\PY{l+s+sd}{    inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations:}
\PY{l+s+sd}{      float [num\PYZus{}inner\PYZus{}rings, num\PYZus{}vertices\PYZus{}per\PYZhy{}ring] numpy.ndarray\PYZhy{}like,}
\PY{l+s+sd}{      per inner mesh\PYZhy{}vertex, the (signed) distance by which the vertex}
\PY{l+s+sd}{      has been moved radially out from the x=y=0 symmetry axis relative to}
\PY{l+s+sd}{      a point on the surface of the conical frustum that is the convex hull}
\PY{l+s+sd}{      of the circles around x=y=0 specified by (z1, r1) and (z2, r2).}

\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    A pair `(vertex\PYZus{}3d\PYZus{}coords, total\PYZus{}area)`, where `vertex\PYZus{}3d\PYZus{}coords` is a}
\PY{l+s+sd}{    float [num\PYZus{}rings\PYZus{}total, num\PYZus{}vertices\PYZus{}per\PYZus{}ring + 1, 3]\PYZhy{}numpy.ndarray with}
\PY{l+s+sd}{    vertex coordinates (including a wraparound\PYZhy{}column),}
\PY{l+s+sd}{    and `total\PYZus{}area` is the total surface area.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
  \PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring} \PY{o}{=} \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{o}{.}\PY{n}{shape}
  \PY{n}{num\PYZus{}rings} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{+} \PY{n}{num\PYZus{}inner\PYZus{}rings}  \PY{c+c1}{\PYZsh{} The fixed \PYZsq{}wire rings\PYZsq{}.}
  \PY{n}{rs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,} \PY{n}{num\PYZus{}rings}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Per\PYZhy{}vertex z\PYZhy{}coordinates.}
  \PY{n}{zs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{tile}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{num\PYZus{}rings}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}\PY{p}{,}
                  \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{]}\PY{p}{)}
  \PY{n}{vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{pad}\PY{p}{(}
      \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{,} \PY{p}{[}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{p}{)}
  \PY{n}{xys\PYZus{}unscaled\PYZus{}complex} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{exp}\PY{p}{(}
      \PY{l+m+mi}{1}\PY{n}{j} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
  \PY{n}{xys\PYZus{}scaled\PYZus{}complex} \PY{o}{=} \PY{n}{xys\PYZus{}unscaled\PYZus{}complex}\PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{*} \PY{p}{(}
      \PY{n}{rs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]} \PY{o}{+} \PY{n}{vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
  \PY{n}{xyzs} \PY{o}{=} \PY{p}{(}  \PY{c+c1}{\PYZsh{} shape [num\PYZus{}rings, num\PYZus{}vertices\PYZus{}per\PYZus{}ring, 3]}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{xys\PYZus{}scaled\PYZus{}complex}\PY{o}{.}\PY{n}{real}\PY{p}{,}
                   \PY{n}{xys\PYZus{}scaled\PYZus{}complex}\PY{o}{.}\PY{n}{imag}\PY{p}{,}
                   \PY{n}{zs}\PY{p}{]}\PY{p}{,}
                  \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
  \PY{n}{xyzs\PYZus{}wrapped} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{xyzs}\PY{p}{,} \PY{n}{xyzs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{k}{for} \PY{n}{delta}\PY{p}{,} \PY{n}{coords} \PY{o+ow}{in} \PY{n}{ddd\PYZus{}delta\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{:}
    \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{n}{coords}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{delta}
  \PY{c+c1}{\PYZsh{} There is a 1\PYZhy{}to\PYZhy{}1 mapping between every point not on the z2\PYZhy{}ring and}
  \PY{c+c1}{\PYZsh{} non\PYZhy{}planar 4\PYZhy{}plaquettes made of two triangles.}
  \PY{c+c1}{\PYZsh{} The four relevant points are:}
  \PY{c+c1}{\PYZsh{} \PYZdq{}here\PYZdq{} == \PYZdq{}SW\PYZdq{},}
  \PY{c+c1}{\PYZsh{} \PYZdq{}up\PYZdq{} (as a shorthand for \PYZdq{}towards z2\PYZdq{}) == \PYZdq{}NW\PYZdq{},}
  \PY{c+c1}{\PYZsh{} \PYZdq{}ccw\PYZdq{} (as a shorthand for \PYZdq{}next\PYZhy{}w.r.t. increasing angle\PYZdq{}) == \PYZdq{}SE\PYZdq{},}
  \PY{c+c1}{\PYZsh{} and \PYZdq{}ccw\PYZhy{}up\PYZdq{} == \PYZdq{}NE\PYZdq{}.}
  \PY{n}{p\PYZus{}sw} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{n}{p\PYZus{}nw} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{n}{p\PYZus{}se} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{n}{p\PYZus{}ne} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{n}{d\PYZus{}sw\PYZus{}nw} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}sw} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}nw}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{d\PYZus{}sw\PYZus{}se} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}sw} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}se}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{d\PYZus{}se\PYZus{}ne} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}se} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}ne}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{d\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}ne} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}nw}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{d\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}se} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}nw}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{area\PYZus{}sw\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{n}{d\PYZus{}sw\PYZus{}se}\PY{p}{,} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{,} \PY{n}{d\PYZus{}sw\PYZus{}nw}\PY{p}{)}
  \PY{n}{area\PYZus{}se\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{n}{d\PYZus{}se\PYZus{}ne}\PY{p}{,} \PY{n}{d\PYZus{}ne\PYZus{}nw}\PY{p}{,} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{)}
  \PY{k}{return} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{,} \PY{n}{area\PYZus{}sw\PYZus{}se\PYZus{}nw}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{area\PYZus{}se\PYZus{}ne\PYZus{}nw}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}


\PY{c+c1}{\PYZsh{} We use the BFGS algorithm, as provided by scipy.optimize, but}
\PY{c+c1}{\PYZsh{} we let scipy determine gradients numerically, but we make the}
\PY{c+c1}{\PYZsh{} gradient\PYZhy{}function an optional parameter to this function:}
\PY{c+c1}{\PYZsh{} If it is not provided, BFGS will use finite differencing to compute}
\PY{c+c1}{\PYZsh{} the gradient.}
\PY{k}{def} \PY{n+nf}{get\PYZus{}minimal\PYZus{}area}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,}
                     \PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,}
                     \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{,}
                     \PY{n}{grad\PYZus{}area}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
                     \PY{n}{report\PYZus{}every\PYZus{}n\PYZus{}calls}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{:}
  \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
  \PY{n}{t\PYZus{}prev} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
  \PY{n}{num\PYZus{}calls} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{k}{def} \PY{n+nf}{f\PYZus{}area}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{also\PYZus{}return\PYZus{}vertices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{k}{nonlocal} \PY{n}{t\PYZus{}prev}\PY{p}{,} \PY{n}{num\PYZus{}calls}
    \PY{n}{num\PYZus{}calls} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{n}{vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{params}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}
        \PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{)}
    \PY{n}{vertices\PYZus{}wrapped}\PY{p}{,} \PY{n}{area} \PY{o}{=} \PY{n}{vertices\PYZus{}and\PYZus{}total\PYZus{}mesh\PYZus{}area}\PY{p}{(}
        \PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,} \PY{n}{vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
    \PY{k}{if} \PY{n}{num\PYZus{}calls} \PY{o}{\PYZpc{}} \PY{n}{report\PYZus{}every\PYZus{}n\PYZus{}calls} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
      \PY{n}{t\PYZus{}now} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[T=}\PY{l+s+si}{\PYZob{}}\PY{n}{t\PYZus{}now}\PY{+w}{ }\PY{o}{\PYZhy{}}\PY{+w}{ }\PY{n}{t0}\PY{l+s+si}{:}\PY{l+s+s1}{8.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ s, }\PY{l+s+s1}{\PYZsq{}}
            \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{+}\PY{l+s+si}{\PYZob{}}\PY{n}{t\PYZus{}now}\PY{o}{\PYZhy{}}\PY{n}{t\PYZus{}prev}\PY{l+s+si}{:}\PY{l+s+s1}{8.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ s for }\PY{l+s+si}{\PYZob{}}\PY{n}{report\PYZus{}every\PYZus{}n\PYZus{}calls}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ calls]: }\PY{l+s+s1}{\PYZsq{}}
            \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a=}\PY{l+s+si}{\PYZob{}}\PY{n}{area}\PY{l+s+si}{:}\PY{l+s+s1}{16.10f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
      \PY{n}{t\PYZus{}prev} \PY{o}{=} \PY{n}{t\PYZus{}now}
    \PY{k}{if} \PY{n}{also\PYZus{}return\PYZus{}vertices}\PY{p}{:}
      \PY{c+c1}{\PYZsh{} This is slightly messy design: Depending on parameters, we either}
      \PY{c+c1}{\PYZsh{} return only the area (as we need it for optimization), or also}
      \PY{c+c1}{\PYZsh{} the other useful information, the vertices.}
      \PY{k}{return} \PY{n}{area}\PY{p}{,} \PY{n}{vertices\PYZus{}wrapped}
    \PY{k}{else}\PY{p}{:}
      \PY{k}{return} \PY{n}{area}
  \PY{k}{def} \PY{n+nf}{fprime\PYZus{}area}\PY{p}{(}\PY{n}{params}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Adapter function \PYZhy{} wraps up the gradient\PYZhy{}function `grad\PYZus{}area`,}
    \PY{c+c1}{\PYZsh{} which has the same input parameters as `vertices\PYZus{}and\PYZus{}total\PYZus{}mesh\PYZus{}area`}
    \PY{c+c1}{\PYZsh{} to take the same inputs as f\PYZus{}area.}
    \PY{n}{vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{params}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}
        \PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{)}
    \PY{n}{s\PYZus{}vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{grad\PYZus{}area}\PY{p}{(}
        \PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,} \PY{n}{vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Here, we are only interested in the dependency on the radial deviations.}
    \PY{k}{return} \PY{n}{s\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
  \PY{n}{opt\PYZus{}pos} \PY{o}{=} \PY{n}{scipy}\PY{o}{.}\PY{n}{optimize}\PY{o}{.}\PY{n}{fmin\PYZus{}bfgs}\PY{p}{(}
      \PY{n}{f\PYZus{}area}\PY{p}{,}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{num\PYZus{}inner\PYZus{}rings} \PY{o}{*} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{)}\PY{p}{,}
      \PY{n}{fprime}\PY{o}{=}\PY{k+kc}{None} \PY{k}{if} \PY{n}{grad\PYZus{}area} \PY{o+ow}{is} \PY{k+kc}{None} \PY{k}{else} \PY{n}{fprime\PYZus{}area}\PY{p}{)}
  \PY{n}{opt\PYZus{}val}\PY{p}{,} \PY{n}{vertices\PYZus{}wrapped} \PY{o}{=} \PY{n}{f\PYZus{}area}\PY{p}{(}\PY{n}{opt\PYZus{}pos}\PY{p}{,} \PY{n}{also\PYZus{}return\PYZus{}vertices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
  \PY{k}{return} \PY{n}{opt\PYZus{}val}\PY{p}{,} \PY{n}{vertices\PYZus{}wrapped}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Running a demo and plotting the minimal surface.}

\PY{n}{demo\PYZus{}opt\PYZus{}val}\PY{p}{,} \PY{n}{demo\PYZus{}vertices} \PY{o}{=} \PY{n}{get\PYZus{}minimal\PYZus{}area}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mf}{14.0}\PY{p}{,} \PY{l+m+mf}{14.0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}


\PY{n}{pyplot\PYZus{}axes} \PY{o}{=} \PY{n}{pyplot}\PY{o}{.}\PY{n}{axes}\PY{p}{(}\PY{n}{projection}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot\PYZus{}axes}\PY{o}{.}\PY{n}{plot\PYZus{}wireframe}\PY{p}{(}\PY{n}{demo\PYZus{}vertices}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                           \PY{n}{demo\PYZus{}vertices}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                           \PY{n}{demo\PYZus{}vertices}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
                           \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[T=   0.558 s, +   0.558 s for 1000 calls]: a= 1465.9422784433
[T=   1.239 s, +   0.681 s for 1000 calls]: a= 1437.2688034468
[T=   1.844 s, +   0.605 s for 1000 calls]: a= 1437.1263986860
[T=   2.447 s, +   0.603 s for 1000 calls]: a= 1437.1156192768
[T=   3.016 s, +   0.570 s for 1000 calls]: a= 1437.1141902485
[T=   3.590 s, +   0.574 s for 1000 calls]: a= 1437.1141420298
[T=   4.164 s, +   0.574 s for 1000 calls]: a= 1437.1141385439
[T=   4.748 s, +   0.583 s for 1000 calls]: a= 1437.1141383110
[T=   5.265 s, +   0.517 s for 1000 calls]: a= 1437.1141383041
Warning: Desired error not necessarily achieved due to precision loss.
         Current function value: 1437.114138
         Iterations: 32
         Function evaluations: 9648
         Gradient evaluations: 48
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<mpl\_toolkits.mplot3d.art3d.Line3DCollection at 0x7946300fea40>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_02_Derivatives_files/ML_02_Derivatives_39_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{let-us-improve-this-by-adding-fast-gradients.}{%
\subsection{Let us improve this by adding fast
gradients.}\label{let-us-improve-this-by-adding-fast-gradients.}}

This notebook already has the code for the main body, but what is
missing (exercise!) is the gradient of Heron's triangle-area formula.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{grad\PYZus{}heron\PYZus{}area}\PY{p}{(}\PY{n}{d1}\PY{p}{,} \PY{n}{d2}\PY{p}{,} \PY{n}{d3}\PY{p}{,} \PY{n}{s\PYZus{}output}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes the gradient for `heron\PYZus{}area`.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} TODO: Document this properly!}
  \PY{c+c1}{\PYZsh{} s\PYZus{}output is the sensitivity of the final result of the computation}
  \PY{c+c1}{\PYZsh{} where this occurs as an intermediate result on the output of this function.}
  \PY{c+c1}{\PYZsh{} s = (d1 + d2 + d3) * 0.5}
  \PY{c+c1}{\PYZsh{} return numpy.sqrt(s * (s \PYZhy{} d1) * (s \PYZhy{} d2) * (s \PYZhy{} d3))}
  \PY{k}{raise} \PY{n+ne}{NotImplementedError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TODO: Implement this! (Participant Exercise)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Solution}


\PY{k}{def} \PY{n+nf}{grad\PYZus{}total\PYZus{}mesh\PYZus{}area}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,}
                         \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{,}
                         \PY{n}{ddd\PYZus{}delta\PYZus{}xyzs\PYZus{}wrapped}\PY{o}{=}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                         \PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}(Partial) gradient for the `total\PYZus{}mesh\PYZus{}area` function\PYZsq{}s area.}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    z1: float, z\PYZhy{}height of the first ring.}
\PY{l+s+sd}{    z2: float, z\PYZhy{}height of the 2nd ring.}
\PY{l+s+sd}{    r1: float, radius of the first ring.}
\PY{l+s+sd}{    r2: float, radius of the 2nd ring.}
\PY{l+s+sd}{    inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations:}
\PY{l+s+sd}{      float [num\PYZus{}inner\PYZus{}rings, num\PYZus{}vertices\PYZus{}per\PYZhy{}ring] numpy.ndarray\PYZhy{}like,}
\PY{l+s+sd}{      per inner mesh\PYZhy{}vertex, the (signed) distance by which the vertex}
\PY{l+s+sd}{      has been moved radially out from the x=y=0 symmetry axis relative to}
\PY{l+s+sd}{      a point on the surface of the conical frustum that is the convex hull}
\PY{l+s+sd}{      of the circles around x=y=0 specified by (z1, r1) and (z2, r2).}

\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    float [num\PYZus{}inner\PYZus{}rings, num\PYZus{}vertices\PYZus{}per\PYZhy{}ring] numpy.ndarray,}
\PY{l+s+sd}{    sensitivities of the mesh area on `inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations`.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} In this example, we only use the sensitivities on the radial deviations,}
  \PY{c+c1}{\PYZsh{} but for the sake of illustration, we compute the full gradient.}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} Also, for the sake of \PYZdq{}sticking to the recipe\PYZdq{}, this code does not do}
  \PY{c+c1}{\PYZsh{} some obvious simplifications.}
  \PY{c+c1}{\PYZsh{} When reading this code, it might make sense to duplicate the browser tab}
  \PY{c+c1}{\PYZsh{} and use both tabs side\PYZhy{}by\PYZhy{}side to view forward and reverse code in}
  \PY{c+c1}{\PYZsh{} separate windows.}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} Participant Exercise: Simplify the code further}
  \PY{c+c1}{\PYZsh{} (such as by eliminating s\PYZus{}area\PYZus{}se\PYZus{}ne\PYZus{}nw, etc.).}
  \PY{n}{z\PYZus{}like} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}  \PY{c+c1}{\PYZsh{} Abbreviation}
  \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
  \PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring} \PY{o}{=} \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{o}{.}\PY{n}{shape}
  \PY{n}{num\PYZus{}rings} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{+} \PY{n}{num\PYZus{}inner\PYZus{}rings}  \PY{c+c1}{\PYZsh{} The fixed \PYZsq{}wire rings\PYZsq{}.}
  \PY{n}{rs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,} \PY{n}{num\PYZus{}rings}\PY{p}{)}
  \PY{n}{s\PYZus{}rs} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{rs}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Per\PYZhy{}vertex z\PYZhy{}coordinates.}
  \PY{n}{zs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{tile}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{num\PYZus{}rings}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}\PY{p}{,}
                  \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{]}\PY{p}{)}
  \PY{n}{s\PYZus{}zs} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{zs}\PY{p}{)}
  \PY{n}{vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{pad}\PY{p}{(}
      \PY{n}{inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{,} \PY{p}{[}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{p}{)}
  \PY{n}{s\PYZus{}vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} This is just a constant that only depends on the discrete parameter}
  \PY{c+c1}{\PYZsh{} `num\PYZus{}vertices\PYZus{}per\PYZus{}ring` \PYZhy{} no need to keep track of a sensitivity here.}
  \PY{n}{xys\PYZus{}unscaled\PYZus{}complex} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{exp}\PY{p}{(}
      \PY{l+m+mi}{1}\PY{n}{j} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
  \PY{n}{xys\PYZus{}scaled\PYZus{}complex} \PY{o}{=} \PY{n}{xys\PYZus{}unscaled\PYZus{}complex}\PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{*} \PY{p}{(}
      \PY{n}{rs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]} \PY{o}{+} \PY{n}{vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{)}
  \PY{n}{s\PYZus{}xys\PYZus{}scaled\PYZus{}complex} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{xys\PYZus{}scaled\PYZus{}complex}\PY{p}{)}
  \PY{n}{xyzs} \PY{o}{=} \PY{p}{(}  \PY{c+c1}{\PYZsh{} shape [num\PYZus{}rings, num\PYZus{}vertices\PYZus{}per\PYZus{}ring, 3]}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{xys\PYZus{}scaled\PYZus{}complex}\PY{o}{.}\PY{n}{real}\PY{p}{,}
                   \PY{n}{xys\PYZus{}scaled\PYZus{}complex}\PY{o}{.}\PY{n}{imag}\PY{p}{,}
                   \PY{n}{zs}\PY{p}{]}\PY{p}{,}
                  \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
  \PY{n}{s\PYZus{}xyzs} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{xyzs}\PY{p}{)}
  \PY{n}{xyzs\PYZus{}wrapped} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{xyzs}\PY{p}{,} \PY{n}{xyzs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{xyzs\PYZus{}wrapped}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} There is a 1\PYZhy{}to\PYZhy{}1 mapping between every point not on the z2\PYZhy{}ring and}
  \PY{c+c1}{\PYZsh{} non\PYZhy{}planar 4\PYZhy{}plaquettes made of two triangles.}
  \PY{c+c1}{\PYZsh{} The four relevant points are:}
  \PY{c+c1}{\PYZsh{} \PYZdq{}here\PYZdq{} == \PYZdq{}SW\PYZdq{},}
  \PY{c+c1}{\PYZsh{} \PYZdq{}up\PYZdq{} (as a shorthand for \PYZdq{}towards z2\PYZdq{}) == \PYZdq{}NW\PYZdq{},}
  \PY{c+c1}{\PYZsh{} \PYZdq{}ccw\PYZdq{} (as a shorthand for \PYZdq{}next\PYZhy{}w.r.t. increasing angle\PYZdq{}) == \PYZdq{}SE\PYZdq{},}
  \PY{c+c1}{\PYZsh{} and \PYZdq{}ccw\PYZhy{}up\PYZdq{} == \PYZdq{}NE\PYZdq{}.}
  \PY{n}{p\PYZus{}sw} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}sw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}sw}\PY{p}{)}
  \PY{n}{p\PYZus{}nw} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}nw}\PY{p}{)}
  \PY{n}{p\PYZus{}se} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}se} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}se}\PY{p}{)}
  \PY{n}{p\PYZus{}ne} \PY{o}{=} \PY{n}{xyzs\PYZus{}wrapped}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}ne} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}ne}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Here, we introduce extra intermediate quantities which become the}
  \PY{c+c1}{\PYZsh{} arguments of numpy.linalg.norm() below.}
  \PY{c+c1}{\PYZsh{} \PYZdq{}We want to remember every intermediate quantity.\PYZdq{}}
  \PY{n}{p\PYZus{}sw\PYZus{}nw} \PY{o}{=} \PY{n}{p\PYZus{}sw} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}nw}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}sw\PYZus{}nw}\PY{p}{)}
  \PY{n}{p\PYZus{}sw\PYZus{}se} \PY{o}{=} \PY{n}{p\PYZus{}sw} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}se}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}se} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}sw\PYZus{}se}\PY{p}{)}
  \PY{n}{p\PYZus{}se\PYZus{}ne} \PY{o}{=} \PY{n}{p\PYZus{}se} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}ne}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}ne} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}se\PYZus{}ne}\PY{p}{)}
  \PY{n}{p\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{p\PYZus{}ne} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}nw}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}ne\PYZus{}nw}\PY{p}{)}
  \PY{n}{p\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{p\PYZus{}se} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}nw}\PY{p}{;} \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{p\PYZus{}se\PYZus{}nw}\PY{p}{)}
  \PY{n}{d\PYZus{}sw\PYZus{}nw} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}sw\PYZus{}nw}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;} \PY{n}{s\PYZus{}d\PYZus{}sw\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}sw\PYZus{}nw}\PY{p}{)}
  \PY{n}{d\PYZus{}sw\PYZus{}se} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}sw\PYZus{}se}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;} \PY{n}{s\PYZus{}d\PYZus{}sw\PYZus{}se} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}sw\PYZus{}se}\PY{p}{)}
  \PY{n}{d\PYZus{}se\PYZus{}ne} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}se\PYZus{}ne}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;} \PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}ne} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}se\PYZus{}ne}\PY{p}{)}
  \PY{n}{d\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}ne\PYZus{}nw}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;} \PY{n}{s\PYZus{}d\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}ne\PYZus{}nw}\PY{p}{)}
  \PY{n}{d\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p\PYZus{}se\PYZus{}nw}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;} \PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{)}
  \PY{n}{area\PYZus{}sw\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{n}{d\PYZus{}sw\PYZus{}se}\PY{p}{,} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{,} \PY{n}{d\PYZus{}sw\PYZus{}nw}\PY{p}{)}
  \PY{n}{s\PYZus{}area\PYZus{}sw\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{area\PYZus{}sw\PYZus{}se\PYZus{}nw}\PY{p}{)}
  \PY{n}{area\PYZus{}se\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{n}{d\PYZus{}se\PYZus{}ne}\PY{p}{,} \PY{n}{d\PYZus{}ne\PYZus{}nw}\PY{p}{,} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{)}
  \PY{n}{s\PYZus{}area\PYZus{}se\PYZus{}ne\PYZus{}nw} \PY{o}{=} \PY{n}{z\PYZus{}like}\PY{p}{(}\PY{n}{area\PYZus{}se\PYZus{}ne\PYZus{}nw}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Forward code ended with:}
  \PY{c+c1}{\PYZsh{} return xyzs\PYZus{}wrapped, area\PYZus{}sw\PYZus{}se\PYZus{}nw.sum() + area\PYZus{}se\PYZus{}ne\PYZus{}nw.sum()}
  \PY{c+c1}{\PYZsh{} We are merely interested in the sensitivity of the 2nd tuple entry}
  \PY{c+c1}{\PYZsh{} (the area) on input parameters.}
  \PY{n}{area} \PY{o}{=} \PY{n}{area\PYZus{}sw\PYZus{}se\PYZus{}nw}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{area\PYZus{}se\PYZus{}ne\PYZus{}nw}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
  \PY{n}{s\PYZus{}area\PYZus{}se\PYZus{}ne\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
  \PY{n}{s\PYZus{}area\PYZus{}sw\PYZus{}se\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
  \PY{n}{s1\PYZus{}d\PYZus{}se\PYZus{}ne}\PY{p}{,} \PY{n}{s1\PYZus{}d\PYZus{}ne\PYZus{}nw}\PY{p}{,} \PY{n}{s1\PYZus{}d\PYZus{}se\PYZus{}nw} \PY{o}{=} \PY{n}{grad\PYZus{}heron\PYZus{}area}\PY{p}{(}
      \PY{n}{d\PYZus{}se\PYZus{}ne}\PY{p}{,} \PY{n}{d\PYZus{}ne\PYZus{}nw}\PY{p}{,} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{,} \PY{n}{s\PYZus{}area\PYZus{}se\PYZus{}ne\PYZus{}nw}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Here, we are adding the gradients we received from grad\PYZus{}heron\PYZus{}area}
  \PY{c+c1}{\PYZsh{} to our sensitivity\PYZhy{}accumulators.}
  \PY{c+c1}{\PYZsh{} There is an obvious improvement opportunity here:}
  \PY{c+c1}{\PYZsh{} Re\PYZhy{}write `grad\PYZus{}heron\PYZus{}area` in such a way that it receives our sensitivity\PYZhy{}}
  \PY{c+c1}{\PYZsh{} accumulator arrays and directly increments the sensitivities in these}
  \PY{c+c1}{\PYZsh{} arrays, avoiding the copying.}
  \PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}ne} \PY{o}{+}\PY{o}{=} \PY{n}{s1\PYZus{}d\PYZus{}se\PYZus{}ne}
  \PY{n}{s\PYZus{}d\PYZus{}ne\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{s1\PYZus{}d\PYZus{}ne\PYZus{}nw}
  \PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{s1\PYZus{}d\PYZus{}se\PYZus{}nw}
  \PY{n}{s1\PYZus{}d\PYZus{}sw\PYZus{}se}\PY{p}{,} \PY{n}{s1\PYZus{}d\PYZus{}se\PYZus{}nw}\PY{p}{,} \PY{n}{s1\PYZus{}d\PYZus{}sw\PYZus{}nw} \PY{o}{=} \PY{n}{grad\PYZus{}heron\PYZus{}area}\PY{p}{(}
      \PY{n}{d\PYZus{}sw\PYZus{}se}\PY{p}{,} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{,} \PY{n}{d\PYZus{}sw\PYZus{}nw}\PY{p}{,} \PY{n}{s\PYZus{}area\PYZus{}sw\PYZus{}se\PYZus{}nw}\PY{p}{)}
  \PY{n}{s\PYZus{}d\PYZus{}sw\PYZus{}se} \PY{o}{+}\PY{o}{=} \PY{n}{s1\PYZus{}d\PYZus{}sw\PYZus{}se}
  \PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{s1\PYZus{}d\PYZus{}se\PYZus{}nw}
  \PY{n}{s\PYZus{}d\PYZus{}sw\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{s1\PYZus{}d\PYZus{}sw\PYZus{}nw}
  \PY{c+c1}{\PYZsh{} Backpropagating through numpy.linalg.norm():}
  \PY{c+c1}{\PYZsh{} d\PYZob{}norm(vector)\PYZcb{} / d\PYZob{}vector\PYZcb{} = \PYZob{}vector\PYZcb{}/\PYZob{}norm(vector)\PYZcb{}}
  \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{p\PYZus{}sw\PYZus{}nw} \PY{o}{*} \PY{p}{(}\PY{n}{s\PYZus{}d\PYZus{}sw\PYZus{}nw} \PY{o}{/} \PY{n}{d\PYZus{}sw\PYZus{}nw}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
  \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}se} \PY{o}{+}\PY{o}{=} \PY{n}{p\PYZus{}sw\PYZus{}se} \PY{o}{*} \PY{p}{(}\PY{n}{s\PYZus{}d\PYZus{}sw\PYZus{}se} \PY{o}{/} \PY{n}{d\PYZus{}sw\PYZus{}se}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
  \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}ne} \PY{o}{+}\PY{o}{=} \PY{n}{p\PYZus{}se\PYZus{}ne} \PY{o}{*} \PY{p}{(}\PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}ne} \PY{o}{/} \PY{n}{d\PYZus{}se\PYZus{}ne}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
  \PY{n}{s\PYZus{}p\PYZus{}ne\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{p\PYZus{}ne\PYZus{}nw} \PY{o}{*} \PY{p}{(}\PY{n}{s\PYZus{}d\PYZus{}ne\PYZus{}nw} \PY{o}{/} \PY{n}{d\PYZus{}ne\PYZus{}nw}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
  \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}nw} \PY{o}{+}\PY{o}{=} \PY{n}{p\PYZus{}se\PYZus{}nw} \PY{o}{*} \PY{p}{(}\PY{n}{s\PYZus{}d\PYZus{}se\PYZus{}nw} \PY{o}{/} \PY{n}{d\PYZus{}se\PYZus{}nw}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
  \PY{n}{s\PYZus{}p\PYZus{}sw} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}nw}
  \PY{n}{s\PYZus{}p\PYZus{}nw} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}nw}
  \PY{n}{s\PYZus{}p\PYZus{}sw} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}se}
  \PY{n}{s\PYZus{}p\PYZus{}se} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}sw\PYZus{}se}
  \PY{n}{s\PYZus{}p\PYZus{}se} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}ne}
  \PY{n}{s\PYZus{}p\PYZus{}ne} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}ne}
  \PY{n}{s\PYZus{}p\PYZus{}ne} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}ne\PYZus{}nw}
  \PY{n}{s\PYZus{}p\PYZus{}nw} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}ne\PYZus{}nw}
  \PY{n}{s\PYZus{}p\PYZus{}se} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}nw}
  \PY{n}{s\PYZus{}p\PYZus{}nw} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}se\PYZus{}nw}
  \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}sw}
  \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}nw}
  \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}se}
  \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}p\PYZus{}ne}
  \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DDD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k}{for} \PY{n}{delta}\PY{p}{,} \PY{n}{coords} \PY{o+ow}{in} \PY{n}{ddd\PYZus{}delta\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DDD s\PYZus{}xyzs\PYZus{}wrapped[}\PY{l+s+si}{\PYZob{}}\PY{n}{coords}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{] = }\PY{l+s+si}{\PYZob{}}\PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{n}{coords}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Forward code:}
  \PY{c+c1}{\PYZsh{} xyzs\PYZus{}wrapped = numpy.concatenate([xyzs, xyzs[:, :1, :]], axis=1)}
  \PY{n}{s\PYZus{}xyzs} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{n}{s\PYZus{}xyzs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}xyzs\PYZus{}wrapped}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{c+c1}{\PYZsh{} Forward code:}
  \PY{c+c1}{\PYZsh{} xyzs = (  \PYZsh{} shape [num\PYZus{}rings, num\PYZus{}vertices\PYZus{}per\PYZus{}ring, 3]}
  \PY{c+c1}{\PYZsh{}     numpy.stack([xys\PYZus{}scaled\PYZus{}complex.real,}
  \PY{c+c1}{\PYZsh{}                  xys\PYZus{}scaled\PYZus{}complex.imag,}
  \PY{c+c1}{\PYZsh{}                  zs],}
  \PY{c+c1}{\PYZsh{}                 axis=\PYZhy{}1))}
  \PY{c+c1}{\PYZsh{} Backpropagating gets interesting here, since we get to see \PYZsq{}sensitivity of}
  \PY{c+c1}{\PYZsh{} the end result on the real/imaginary part of a complex quantity\PYZsq{}.}
  \PY{c+c1}{\PYZsh{} It so turns out that we have to take the complex conjugate here(!).}
  \PY{c+c1}{\PYZsh{} This is discussed at the end of this notebook.}
  \PY{n}{s\PYZus{}xys\PYZus{}scaled\PYZus{}complex} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}xyzs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{n}{j} \PY{o}{*} \PY{n}{s\PYZus{}xyzs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
  \PY{n}{s\PYZus{}zs} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}xyzs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}
  \PY{c+c1}{\PYZsh{} Forward code:}
  \PY{c+c1}{\PYZsh{} xys\PYZus{}scaled\PYZus{}complex = xys\PYZus{}unscaled\PYZus{}complex[numpy.newaxis, :] * (}
  \PY{c+c1}{\PYZsh{}     rs[:, numpy.newaxis] + vertex\PYZus{}radial\PYZus{}deviations)}
  \PY{c+c1}{\PYZsh{} The `xys\PYZus{}unscaled\PYZus{}complex` are just \PYZdq{}constants\PYZdq{} we do not need to backprop}
  \PY{c+c1}{\PYZsh{} into.}
  \PY{c+c1}{\PYZsh{} We perhaps should have introduced the 2nd factor as another intermediate}
  \PY{c+c1}{\PYZsh{} quantity above, with its own sensitivity\PYZhy{}accumulator, but since this}
  \PY{c+c1}{\PYZsh{} subexpression is a simple sum that only shows up once, its}
  \PY{c+c1}{\PYZsh{} sensitivity\PYZhy{}accumulator only receives a single contribution, so we might}
  \PY{c+c1}{\PYZsh{} just as well define it down here.}
  \PY{n}{s\PYZus{}rs\PYZus{}plus\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{s\PYZus{}xys\PYZus{}scaled\PYZus{}complex} \PY{o}{*} \PY{p}{(}
      \PY{n}{xys\PYZus{}unscaled\PYZus{}complex}\PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} This is interesting: We just switched over to taking a product of}
  \PY{c+c1}{\PYZsh{} complex sensitivities with a complex number \PYZhy{} so we are now in}
  \PY{c+c1}{\PYZsh{} complex\PYZhy{}backprop land. Hence, the sensitivity introduced above is}
  \PY{c+c1}{\PYZsh{} complex\PYZhy{}valued, but we only want/need the real part here.}
  \PY{n}{s\PYZus{}rs} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}rs\PYZus{}plus\PYZus{}radial\PYZus{}deviations}\PY{o}{.}\PY{n}{real}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{s\PYZus{}vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}rs\PYZus{}plus\PYZus{}radial\PYZus{}deviations}\PY{o}{.}\PY{n}{real}
  \PY{c+c1}{\PYZsh{} This is the most important part of the \PYZdq{}full gradient\PYZdq{}, which we are}
  \PY{c+c1}{\PYZsh{} actually using in our computation.}
  \PY{c+c1}{\PYZsh{} We could proceed to compute the r1/r2/z1/z2\PYZhy{}sensitivities for the sake}
  \PY{c+c1}{\PYZsh{} of completeness here. Physically, these would tell us about the force with}
  \PY{c+c1}{\PYZsh{} which a soap film of constant surface tension would try to bring the}
  \PY{c+c1}{\PYZsh{} rings together, respectively squeeze them.}
  \PY{c+c1}{\PYZsh{} (This might be a Participant Exercise for very motivated participants,}
  \PY{c+c1}{\PYZsh{} since backprop through numpy.linspace() will require a bit of thinking here,}
  \PY{c+c1}{\PYZsh{} and potentially also debugging \PYZhy{} so this could get somewhat frustrating}
  \PY{c+c1}{\PYZsh{} when tried as an early exercise. If, however, we do not do this, then}
  \PY{c+c1}{\PYZsh{} there also is no need to compute s\PYZus{}rs above.)}
  \PY{n}{s\PYZus{}inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations} \PY{o}{=} \PY{n}{s\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
  \PY{k}{return} \PY{n}{s\PYZus{}inner\PYZus{}vertex\PYZus{}radial\PYZus{}deviations}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{hints-for-implementing-grad_heron_area}{%
\subsubsection{Hints for implementing
grad\_heron\_area}\label{hints-for-implementing-grad_heron_area}}

(Not shown in the transcript, only in the accompanying notebooks.)



    \hypertarget{testing-it-out}{%
\subsubsection{Testing it out}\label{testing-it-out}}

Let us actually try to gain some confidence in our gradient before we
use it.

Our strategy: Define a random-but-reproducibly-so point and direction,
define a 1-parameter function that walks along that direction, compare
backprop-gradient with Taylor expansion.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{verify\PYZus{}grad\PYZus{}mesh\PYZus{}area}\PY{p}{(}\PY{n}{rng\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} Let us get a seeded guaranteed\PYZhy{}reproducible\PYZhy{}even\PYZhy{}across\PYZhy{}numpy\PYZhy{}versions}
  \PY{c+c1}{\PYZsh{} random number generator.}
  \PY{n}{rng} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{n}{rng\PYZus{}seed}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} We are taking somewhat generic values for the problem.}
  \PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{20.0}\PY{p}{,} \PY{l+m+mf}{21.0}
  \PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}
  \PY{n}{x0} \PY{o}{=} \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
  \PY{n}{x\PYZus{}dir\PYZus{}unnormalized} \PY{o}{=} \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{num\PYZus{}inner\PYZus{}rings}\PY{p}{,} \PY{n}{num\PYZus{}vertices\PYZus{}per\PYZus{}ring}\PY{p}{)}\PY{p}{)}
  \PY{n}{x\PYZus{}dir} \PY{o}{=} \PY{n}{x\PYZus{}dir\PYZus{}unnormalized} \PY{o}{/} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{x\PYZus{}dir\PYZus{}unnormalized}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{f\PYZus{}area}\PY{p}{(}\PY{n}{s}\PY{p}{)}\PY{p}{:}
    \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{area} \PY{o}{=} \PY{n}{vertices\PYZus{}and\PYZus{}total\PYZus{}mesh\PYZus{}area}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,} \PY{n}{x0} \PY{o}{+} \PY{n}{s} \PY{o}{*} \PY{n}{x\PYZus{}dir}\PY{p}{)}
    \PY{k}{return} \PY{n}{area}
  \PY{n}{fprime\PYZus{}area\PYZus{}numerical} \PY{o}{=} \PY{n}{numerical\PYZus{}derivative}\PY{p}{(}\PY{n}{f\PYZus{}area}\PY{p}{)}
  \PY{n}{grad\PYZus{}at\PYZus{}x0} \PY{o}{=} \PY{n}{grad\PYZus{}total\PYZus{}mesh\PYZus{}area}\PY{p}{(}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{,} \PY{n}{r1}\PY{p}{,} \PY{n}{r2}\PY{p}{,} \PY{n}{x0}\PY{p}{)}
  \PY{k}{return} \PY{n}{fprime\PYZus{}area\PYZus{}numerical}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{grad\PYZus{}at\PYZus{}x0}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x\PYZus{}dir}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} We compare 10 randomly picked positions and directions.}
\PY{c+c1}{\PYZsh{} If these gradients match, this provides rather strong validation.}
\PY{k}{for} \PY{n}{seed} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seed:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{verify\PYZus{}grad\PYZus{}mesh\PYZus{}area}\PY{p}{(}\PY{n}{rng\PYZus{}seed}\PY{o}{=}\PY{n}{seed}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Let us also quickly ad\PYZhy{}hoc check our \PYZdq{}Heron\PYZsq{}s area formula\PYZdq{} gradient}
\PY{c+c1}{\PYZsh{} (To better illustrate this).}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Heron\PYZhy{}Check: area}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{l+m+mf}{3.}\PY{p}{,}\PY{l+m+mf}{4.}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{)}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{numerical\PYZus{}grad[2]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{(}\PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{l+m+mf}{3.}\PY{p}{,}\PY{l+m+mf}{4.}\PY{p}{,}\PY{l+m+mf}{4.5} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)} \PY{o}{\PYZhy{}}
                            \PY{n}{heron\PYZus{}area}\PY{p}{(}\PY{l+m+mf}{3.}\PY{p}{,}\PY{l+m+mf}{4.}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{1e\PYZhy{}7} \PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{backprop\PYZus{}grad\PYZus{}full}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{grad\PYZus{}heron\PYZus{}area}\PY{p}{(}\PY{l+m+mf}{3.}\PY{p}{,}\PY{l+m+mf}{4.}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} The same demo as before \PYZhy{} but this time, we report every call,}
\PY{c+c1}{\PYZsh{} rather than every 1000th.}

\PY{n}{demo2\PYZus{}opt\PYZus{}val}\PY{p}{,} \PY{n}{demo2\PYZus{}vertices} \PY{o}{=} \PY{n}{get\PYZus{}minimal\PYZus{}area}\PY{p}{(}
    \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mf}{14.0}\PY{p}{,} \PY{l+m+mf}{14.0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,}
    \PY{n}{report\PYZus{}every\PYZus{}n\PYZus{}calls}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{grad\PYZus{}area}\PY{o}{=}\PY{n}{grad\PYZus{}total\PYZus{}mesh\PYZus{}area}\PY{p}{)}


\PY{n}{pyplot2\PYZus{}axes} \PY{o}{=} \PY{n}{pyplot}\PY{o}{.}\PY{n}{axes}\PY{p}{(}\PY{n}{projection}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot2\PYZus{}axes}\PY{o}{.}\PY{n}{plot\PYZus{}wireframe}\PY{p}{(}\PY{n}{demo2\PYZus{}vertices}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                            \PY{n}{demo2\PYZus{}vertices}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                            \PY{n}{demo2\PYZus{}vertices}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
                            \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Participant Exercise:}
\PY{c+c1}{\PYZsh{} Compare total optimization run time for the previous and this exercise.}

\PY{c+c1}{\PYZsh{} Participant Exercise:}
\PY{c+c1}{\PYZsh{} How much finer can we make the discretization?}
\PY{c+c1}{\PYZsh{} At how many degrees of freedom does this get \PYZdq{}high effort\PYZdq{}?}

\PY{c+c1}{\PYZsh{} Participant Exercise:}
\PY{c+c1}{\PYZsh{} Put a numerical bug(!) into the gradient\PYZhy{}of\PYZhy{}total\PYZhy{}area function in some}
\PY{c+c1}{\PYZsh{} random place (such as: change a sign from + to \PYZhy{})}
\PY{c+c1}{\PYZsh{} and check how this then impacts optimization here.}

\PY{c+c1}{\PYZsh{} Participant Exercise:}
\PY{c+c1}{\PYZsh{} Adjust the code to allow other boundary shapes than the edges of a symmetric}
\PY{c+c1}{\PYZsh{} conical frustum. (Such as: using different semimajor axes,}
\PY{c+c1}{\PYZsh{} adding an in\PYZhy{}plane displacement, etc.)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{a-first-machine-learning-example}{%
\section{A first Machine Learning
Example}\label{a-first-machine-learning-example}}

Let us actually use what we learned to build and train an extremely
simple machine learning model without using any ML or backpropagation
framework library (apart from using TensorFlow to load the example
data).

For quite a few years, the ``fruit fly'' / ``hydrogen problem'' / ``N=4
SYM'' type system used for ML was handwritten digit classification using
the ``MNIST'' dataset of digitized handwriting samples - tens of
thousands of digits written by real people and digitized to 28x28 pixel
grayscale resolution, with center-of-gravity for the ink
shifted-to-center and some other such normalizations.

One of the big problems with MNIST is that it is ``too easy to learn''.
Nowadays, it is only used rarely, and mostly in the context of unusual
architectures or applications. Nevertheless, knowing about this dataset
is important background for reading some of the older foundational
papers.

Let us build a very primitive classifier that tells us ``Yes'' or
``No'', depending on whether or not it thinks the digit is an 8. We will
do this not on 28x28 images, but on images downsampled to 14x14. Also,
we will only use 1000 example images for training - which everybody
would recognize in these days as ``being way too few examples to
meaningfully do ML''.

But first, let us load and inspect this popular dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{tensorflow\PYZus{}datasets} \PY{k}{as} \PY{n+nn}{tfds}

\PY{p}{(}\PY{n}{ds\PYZus{}train}\PY{p}{,} \PY{n}{ds\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{ds\PYZus{}info} \PY{o}{=} \PY{n}{tfds}\PY{o}{.}\PY{n}{load}\PY{p}{(}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{split}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{shuffle\PYZus{}files}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
    \PY{n}{as\PYZus{}supervised}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{with\PYZus{}info}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{ds\PYZus{}info}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tfds.core.DatasetInfo(
    name='mnist',
    full\_name='mnist/3.0.1',
    description="""
    The MNIST database of handwritten digits.
    """,
    homepage='http://yann.lecun.com/exdb/mnist/',
    data\_path='/root/tensorflow\_datasets/mnist/3.0.1',
    file\_format=tfrecord,
    download\_size=11.06 MiB,
    dataset\_size=21.00 MiB,
    features=FeaturesDict(\{
        'image': Image(shape=(28, 28, 1), dtype=uint8),
        'label': ClassLabel(shape=(), dtype=int64, num\_classes=10),
    \}),
    supervised\_keys=('image', 'label'),
    disable\_shuffling=False,
    splits=\{
        'test': <SplitInfo num\_examples=10000, num\_shards=1>,
        'train': <SplitInfo num\_examples=60000, num\_shards=1>,
    \},
    citation="""@article\{lecun2010mnist,
      title=\{MNIST handwritten digit database\},
      author=\{LeCun, Yann and Cortes, Corinna and Burges, CJ\},
      journal=\{ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist\},
      volume=\{2\},
      year=\{2010\}
    \}""",
)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We actually take 4000 image samples from the \PYZdq{}MNIST training set\PYZdq{}}
\PY{c+c1}{\PYZsh{} and split them into two sets, the \PYZdq{}even indices\PYZdq{} and \PYZdq{}odd indices\PYZdq{},}
\PY{c+c1}{\PYZsh{} where we use one of these as our reduced\PYZhy{}size training set,}
\PY{c+c1}{\PYZsh{} and the other one to assess how well our model manages to handle}
\PY{c+c1}{\PYZsh{} input it has never seen before.}
\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{ds\PYZus{}train}\PY{o}{.}\PY{n}{take}\PY{p}{(}\PY{l+m+mi}{4000}\PY{p}{)}\PY{o}{.}\PY{n}{as\PYZus{}numpy\PYZus{}iterator}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}training} \PY{o}{=} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}test} \PY{o}{=} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Maps a [..., 28, 28]\PYZhy{}array to a [..., 14, 14]\PYZhy{}array by 2x2\PYZhy{}averaging.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{data} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{data}\PY{p}{)}
  \PY{n}{shape} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{shape}
  \PY{k}{return} \PY{n}{data}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{shape}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Raw data use unsigned 8\PYZhy{}bit grayscale values (integers in [0..255]).}
\PY{c+c1}{\PYZsh{} Rescale to floats in 0..1 and stack them up into a [num\PYZus{}examples, 28, 28]}
\PY{c+c1}{\PYZsh{} array.}
\PY{n}{images\PYZus{}training} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}
    \PY{p}{[}\PY{n}{image} \PY{o}{/} \PY{l+m+mf}{255.0} \PY{k}{for} \PY{n}{image}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}training}\PY{p}{]}\PY{p}{,}
    \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{labels\PYZus{}training} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}
    \PY{p}{[}\PY{n}{label} \PY{k}{for} \PY{n}{image}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}training}\PY{p}{]}\PY{p}{)}
\PY{n}{images\PYZus{}test} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}
    \PY{p}{[}\PY{n}{image} \PY{o}{/} \PY{l+m+mf}{255.0} \PY{k}{for} \PY{n}{image}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}test}\PY{p}{]}\PY{p}{,}
    \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{labels\PYZus{}test} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}
    \PY{p}{[}\PY{n}{label} \PY{k}{for} \PY{n}{image}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}test}\PY{p}{]}\PY{p}{)}

\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{)} \PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{labels\PYZus{}training}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Let us see how many 8\PYZhy{}digits there are in this sample:}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of 8\PYZhy{}digits:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{x}\PY{o}{==}\PY{l+m+mi}{8} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{labels\PYZus{}training}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_02_Derivatives_files/ML_02_Derivatives_51_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
[4 0 8 2 1]
Number of 8-digits: 209
    \end{Verbatim}

    Overall, we have 1000 training images which we will downscale to
14x14=196 brightness float-values (one per pixel). So, our reduced-size
dataset consists of 39200 floating point numbers in total.

There are many different approaches one could try to come up with a
digit-8 classifier, such as for example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write tailored code that exploits special properties of the digit
  ``8'', such as:

  ``Classify this digit as 8 if and only if there are three separate
  non-inked regions''.
\item
  Consider each example as a point in high-dimensional (D=\(14^2=196\))
  space and, for a given input, look at the classifications of the 10
  nearest-neighbor examples.
\end{enumerate}

We will look into these later. Approach (1.) was used somewhat widely in
the 90s for problems ``where the best we can do is ad hoc heuristics''.
ML follows the ideas presented in Turing's 1948 paper
``\href{https://hashingit.com/elements/research-resources/1948-intelligent-machinery.pdf}{Intelligent
Machinery} \cite{turing2004intelligent}'': ``can we use examples to fine-tune parameters of a general
architecture via optimization, in order to maximize classifier
performance (w.r.t. the capabilities of the given architecture)?''

\begin{verbatim}
  A great positive reason for believing in the possibility of making thinking
  machinery is the fact that it is possible to make machinery to imitate any
  small part of a man. That the microphone does this for the ear, and the
  television camera for the eye are commonplaces. (...) Here we are chiefly
  interested in the nervous system. We could produce fairly accurate electrical
  models to copy the behaviour of nerves, but there seems very little point in
  doing so. It would be rather like (...) cars which walked on legs (...).

  (...)

  If we are trying to produce an intelligent machine, and are following the
  human model as closely as we can, we should begin with a machine with very
  little capacity to carry out elaborate operations or to react in a disciplined
  manner to orders (taking the form of inference). Then by applying appropriate
  inference, mimicking education, we should hope to modify the machine until it
  could be relied on to produce definite reactions to certain commands.
\end{verbatim}

What is the ``dumbest'' possible idea we can come up with here that
still looks somewhat reasonable? We want a function that maps input
data, as a \({\mathbb R}^{14\times 14}\) vector, to some sort of
``accumulated evidence that we are looking at a number 8 digit''.

The structurally simplest functions we can think of doing something like
this are linear (rather, affine, i.e.~``with offset'' functions. So, we
are looking for a linear function that we want to output some large
positive number if we are looking at a digit 8 and some large negative
number if not. But what would our objective function for numerical
minimization then look like?

Right now, our focus is on ``numerical optimization'' and ``doing
everything from scratch, no ML frameworks allowed'', so we will have to
come back to this question later in the course. Here, we will only use
two ideas - to be justified later.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For every given input image, we want to map the accumulated evidence
  to a ``this is how I, the classifier, would bet'' (Bayesian)
  probability for the example to have a positive (binary classification)
  label.

  It so turns out that the appropriate function to do this is the
  ``sigmoid function'' \(f(x)=1/(1+\exp(k-x))\), which we of course also
  are familiar with in the context of the thermodynamics of a 2-state
  system. Here, the offset is a tunable/``learnable'' parameter.
\item
  Given probabilities and ground truth labels, we need to produce a
  ``how far are we off on these examples'' score. Here, we go with a
  smooth variant of the basic idea that ``for misclassifications, we
  want to count the number of interval bisections that tell us how far
  off our probability estimate was''. So, if we have a label=True case
  where we say ``p=6.25\% this is True'', this needs ``3 bisecting
  binary questions'' (50\% -\textgreater{} 25\% -\textgreater{} 12.5\%
  -\textgreater{} 6.25\%), while if we say ``p=25\%'', this needs ``1
  bisecting binary question'' so is only 1/3 as bad overall in terms of
  information content.
\end{enumerate}

Let's build this.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}digit8\PYZus{}classifier}\PY{p}{(}\PY{n}{training\PYZus{}images}\PY{p}{,}
                          \PY{n}{training\PYZus{}labels}\PY{p}{,}
                          \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                          \PY{n}{maxiter}\PY{o}{=}\PY{l+m+mi}{10\PYZus{}000}\PY{p}{,}
                          \PY{n}{debug\PYZus{}print\PYZus{}every\PYZus{}n\PYZus{}calls}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a pair (classifier\PYZus{}parameters, classifier) for detecting 8\PYZhy{}s.}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    training\PYZus{}images:}
\PY{l+s+sd}{      [0..1]\PYZhy{}float [num\PYZus{}images, num\PYZus{}rows, num\PYZus{}columns]\PYZhy{}array\PYZhy{}like,}
\PY{l+s+sd}{      the training dataset.}
\PY{l+s+sd}{    training\PYZus{}labels: [num\PYZus{}images]\PYZhy{}array\PYZhy{}like, array of ground truth labels}
\PY{l+s+sd}{      (0\PYZhy{}9) for the images in `training\PYZus{}images`.}
\PY{l+s+sd}{    seed: Random number generator seed.}
\PY{l+s+sd}{    maxiter: Maximal number of iterations for the BFGS\PYZhy{}minimizer.}
\PY{l+s+sd}{    debug\PYZus{}print\PYZus{}every\PYZus{}n\PYZus{}calls: Controls how often we print progress information.}

\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    A pair of `(classifier\PYZus{}parameters\PYZus{}vector, classifier\PYZus{}callable)`,}
\PY{l+s+sd}{    where calling `classifier\PYZus{}callable` on a}
\PY{l+s+sd}{    [num\PYZus{}images\PYZus{}to\PYZus{}classify, num\PYZus{}rows, num\PYZus{}columns]\PYZhy{}array\PYZhy{}like collection}
\PY{l+s+sd}{    of image data returns a corresponding vector of best\PYZhy{}estimate probabilities}
\PY{l+s+sd}{    for the image to show an 8\PYZhy{}digit.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{training\PYZus{}images} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{training\PYZus{}images}\PY{p}{)}
  \PY{n}{training\PYZus{}labels} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{training\PYZus{}labels}\PY{p}{)}
  \PY{n}{label\PYZus{}positive} \PY{o}{=} \PY{n}{training\PYZus{}labels} \PY{o}{==} \PY{l+m+mi}{8}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}prob}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{images}\PY{p}{)}\PY{p}{:}
    \PY{n}{stencil\PYZus{}params} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{offset} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}  \PY{c+c1}{\PYZsh{} A [1]\PYZhy{}vector, for broadcasting.}
    \PY{n}{evidence} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ix,x\PYZhy{}\PYZgt{}i}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                            \PY{n}{images}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                            \PY{n}{stencil\PYZus{}params}\PY{p}{)}
    \PY{k}{return} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{numpy}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{offset}\PY{o}{\PYZhy{}}\PY{n}{evidence}\PY{p}{)}\PY{p}{)}
  \PY{n}{num\PYZus{}calls} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}quality\PYZus{}score}\PY{p}{(}\PY{n}{params}\PY{p}{)}\PY{p}{:}
    \PY{k}{nonlocal} \PY{n}{num\PYZus{}calls}
    \PY{n}{prob} \PY{o}{=} \PY{n}{get\PYZus{}prob}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{training\PYZus{}images}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} The 1e\PYZhy{}12 offsets help handling cases where exp(\PYZhy{}evidence) is numerically}
    \PY{c+c1}{\PYZsh{} zero due to underflow. This avoids running into log(0), which produces}
    \PY{c+c1}{\PYZsh{} NaN intermediate results.}
    \PY{n}{result} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{label\PYZus{}positive} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}12} \PY{o}{+} \PY{n}{prob}\PY{p}{)} \PY{o}{+}
               \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{label\PYZus{}positive}\PY{p}{)} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{l+m+mf}{1e\PYZhy{}12}\PY{o}{\PYZhy{}}\PY{n}{prob}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
    \PY{n}{num\PYZus{}calls} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{if} \PY{p}{(}\PY{n}{debug\PYZus{}print\PYZus{}every\PYZus{}n\PYZus{}calls} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{o+ow}{and}
        \PY{n}{num\PYZus{}calls} \PY{o}{\PYZpc{}} \PY{n}{debug\PYZus{}print\PYZus{}every\PYZus{}n\PYZus{}calls} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{N=}\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}calls}\PY{l+s+si}{:}\PY{l+s+s1}{6d}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{: score=}\PY{l+s+si}{\PYZob{}}\PY{n}{result}\PY{l+s+si}{:}\PY{l+s+s1}{16.10f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{result}
  \PY{c+c1}{\PYZsh{} BEGIN Gradient code}
  \PY{c+c1}{\PYZsh{} We want the sensitivity of `prob` on parameters.}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}s\PYZus{}prob}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{images}\PY{p}{,} \PY{n}{s\PYZus{}result}\PY{p}{)}\PY{p}{:}
    \PY{n}{stencil\PYZus{}params} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{offset} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}  \PY{c+c1}{\PYZsh{} A [1]\PYZhy{}vector, for broadcasting.}
    \PY{n}{s\PYZus{}params} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{params}\PY{p}{)}
    \PY{n}{evidence} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ix,x\PYZhy{}\PYZgt{}i}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                            \PY{n}{images}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                            \PY{n}{stencil\PYZus{}params}\PY{p}{)}
    \PY{n}{exp\PYZus{}neg\PYZus{}evidence} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{offset}\PY{o}{\PYZhy{}}\PY{n}{evidence}\PY{p}{)}
    \PY{n}{denom} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{+} \PY{n}{exp\PYZus{}neg\PYZus{}evidence}
    \PY{n}{result} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{denom}
    \PY{c+c1}{\PYZsh{}}
    \PY{c+c1}{\PYZsh{} s\PYZus{}denom = \PYZhy{}s\PYZus{}result / (denom * denom)}
    \PY{n}{s\PYZus{}exp\PYZus{}neg\PYZus{}evidence} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{s\PYZus{}result} \PY{o}{/} \PY{p}{(}\PY{n}{denom} \PY{o}{*} \PY{n}{denom}\PY{p}{)}
    \PY{n}{s\PYZus{}evidence} \PY{o}{=} \PY{n}{s\PYZus{}exp\PYZus{}neg\PYZus{}evidence} \PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{exp\PYZus{}neg\PYZus{}evidence}\PY{p}{)}
    \PY{n}{s\PYZus{}offset} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{s\PYZus{}evidence}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{s\PYZus{}stencil\PYZus{}params} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ix,i\PYZhy{}\PYZgt{}x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                   \PY{n}{images}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                                   \PY{n}{s\PYZus{}evidence}\PY{p}{)}
    \PY{k}{return} \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{s\PYZus{}stencil\PYZus{}params}\PY{p}{,} \PY{n}{s\PYZus{}offset}\PY{p}{]}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}s\PYZus{}quality\PYZus{}score}\PY{p}{(}\PY{n}{params}\PY{p}{)}\PY{p}{:}
    \PY{n}{prob} \PY{o}{=} \PY{n}{get\PYZus{}prob}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{training\PYZus{}images}\PY{p}{)}
    \PY{n}{s\PYZus{}prob} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{prob}\PY{p}{)}
    \PY{n}{part1} \PY{o}{=} \PY{n}{label\PYZus{}positive} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}12} \PY{o}{+} \PY{n}{prob}\PY{p}{)}
    \PY{n}{part2} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{label\PYZus{}positive}\PY{p}{)} \PY{o}{*} \PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{l+m+mf}{1e\PYZhy{}12}\PY{o}{\PYZhy{}}\PY{n}{prob}\PY{p}{)}
    \PY{n}{result} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{part1} \PY{o}{+} \PY{n}{part2}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{s\PYZus{}part1} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{part1}\PY{p}{)}
    \PY{n}{s\PYZus{}part2} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{part2}\PY{p}{)}
    \PY{n}{s\PYZus{}prob} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}part1} \PY{o}{*} \PY{n}{label\PYZus{}positive} \PY{o}{/} \PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}12} \PY{o}{+} \PY{n}{prob}\PY{p}{)}
    \PY{n}{s\PYZus{}prob} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}part2} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{label\PYZus{}positive}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{l+m+mf}{1e\PYZhy{}12}\PY{o}{\PYZhy{}}\PY{n}{prob}\PY{p}{)}
    \PY{k}{return} \PY{n}{get\PYZus{}s\PYZus{}prob}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{training\PYZus{}images}\PY{p}{,} \PY{n}{s\PYZus{}prob}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} END Gradient code}
  \PY{n}{rng} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{n}{seed}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Model parameters are: 14x14 per\PYZhy{}pixel\PYZhy{}evidence weights, plus one overall}
  \PY{c+c1}{\PYZsh{} evidence\PYZhy{}offset.}
  \PY{n}{v\PYZus{}start} \PY{o}{=} \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{14}\PY{o}{*}\PY{l+m+mi}{14} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}2}\PY{p}{)}
  \PY{n}{opt} \PY{o}{=} \PY{n}{scipy}\PY{o}{.}\PY{n}{optimize}\PY{o}{.}\PY{n}{fmin\PYZus{}bfgs}\PY{p}{(}\PY{n}{get\PYZus{}quality\PYZus{}score}\PY{p}{,}
                                 \PY{n}{v\PYZus{}start}\PY{p}{,}
                                 \PY{n}{fprime}\PY{o}{=}\PY{n}{get\PYZus{}s\PYZus{}quality\PYZus{}score}\PY{p}{,}
                                 \PY{n}{gtol}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}10}\PY{p}{,}
                                 \PY{n}{maxiter}\PY{o}{=}\PY{n}{maxiter}\PY{p}{)}
  \PY{k}{return} \PY{n}{opt}\PY{p}{,} \PY{k}{lambda} \PY{n}{images}\PY{p}{:} \PY{n}{get\PYZus{}prob}\PY{p}{(}\PY{n}{opt}\PY{p}{,} \PY{n}{images}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Quite likely, training/optimization will stop here once we run into}
\PY{c+c1}{\PYZsh{} exp() overflow \PYZhy{} but even with optimization not proceeding to the minimum,}
\PY{c+c1}{\PYZsh{} we do get a somewhat useful classifier.}
\PY{n}{opt\PYZus{}params}\PY{p}{,} \PY{n}{classifier} \PY{o}{=} \PY{n}{get\PYZus{}digit8\PYZus{}classifier}\PY{p}{(}\PY{n}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{p}{)}\PY{p}{,}
                                               \PY{n}{labels\PYZus{}training}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
N=   100: score=    0.0747413554
N=   200: score=    0.0651305360
N=   300: score=    0.0620802156
N=   400: score=    0.0607213317
N=   500: score=    0.0600792142
N=   600: score=    0.0596360551
N=   700: score=    0.0591480792
N=   800: score=    0.0588810554
N=   900: score=    0.0585761021
N=  1000: score=    0.0580431305
N=  1100: score=    0.0578176706
N=  1200: score=    0.0576444457
N=  1300: score=    0.0574787213
N=  1400: score=    0.0573991570
N=  1500: score=    0.0573255619
N=  1600: score=    0.0572122781
N=  1700: score=    0.0569957265
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<ipython-input-68-cbe0ab0e1b3e>:63: RuntimeWarning: overflow encountered in
multiply
  s\_exp\_neg\_evidence = -s\_result / (denom * denom)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Warning: Desired error not necessarily achieved due to precision loss.
         Current function value: 0.056684
         Iterations: 1751
         Function evaluations: 1768
         Gradient evaluations: 1756
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<ipython-input-68-cbe0ab0e1b3e>:34: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + numpy.exp(offset-evidence))
<ipython-input-68-cbe0ab0e1b3e>:58: RuntimeWarning: overflow encountered in exp
  exp\_neg\_evidence = numpy.exp(offset-evidence)
<ipython-input-68-cbe0ab0e1b3e>:64: RuntimeWarning: invalid value encountered in
multiply
  s\_evidence = s\_exp\_neg\_evidence * (-exp\_neg\_evidence)
<ipython-input-68-cbe0ab0e1b3e>:34: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + numpy.exp(offset-evidence))
<ipython-input-68-cbe0ab0e1b3e>:58: RuntimeWarning: overflow encountered in exp
  exp\_neg\_evidence = numpy.exp(offset-evidence)
<ipython-input-68-cbe0ab0e1b3e>:63: RuntimeWarning: overflow encountered in
multiply
  s\_exp\_neg\_evidence = -s\_result / (denom * denom)
<ipython-input-68-cbe0ab0e1b3e>:64: RuntimeWarning: invalid value encountered in
multiply
  s\_evidence = s\_exp\_neg\_evidence * (-exp\_neg\_evidence)
    \end{Verbatim}

    We do observe that optimization failed here since we were not
appropriately careful implementing our objective function in a way that
does not readily run into numerical pathologies. It is perhaps useful to
show that such things can and do happen, and thinking about how to avoid
numerical cancellation, blow-ups etc. is generally important when
implementing machine learning components.

We could fix this here, but do not really have to - optimization, even
as it technically failed to complete, has already produced a somewhat
useful performance-enhancement. Had we used a loss function from a ML
library, this point would have been addressed in its implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us actually try out our classifier on some of our test images:}
\PY{n}{num\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{2000}
\PY{n}{max\PYZus{}num\PYZus{}bad\PYZus{}to\PYZus{}show} \PY{o}{=} \PY{l+m+mi}{20}
\PY{n}{num\PYZus{}bad\PYZus{}shown} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{images\PYZus{}test\PYZus{}14x14} \PY{o}{=} \PY{n}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{images\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}test}\PY{p}{]}\PY{p}{)}
\PY{n}{classified\PYZus{}test\PYZus{}images} \PY{o}{=} \PY{n}{classifier}\PY{p}{(}\PY{n}{images\PYZus{}test\PYZus{}14x14}\PY{p}{)}
\PY{n}{bad\PYZus{}labels\PYZus{}images} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{n}\PY{p}{,} \PY{n}{prob}\PY{p}{,} \PY{n}{label\PYZus{}test} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}test}\PY{p}{)}\PY{p}{,}
                               \PY{n}{classified\PYZus{}test\PYZus{}images}\PY{p}{,}
                               \PY{n}{labels\PYZus{}test}\PY{p}{)}\PY{p}{:}
  \PY{n}{is\PYZus{}bad} \PY{o}{=} \PY{p}{(}\PY{n}{prob} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)} \PY{o}{!=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{label\PYZus{}test} \PY{o}{==} \PY{l+m+mi}{8}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Disable this check to print all classifications.}
  \PY{k}{if} \PY{n}{is\PYZus{}bad} \PY{o+ow}{and} \PY{n}{num\PYZus{}bad\PYZus{}shown} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}num\PYZus{}bad\PYZus{}to\PYZus{}show}\PY{p}{:}
    \PY{n}{num\PYZus{}bad\PYZus{}shown} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{N=}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{:}\PY{l+s+s1}{3d}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{: label=}\PY{l+s+si}{\PYZob{}}\PY{n}{label\PYZus{}test}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, }\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{prob=}\PY{l+s+si}{\PYZob{}}\PY{n}{prob}\PY{l+s+si}{:}\PY{l+s+s1}{.10f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{+w}{ }\PY{k}{if}\PY{+w}{ }\PY{n}{is\PYZus{}bad}\PY{+w}{ }\PY{k}{else}\PY{+w}{ }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{k}{if} \PY{n}{is\PYZus{}bad}\PY{p}{:}
    \PY{n}{bad\PYZus{}labels\PYZus{}images}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{label\PYZus{}test}\PY{p}{,} \PY{n}{images\PYZus{}test\PYZus{}14x14}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}
    \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}
        \PY{p}{[}\PY{n}{image} \PY{k}{for} \PY{n}{label}\PY{p}{,} \PY{n}{image} \PY{o+ow}{in} \PY{n}{bad\PYZus{}labels\PYZus{}images}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{]}\PY{p}{,}
        \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\PY{n}{bad\PYZus{}labels} \PY{o}{=} \PY{p}{[}\PY{n}{label} \PY{k}{for} \PY{n}{label}\PY{p}{,} \PY{n}{image} \PY{o+ow}{in} \PY{n}{bad\PYZus{}labels\PYZus{}images}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{bad\PYZus{}labels}\PY{p}{)}\PY{p}{,} \PY{n}{bad\PYZus{}labels}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Let us also check if we are actually better than always\PYZhy{}guessing\PYZhy{}\PYZdq{}not an 8\PYZdq{}.}
\PY{n}{num\PYZus{}test\PYZus{}8s} \PY{o}{=} \PY{p}{(}\PY{n}{labels\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}test}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{8}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\PY{n}{sigma} \PY{o}{=} \PY{p}{(}\PY{n}{num\PYZus{}test} \PY{o}{*} \PY{l+m+mf}{0.1} \PY{o}{*} \PY{l+m+mf}{0.9}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Always\PYZhy{}guessing\PYZhy{}not\PYZhy{}8 would make }\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}test\PYZus{}8s}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ misclassifications }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{100}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{n}{num\PYZus{}test\PYZus{}8s}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n}{num\PYZus{}test}\PY{l+s+si}{:}\PY{l+s+s1}{.1f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZpc{}).}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{We made }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{bad\PYZus{}labels}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ misclassifications }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{100}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{bad\PYZus{}labels}\PY{p}{)}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n}{num\PYZus{}test}\PY{l+s+si}{:}\PY{l+s+s1}{.1f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZpc{}).}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{With sigma = sqrt(}\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}test}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ * 0.1 * 0.9) = }\PY{l+s+si}{\PYZob{}}\PY{n}{sigma}\PY{l+s+si}{:}\PY{l+s+s1}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, this is at}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{abs}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{bad\PYZus{}labels}\PY{p}{)}\PY{+w}{ }\PY{o}{\PYZhy{}}\PY{+w}{ }\PY{n}{num\PYZus{}test\PYZus{}8s}\PY{p}{)}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n}{sigma}\PY{l+s+si}{:}\PY{l+s+s1}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ sigma for the }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{baseline classifier.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
N=  9: label=1, prob=0.9785949016 X
N= 31: label=8, prob=0.0017661758 X
N= 97: label=8, prob=0.4151601095 X
N=129: label=6, prob=0.6621850765 X
N=163: label=2, prob=0.7174619928 X
N=169: label=6, prob=0.5418635124 X
N=170: label=8, prob=0.0022988911 X
N=184: label=8, prob=0.3697129140 X
N=187: label=8, prob=0.2133721492 X
N=212: label=8, prob=0.0934954688 X
N=227: label=8, prob=0.0000000000 X
N=228: label=8, prob=0.0000000000 X
N=262: label=8, prob=0.1821384720 X
N=282: label=3, prob=0.9977189467 X
N=289: label=5, prob=0.9264995652 X
N=290: label=8, prob=0.0000002562 X
N=291: label=8, prob=0.0000000032 X
N=293: label=8, prob=0.0000000000 X
N=307: label=8, prob=0.0574443067 X
N=318: label=9, prob=0.7543842702 X
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_02_Derivatives_files/ML_02_Derivatives_55_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
121 [1, 8, 8, 6, 2, 6, 8, 8, 8, 8, 8, 8, 8, 3, 5, 8, 8, 8, 8, 9, 0, 4, 8, 0, 8,
8, 0, 2, 9, 1, 0, 1, 2, 8, 8, 5, 8, 0, 8, 8, 8, 2, 5, 2, 8, 4, 2, 8, 4, 5, 8, 6,
7, 4, 4, 2, 9, 8, 8, 3, 8, 0, 8, 8, 8, 8, 7, 5, 5, 8, 9, 0, 1, 8, 5, 8, 8, 8, 2,
4, 8, 8, 8, 7, 2, 8, 4, 5, 5, 8, 1, 8, 8, 8, 8, 8, 5, 2, 8, 8, 8, 2, 8, 8, 8, 2,
8, 8, 8, 8, 5, 8, 1, 8, 8, 4, 5, 8, 5, 4, 2]

\#\#\#
Always-guessing-not-8 would make 203 misclassifications (10.2\%).
We made 121 misclassifications (6.0\%).
With sigma = sqrt(2000 * 0.1 * 0.9) = 13.42, this is at
6.11 sigma for the baseline classifier.
    \end{Verbatim}

    \hypertarget{participant-exercises}{%
\subsection{Participant Exercises}\label{participant-exercises}}

\begin{itemize}
\item
  On the MNIST dataset, humans are observed to have a ``which digit is
  it'' classification error rate in the ballpark of 2-3\%. Change the
  code to load 500 examples and visualize them in a 50x10 grid.
  Hand-classify them and compare your classifications with the ``ground
  truth'' labels. Is this claimed human error rate plausible?
\item
  Above, we introduced an `offset'. Does this parameter actually have an
  impact on performance? Replace \texttt{offset} with
  \texttt{0\ *\ offset} in the code above to check this.
\item
  Find out if the MNIST training dataset (50\_000 examples) contains any
  exact duplicate images. (This might need some creativity.)
\item
  Change the above code to only see examples where the ground truth
  label is ``1'' or ``2'', and build a binary ``is it a digit 1?''
  classifier. Is it much better than random guessing? {[}It should
  be!{]}
\item
  Can you fix the ``numerical overflow'' problem observed above?
\end{itemize}

    One way to think about such classification problems is as follows: We
decided to believe in the existence of two smooth
\({\mathbb R}^{14\times 14}\mapsto{\mathbb R}\) probability distribution
functions which describe the probability-density for ``a human asked to
write a digit 1 (respectively, 2) will produce something that looks like
this''. We do not know what they look like, but we can sample thousands
of the examples from these probability distributions - which we did.

When we classify a never-seen-before example, we try to use what we have
seen to estimate the likelihood ratio ``is this a 1'' vs.~``is this a
2'', and make a call based on this estimate.

Let us try a radically different approach that does not really use any
``learning'': For a given not-observed-before input, we determine the
euclidean distance to each training set example, and look at the N
closest neighbors and their classes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}neighborhood\PYZus{}analyzer}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{p}{,} \PY{n}{labels\PYZus{}training}\PY{p}{)}\PY{p}{:}
  \PY{n}{images\PYZus{}training} \PY{o}{=} \PY{p}{(}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
  \PY{n}{labels\PYZus{}training} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{labels\PYZus{}training}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{get\PYZus{}neighborhood}\PY{p}{(}\PY{n}{image\PYZus{}new}\PY{p}{,} \PY{n}{num\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{image\PYZus{}new} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{image\PYZus{}new}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{distances} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{images\PYZus{}training} \PY{o}{\PYZhy{}} \PY{n}{image\PYZus{}new}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{sorted\PYZus{}indexed\PYZus{}distances} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{distances}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{nd}\PY{p}{:} \PY{n}{nd}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{k}{return} \PY{p}{[}\PY{p}{(}\PY{n}{dist}\PY{p}{,} \PY{n}{labels\PYZus{}training}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{)}
            \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{dist} \PY{o+ow}{in} \PY{n}{sorted\PYZus{}indexed\PYZus{}distances}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}neighbors}\PY{p}{]}\PY{p}{]}
  \PY{k}{return} \PY{n}{get\PYZus{}neighborhood}

\PY{n}{analyzer} \PY{o}{=} \PY{n}{get\PYZus{}neighborhood\PYZus{}analyzer}\PY{p}{(}\PY{n}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{p}{)}\PY{p}{,} \PY{n}{labels\PYZus{}training}\PY{p}{)}
\PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{:}
  \PY{n}{analyzed} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{dist}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{p}{)}
              \PY{k}{for} \PY{n}{dist}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{analyzer}\PY{p}{(}\PY{n}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{images\PYZus{}test}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}
  \PY{n+nb}{print}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{labels\PYZus{}test}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{,} \PY{n}{analyzed}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Participant Exercise:}
\PY{c+c1}{\PYZsh{} Build a 1\PYZhy{}out\PYZhy{}of\PYZhy{}10 classifier out of such a neighborhood\PYZhy{}analyzer by}
\PY{c+c1}{\PYZsh{} doing some (distance\PYZhy{}weighted?) counting of the labels of the 10 (or 100)}
\PY{c+c1}{\PYZsh{} nearest neighbors in the training dataset.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0 1 [(0.6, 1), (0.94, 1), (0.96, 1), (1.0, 1), (1.15, 1), (1.17, 1), (1.21, 1),
(1.21, 1), (1.22, 1), (1.28, 1)]
1 7 [(2.24, 7), (2.25, 7), (2.35, 7), (2.37, 7), (2.39, 7), (2.39, 7), (2.44,
7), (2.47, 7), (2.47, 9), (2.49, 7)]
2 1 [(1.06, 1), (1.13, 1), (1.2, 1), (1.24, 1), (1.24, 1), (1.24, 1), (1.3, 1),
(1.32, 1), (1.34, 1), (1.36, 1)]
3 7 [(1.57, 7), (1.8, 7), (1.8, 7), (1.86, 7), (2.0, 7), (2.16, 7), (2.26, 7),
(2.36, 7), (2.36, 7), (2.38, 7)]
4 6 [(1.55, 6), (1.69, 6), (1.72, 6), (1.92, 6), (1.96, 6), (1.98, 6), (2.01,
6), (2.01, 6), (2.16, 6), (2.17, 6)]
5 4 [(2.19, 4), (2.28, 4), (2.67, 4), (2.84, 4), (2.89, 5), (2.95, 4), (2.99,
9), (3.05, 4), (3.07, 4), (3.08, 4)]
6 7 [(1.89, 9), (1.92, 7), (1.92, 7), (1.94, 7), (2.14, 1), (2.16, 7), (2.22,
9), (2.23, 7), (2.24, 7), (2.26, 9)]
7 3 [(1.86, 3), (2.27, 3), (2.35, 3), (2.45, 3), (2.5, 3), (2.56, 3), (2.58, 3),
(2.6, 3), (2.62, 3), (2.7, 3)]
8 9 [(2.28, 9), (2.4, 9), (2.45, 9), (2.56, 9), (2.56, 3), (2.6, 9), (2.63, 9),
(2.67, 9), (2.68, 9), (2.68, 9)]
9 1 [(1.23, 1), (1.4, 1), (1.4, 1), (1.49, 1), (1.62, 1), (1.72, 1), (1.73, 1),
(1.91, 1), (2.04, 1), (2.08, 1)]
10 6 [(1.6, 6), (1.64, 6), (1.88, 6), (1.91, 6), (1.94, 6), (1.97, 6), (1.97,
6), (1.99, 6), (2.0, 6), (2.04, 6)]
11 9 [(1.6, 9), (2.17, 9), (2.29, 9), (2.9, 4), (2.95, 4), (2.96, 9), (3.06, 9),
(3.11, 9), (3.12, 4), (3.14, 9)]
12 4 [(1.57, 4), (1.67, 4), (1.93, 4), (2.0, 9), (2.04, 4), (2.13, 4), (2.26,
9), (2.27, 9), (2.27, 9), (2.29, 9)]
13 9 [(1.56, 9), (1.63, 7), (1.68, 9), (1.7, 9), (1.73, 9), (1.73, 9), (1.87,
9), (1.87, 9), (1.91, 9), (1.96, 9)]
14 7 [(1.4, 7), (1.6, 7), (1.64, 7), (1.81, 7), (1.83, 7), (1.84, 7), (1.87, 7),
(1.91, 7), (1.91, 7), (1.97, 7)]
15 3 [(2.41, 8), (2.44, 3), (2.48, 3), (2.49, 3), (2.52, 3), (2.53, 3), (2.57,
3), (2.71, 3), (2.72, 3), (2.72, 8)]
16 9 [(2.76, 9), (2.8, 2), (2.85, 9), (2.88, 9), (2.97, 9), (3.06, 9), (3.1, 9),
(3.11, 7), (3.12, 9), (3.13, 7)]
17 9 [(2.52, 9), (2.59, 9), (2.7, 9), (2.7, 7), (2.71, 9), (2.73, 9), (2.73, 9),
(2.76, 9), (2.79, 4), (2.84, 9)]
18 6 [(2.29, 6), (2.45, 6), (2.67, 6), (2.72, 6), (2.78, 6), (2.79, 6), (2.82,
6), (2.82, 6), (2.82, 6), (2.83, 6)]
19 4 [(2.17, 4), (2.27, 4), (2.36, 4), (2.64, 4), (2.65, 4), (2.68, 4), (2.72,
4), (2.74, 9), (2.76, 4), (2.79, 4)]
    \end{Verbatim}

    Overall, what we have been doing here, especially with this last
nearest-neighbors approach, has some smell of ``having made many ad-hoc
and arbitrary choices''. For example, the distribution of
brightness-values observed on the training set for a near-center pixel
will in general be a rather different one than for a pixel that only
very rarely receives ink - and yet, ``we treat them all the same w.r.t.
measuring distance'' - is this actually justified?

Clearly, there are numerous ways to tune and tweak any such ML-ish
approach, and (unfortunately) a widely held view in the ML community is
that a criterion for publication-worthiness of research is whether it
meaningfully manages to improve performance numbers (such as: fraction
of correct classifications) on some of the (more or less) widely
discussed model problems/datasets. Due to this situation, it can be both
very easy to get published in ML but at the same time accept/reject
decisions on papers do strongly depend on chance (a large body of
articles in need of review naturally impacts the quality of reviews).

It has been said that ``ML exists because humans are not good at seeing
structure in very high dimensions'', and ``ML works'' because real-world
probability distributions may have some subtle structure but in general
are not as badly behaved as they could be in principle. In this sense,
we can see ML like a ``telescope like perception-enhancing tool, but for
seeing (coarse) structure in very high dimensions''.

So, much of (``supervised'', i.e.~``ground truth target labels based'')
Machine Learning is about finding ``tight'' approximations to high
dimensional probability distributions. Depending on how we build our
parametric model, we can express only some structural features but not
others. For our optimizer-based classifier above, the ``language'' of
likelihood-ratio functions expressible by our ansatz has a lot of extra
constraining structure - if we undo the final sigmoid that gives us a
probability, we find a function that only linearly depends on the
brightness (``ink value'') of each pixel. This means especially: If we
start from a blank image, then there will be a way to add ink that
increases the digit-8-evidence fastest, and along every direction that
has a positive scalar product with this one, we will add
figure-8-evidence. It might just be that we ``hit maximum brightness''
on some pixel before we build up any meaningful such evidence. In
particular, there is no way to represent any ``if these two pixels have
ink, that correlation provides us extra evidence'' type of information
in this overly simple model. We could of course amend our model,
introducing extra (tunable) parameters, that are sensitive to such
correlations.

Ultimately, this class of ML applications revolve around: ``If we get to
see examples drawn from a distribution that is not too different from
what we obtained the training set from, what can we achieve by trying to
maximize the odds of getting our predictions right?'' In a way, this
essentially statistics-based approach is very different from what one
would intuitively call ``learning'', and this is nicely illustrated by
training (i.e.~parameter-tuning) a ML model on the training set but then
feeding it trivially-transformed input, such as handwritten digits that
have been rotated by 90 degrees. Clearly, ``learning'' is about as much
a different notion in ML and human education context as ``force'' has
different meanings in physical and colloquial context.

This ``most likely the thing we want according to the background
distribution'' approach can have quite interesting - and sometimes
problematic - consequences, especially if examples codify some form of
``established bad habits'' which then get baked into models. On Google
Shopping, it is no problem to buy ``two inch nails'', but basically
impossible to buy ``nine inch nails'', since that also happens to be the
name of a popular band.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The classifier we built is simple enough to do one more thing with it:
Let us actually plot the per-pixel-digit-8-evidence-weights themselves,
reshaped to 14x14. This tells us ``the direction in
\({\mathbb R}^{14\times 14}\) along which evidence-for-digit-8 increases
fastest''.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Looking at numerical values:}
\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{opt\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
\PY{c+c1}{\PYZsh{} \PYZdq{}Clipping outliers\PYZdq{} which provide a lot of evidence.}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n}{opt\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cool}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} We observe that there are some pixels \PYZsq{}just outside where a figure\PYZhy{}8}
\PY{c+c1}{\PYZsh{} would normally have ink\PYZsq{} that yield strong negative evidence, while some}
\PY{c+c1}{\PYZsh{} pixels that sit where we naturally would expect a figure\PYZhy{}8 to be}
\PY{c+c1}{\PYZsh{} to provide mild positive evidence, the more the less likely any of the other}
\PY{c+c1}{\PYZsh{} digits are to have ink there.}

\PY{c+c1}{\PYZsh{} Let us build an alternative training set that consists of digits\PYZhy{}8 plus}
\PY{c+c1}{\PYZsh{} \PYZdq{}noise with statistically the same pixel\PYZhy{}brightness distribution\PYZdq{}.}
\PY{n}{training\PYZus{}14x14} \PY{o}{=} \PY{n}{as\PYZus{}14x14}\PY{p}{(}\PY{n}{images\PYZus{}training}\PY{p}{)}
\PY{c+c1}{\PYZsh{} These are only the digits\PYZhy{}8.}
\PY{n}{training\PYZus{}14x14\PYZus{}digit8} \PY{o}{=} \PY{n}{training\PYZus{}14x14}\PY{p}{[}\PY{n}{labels\PYZus{}training} \PY{o}{==} \PY{l+m+mi}{8}\PY{p}{]}
\PY{c+c1}{\PYZsh{} We start by repeating this dataset of digits\PYZhy{}8 nine times.}
\PY{c+c1}{\PYZsh{} On these copies, we randomly shuffle the 196 pixels, so we get \PYZdq{}noise images\PYZdq{}}
\PY{c+c1}{\PYZsh{} which however have the same ink\PYZhy{}distributions as our digit\PYZhy{}8 samples.}
\PY{n}{training\PYZus{}14x14\PYZus{}clones} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{tile}\PY{p}{(}\PY{n}{training\PYZus{}14x14\PYZus{}digit8}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{rng} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}  \PY{c+c1}{\PYZsh{} For reproducibility.}
\PY{c+c1}{\PYZsh{} Per\PYZhy{}pixel ink\PYZhy{}values are in [0..1]. We rescale to [0..0.5], then add a}
\PY{c+c1}{\PYZsh{} random integer, rescale to [num\PYZus{}samples, 14*14], sort along the total\PYZhy{}pixels}
\PY{c+c1}{\PYZsh{} axis according to ink\PYZhy{}value, and throw away the integral part and scale back.}

\PY{n}{random\PYZus{}14x14} \PY{o}{=} \PY{p}{(}
     \PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{training\PYZus{}14x14\PYZus{}clones}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{14}\PY{o}{*}\PY{l+m+mi}{14}\PY{p}{)} \PY{o}{+}
     \PY{n}{rng}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{9}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{training\PYZus{}14x14\PYZus{}clones}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{14}\PY{o}{*}\PY{l+m+mi}{14}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{random\PYZus{}14x14}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{training\PYZus{}random\PYZus{}14x14} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}
    \PY{p}{[}\PY{n}{training\PYZus{}14x14\PYZus{}digit8}\PY{p}{,}
     \PY{p}{(}\PY{n}{random\PYZus{}14x14}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
     \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{training\PYZus{}random\PYZus{}14x14}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}


\PY{n}{opt\PYZus{}params\PYZus{}rand}\PY{p}{,} \PY{n}{classifier\PYZus{}rand} \PY{o}{=} \PY{n}{get\PYZus{}digit8\PYZus{}classifier}\PY{p}{(}
    \PY{n}{training\PYZus{}random\PYZus{}14x14}\PY{p}{,}
    \PY{p}{[}\PY{l+m+mi}{8}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{training\PYZus{}14x14\PYZus{}digit8}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{training\PYZus{}14x14\PYZus{}digit8}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{9}\PY{p}{,}
    \PY{n}{debug\PYZus{}print\PYZus{}every\PYZus{}n\PYZus{}calls}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Showing the per\PYZhy{}pixel\PYZhy{}evidence\PYZhy{}contribution for this classifier would}
\PY{c+c1}{\PYZsh{} be naturally expected to give us something like an \PYZdq{}averaged digit\PYZhy{}8\PYZdq{}.}
\PY{c+c1}{\PYZsh{} This is indeed what happens here.}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n}{opt\PYZus{}params\PYZus{}rand}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}
              \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cool}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[0.02, 0.0, 0.01, 0.02, 0.01, -0.09, -0.02, -0.12, -0.04, -0.0, 0.0, 0.01,
0.01, 0.0],
 [0.0, 0.0, -0.24, -3.42, -40.18, -120.73, -574.21, 6.38, -6.84, 0.56, -25.44,
-23.46, -0.09, -0.0],
 [0.02, -0.09, -2.5, -71.73, -4.82, 7.0, 0.16, -3.4, 0.65, -2.22, -0.84, -1.58,
5.17, -132.14],
 [-0.02, -22.14, -41.19, 6.13, -3.71, 0.8, -2.13, 0.06, 0.97, 0.51, 0.5, -1.09,
-6.02, -14.92],
 [-0.01, -30.91, 11.39, -4.16, 3.03, 0.89, 1.81, -9.05, 1.41, -0.3, 0.76, 2.18,
-4.29, -70.48],
 [-0.06, -9.57, -1.2, 0.01, -0.76, 3.76, 1.43, -0.01, -2.69, 0.06, 5.22, 8.11,
5.48, -26.5],
 [0.01, -0.55, -23.04, 8.86, 2.74, -1.91, 7.52, -2.33, 2.5, -0.53, 2.29, -1.14,
3.02, -2.64],
 [0.0, -0.04, -148.09, -15.03, -9.45, 0.45, 2.13, 2.04, 1.23, -6.61, 0.68,
-0.97, 6.07, -7.64],
 [-0.01, -0.2, -109.31, -5.99, 7.9, 0.85, 4.81, -1.19, -0.96, -1.11, -2.91,
-8.95, 14.29, -1.41],
 [-0.01, -54.86, 1.46, 0.03, -2.59, 4.47, 1.8, -0.76, 1.67, 0.34, 6.12, 3.34,
-139.21, -7.2],
 [-0.04, -57.08, -4.11, -1.84, 2.67, -1.82, -1.91, 0.16, -0.97, 0.85, -4.46,
2.98, -51.81, -30.28],
 [-0.0, -12.44, 2.95, 0.94, -3.64, -0.87, 4.8, 1.45, -0.29, 4.01, -5.57, 16.58,
-345.54, -87.28],
 [0.01, -0.16, -6.87, -3.71, -1.92, 1.89, -3.54, 3.94, -1.65, 7.43, -1.51,
-14.59, -251.62, -0.01],
 [-0.01, -0.02, -18.04, -7.59, -51.83, -86.17, -71.54, -107.24, -48.96, -95.22,
-2.52, -0.01, 0.01, -0.0]]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_02_Derivatives_files/ML_02_Derivatives_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_02_Derivatives_files/ML_02_Derivatives_60_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
N=     1: score=    0.6967067467
N=     2: score=    0.2954618683
N=     3: score=    0.2270596313
N=     4: score=    0.1190390495
N=     5: score=    0.0540091239
N=     6: score=    0.0340005872
N=     7: score=    0.0185143469
N=     8: score=    0.0105583436
N=     9: score=    0.0059006104
N=    10: score=    0.0033308529
N=    11: score=    0.0018819415
N=    12: score=    0.0010696353
N=    13: score=    0.0006107751
N=    14: score=    0.0003504146
N=    15: score=    0.0002016547
N=    16: score=    0.0001161327
N=    17: score=    0.0000667174
N=    18: score=    0.0000381050
N=    19: score=    0.0000215689
N=    20: score=    0.0000120717
N=    21: score=    0.0000066713
N=    22: score=    0.0000036383
N=    23: score=    0.0000019582
N=    24: score=    0.0000010408
N=    25: score=    0.0000005468
N=    26: score=    0.0000002843
N=    27: score=    0.0000001466
N=    28: score=    0.0000000751
N=    29: score=    0.0000000383
N=    30: score=    0.0000000194
N=    31: score=    0.0000000098
N=    32: score=    0.0000000050
N=    33: score=    0.0000000025
N=    34: score=    0.0000000013
N=    35: score=    0.0000000006
N=    36: score=    0.0000000003
N=    37: score=    0.0000000002
N=    38: score=    0.0000000001
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 37
         Function evaluations: 38
         Gradient evaluations: 38
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_02_Derivatives_files/ML_02_Derivatives_60_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

A few concluding remarks. As we have seen, a substantial fraction of ML
applications are based on the idea of ``probabilistically doing the
right thing''. This means that the question ``how the training set was
sampled'' plays a very important role. Also, we leave it to the model to
``discover'', via training, what (correlation-)properties of input data
might give clues that could be useful for the task at hand.

As a consequence, this ``empirical risk minimization based'' approach to
``Artificial Intelligence'' has a rather visible tendency to see and
interpret the world that is quite different from how humans see the
world.

Our current model architecture - and also the one we will discuss next -
has an interesting property: the relative arrangement of pixels does not
matter. So, if we decided on one particular way to randomly reshuffle
(``scramble'') of all the \(14\times 14\), respectively \(28\times 28\)
input pixels, and applied that same transformation to all training and
test images, this would not impact classifier performance - whereas a
human would be hopelessly lost with such a task.

Also, with this approach, there is a risk of models discovering that
some particular ``input data constellations'' are ``rarely seen on the
training set, but tell-tale signs'', so, ``if these are present, this
should contribute a lot of evidence that overshadows other signals''.
But then, if we use such a model on input data drawn from a different
distribution than th one we drew training examples from, this can
readily lead to mis-classifications where every actually intelligent
interpreter of the data would wonder: ``What the hell is going on?''

\hypertarget{how-things-can-go-wrong}{%
\subsection{How things can go wrong}\label{how-things-can-go-wrong}}

One illustrative such sample has been given in the paper
``\href{https://www.semanticscholar.org/paper/Can-Your-AI-Differentiate-Cats-from-Covid-19Sample-Mallick-Dwivedi/c100c33afbb2efa5197f7d6042022e1227c5e298}{Can
Your AI Differentiate Cats from Covid} \cite{mallick2020can} - see Figure 1 from that
publication.

Another similar such problem, observed in a model with text awareness,
even made newspaper headlines such as with this article:
\url{https://www.theguardian.com/technology/2021/mar/08/typographic-attack-pen-paper-fool-ai-thinking-apple-ipod-clip}
- the underlying research is
``\href{https://distill.pub/2021/multimodal-neurons/}{Multimodal neurons
in artificial neural networks} \cite{goh2021multimodal}''.

A highly relevant question for applications hence is: Can we understand
how a particular classification produced by a ML model happened? Can we
explore the justification for a given classification? For Deep Learning,
the answer is pretty much always: No, except in quite unusual
circumstances, where something very clever was done to allow this.

\hypertarget{adversarial-attacks}{%
\subsubsection{Adversarial attacks}\label{adversarial-attacks}}

Given that we know how to backpropagate gradients, we can of course also
ask the question: Given a particular input, say an image, and a
classification, how sensitive is the classification with respect to not
model parameter changes (which is what we use in training), but input
data changes - so, ``what's the fastest way to distort this image of a
penguin to make the model think this is a zebra?'' Unfortunately, ML
models can easily be sensitive to very low-amplitude correlations
``which cannot be accidental'', so just distorting an input image a
little bit - if we have (say) gradients that give us some hint where to
go - can utterly fool many a ML classifier.

The study of ``adversarial attacks'' on ML systems has a long history,
with many inreresting publications. To quote from just one such paper,
\href{https://arxiv.org/abs/1412.6572}{https://arxiv.org/abs/1412.6572 -
Explaining and Harnessing Adversarial Examples} \cite{goodfellow2014explaining}:

\begin{verbatim}
8 WHY DO ADVERSARIAL EXAMPLES GENERALIZE?

An intriguing aspect of adversarial examples is that an example
generated for one model is often misclassified by other models, even
when they have different architecures or were trained on disjoint
training sets. Moreover, when these different models misclassify an
adversarial example, they often agree with each other on its
class. Explanations based on extreme non-linearity and overfitting
cannot readily account for this behavior—why should multiple extremely
non-linear model with excess capacity consistently label
out-of-distribution points in the same way? This behavior is
especially surprising from the view of the hypothesis that adversarial
examples finely tile space like the rational numbers among the reals,
because in this view adversarial examples are common but occur only at
very precise locations.

Under the linear view, adversarial examples occur in broad
subspaces. The direction eta need only have positive dot product with
the gradient of the cost function, and epsilon need only be large
enough.  Fig. 4 demonstrates this phenomenon. By tracing out different
values of epsilon we see that adversarial examples occur in contiguous
regions of the 1-D subspace defined by the fast gradient sign method,
not in fine pockets. This explains why adversarial examples are
abundant and why an example misclassified by one classifier has a
fairly high prior probability of being misclassified by another
classifier.
\end{verbatim}

\hypertarget{generative-models}{%
\subsubsection{Generative Models}\label{generative-models}}

Exploration of the phenomenon of adversarial attacks soon gave rise to
the idea of pitching one ML model against another in a co-evolutionary
``arms race'' - with one model trying to generate plausible-looking
random data (such as images trying to mimic portrait photos), and the
other model trying to tell apart synthetic data from real data. While
(co-)training such set-ups is a bit tricky, the results can be quite
spectacular - see e.g.~\url{https://www.whichfaceisreal.com/} for a
demo-and-game.

    \hypertarget{appendices}{%
\section{Appendices}\label{appendices}}

    \hypertarget{complex-backpropagation}{%
\subsection{Complex Backpropagation}\label{complex-backpropagation}}

Most ML practitioners only encounter the need to do
complex-backpropagation when they try to extract signals from Fourier
transformed audio data.

Naturally, situations where intermediate quantities are complex are much
more common in theoretical physics, so it makes sense to spend some time
on ensuring we understand how this works all the way down to the lowest
level.

The code further up did incidentally use complex-backprop in one place
(when working out coordinates of vertices along a ring on which a
membrane ends), but this use case was non-essential, and we might easily
have done this with real numbers only.

The general situation is: We have a real-valued objective function, but
the calculation uses complex intermediate results.

It is still meaningful to change perspective and see each individual
real/imaginary part as a real intermediate quantity that comes with its
own sensitivity-accumulator. Also, it is still meaningful to ask: ``by
how much would the end result change if I intercepted the forward
calculation and changed this intermediate quantity by \(\epsilon\)''?

The open question then is: Can we lift all this back interpret these
real-part-sensitivities and complex-part-sensitivities in terms of
performing complex operations?

Let us look at ``our final result is the imaginary part of a complex
intermediate quantity'', and ``the calculation consists of multiplying
the real input with \texttt{A+1j*B}'':

\begin{verbatim}
def example(x):
  A, B = 10, 20j
  C = A + 1j * B
  result_c = C * x
  result = result_c.imag
  return result
\end{verbatim}

If we instead wrote this as\ldots:

\begin{verbatim}
def example_v2(x):
  A, B = 10, 20j
  result_c_re = A * x
  result_c_im = B * x
  return result_c_im
\end{verbatim}

\ldots it is immediately clear that our sensitivity is:
\texttt{s\_x\ =\ B\ *\ s\_result\_c\_im}:

\begin{verbatim}
def example_v2_backprop(x):
  A, B = 10, 20j
  s_x = 0.0
  result_c_re = A * x
  result_c_im = B * x
  # return result_c_im
  s_result_c_im = 1
  s_x += B * s_result_c_im
\end{verbatim}

\ldots however, if, on the original code, we did
\texttt{s\_result\_c\ =\ 0+1j;\ s\_x\ +=\ C\ *\ s\_result\_c}, this
would be off - we would get \texttt{s\_x\ =\ -B} rather than
\texttt{s\_x\ =\ B}. The answer is straightforward: If we regard complex
numbers as \({\mathbb R}^2\)-vectors, then complex sensitivities ``live
in the dual vector space'', \({\mathbb R}^{2,*}\), and hence we need to
complex-conjugate. This is basically the same story as with
\(dz=dx+i\,dy\)
vs.~\(\frac{\partial}{\partial z} = \frac{\partial}{\partial x}-i\,\frac{\partial}{\partial y}\).


    % Add a bibliography block to the postdoc
    
    
    

    
    
    
    

    
    \hypertarget{basic-ml-concepts-and-ideas}{%
\section{Basic ML Concepts and
Ideas}\label{basic-ml-concepts-and-ideas}}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  More about our basic MNIST-themed binary classifiers
\item
  A Tour of General ML Concepts and Terminology
\end{enumerate}

\hypertarget{more-about-our-mnist-classifiers}{%
\subsection{More about our MNIST
classifiers}\label{more-about-our-mnist-classifiers}}

The simple ``digit-8 recognizer'' we built contains a few fundamental
ideas that are ubiquitous, and hence useful to understand thoroughly -
but we so far only justified them somewhat superficially as ``plausible
choices''. Let us try to understand these more deeply.

The trainable classifier we built only has \(14\times 14+1\) model
parameters - and rather limited performance. We will see later how to
get better performance out of training models that can capture more
complex high-dimensional structure via admitting nonlinearities.

Before we go there, let's also briefly come back to our ``k nearest
neighbors classifier'' (For more background on this, see:
\url{https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm} ).
While there are some choices to be made, such as about how many
neighbors to take into account, how to scale the dimensions (actual
features, or perhaps better feature-percentiles?), and how to weight
neighbors, and we could use optimization to fine-tune these, we see that
even with ad-hoc choices, this approach has an interesting property: If
training set examples are drawn at random and independently from the
same distribution as the examples we want to make predictions about,
then, ``given sufficiently many examples'', ``we ultimately could learn
every structure that there is in high dimensional space''. The number of
examples needed to do this might be prohibitively large, but in
principle, at some point, no matter what we see, we will have seen a few
very similar training examples to use as a reference for our decision.
Other approaches to learning then are, in a way about ``using extra
structural properties of the training examples in a clever way to be
more resource-efficient in terms of how many examples we need''. But as
a first question when evaluating a ML application or proposal, it often
makes sense to ask: ``Could a k-nearest-neighbors classifier do this at
least in principle, with very many training examples?''

The idea behind this approach resembles the design of ``case law'' -
``make decisions in alignment with what was done on a closeby other
example''.

This clearly differs from human intelligence, and in ML, there is the
technical term ``transfer learning'' (See e.g.:
\url{https://en.wikipedia.org/wiki/Transfer_learning}) to describe a
higher-level objective: Can a ML system generalize to examples beyond
the training set distribution? For many architectures, the answer is
``No'' - a ML system may be able to learn to approximate the shape of a
sine function over those 20 oscillations represented in the training
set, but when asked to extrapolate beyond that range, would typically
``just shoot off at random'', demonstrating no ``knowledge of the
essence of an oscillation''.

For practical applications, when using a k-nearest-neighbors approach,
one often finds that using a fixed-but-essentially-random projection
down to lower dimensions will retain much of the higher-dimensional
probability density lumping/clustering properties while making the
problem overall easier to manage.

Let us study those pieces of our optimization-based digit-8 classifier
we have not discussed in detail yet ``back-to-front'', i.e.~starting at
the output side.

    \hypertarget{the-loss-function}{%
\subsubsection{The Loss Function}\label{the-loss-function}}

For a given choice of parameters, we compute a score that somehow tells
us ``how well our classifier performs on the training set''.
Interestingly, we never actually use that score for anything relevant!
We only use the gradient of that score-function to minimize, and a more
primitive minimizing method, such as ``iteratively taking a small step
along the direction of the gradient'' would work as well. (Subtle
detail: The gradient is a co-vector, so ``taking a step along the
gradient direction'' is a fishy concept if we lack a way to convert a
co-vector to a vector, i.e.~some scalar product. We will come back to
this.) So, in a way, one important role of this
quality-of-prediction-score is to ensure (via
\({\rm rot}\;{\rm grad}\;\phi=0\)) ``that the steps we take to improve
performance won't make us run in circles''. If we want to report the
quality of the trained model, we generally would want to refer to other
figures of merit - and in particular, since we want to use it for
generalization, we want to report quality as measured on some test set.

But still - what does this quantity represent? If the label of a given
training example \(y\) is 1 (``positive case'') or 0 (``negative
case''), this example contributes
\(-y\,{\rm log}\,p-(1-y)\,{\rm log}\,(1-p)\) to the objective function
we want to minimize, where \(p\) is our classifier's best estimate for
``probability for this example to be a positive case''.

One somewhat intuitive-and-also-quantitative way to think about this is
based on a coding theory interpretation. As we will see, ``this quantity
measures the number of bits, in a proper continuous way, needed to
encode the actual labels given the predictions of the classifier'', or,
to say the same thing in slightly different terms, ``a quantitative
measure (in bits) of the amount of correction needed to get from the
classifier's output to the desired target `message'\,''.

Let's approach this idea in small steps. Suppose we have an `Alphabet'
consisting of only 4 symbols, A, B, C, D (if you prefer, feel free to
call these, for example, C, T, A, G). We want to communicate very long
messages made of random uncorrelated sequences of these four letters.
How many bits do we expect to need for a 100-letters message?

It actually depends on the probabilities with which each of these
letters show up. If probabilities are 1/4-1/4-1/4-1/4, we can simply
encode every letter with two bits, A=00, B=01, C=10, D=11, transport the
message, and only need 200 bits. We can always do that, irrespective of
probabilities, so we will never need more than 200 bits. But sometimes,
we can do better. Obviously, ``if C and D never show up'',
i.e.~probabilities are 1/2/-1/2-0-0, we can simply encode A=0, B=1, and
get away with 100 bits. Furthermore, ``if no other letter than A ever
shows up'', there is ``no surprise whatsoever'', we know what a length-N
message must look like, and we need 0 bits.

Now, how about a situation where probabilities are 1/2-1/4-1/4-0? Half
the letters are A, and 1/4 are B, another 1/4 are C, and D never occurs?
We have seen that we should use 1 bit to encode a p=1/2 letter, and 2
bits to encode a p=1/4 letter - and indeed, we can un-ambiguously decode
a message that uses this encoding scheme: A=0, B=10, C=11. This uses
\(\sum_i p_i {\rm log}_{1/2} p_i = 0.5\cdot 1+0.25\cdot 2+0.25\cdot 2+0=1.5\)
bits per letter, so we would expect to need 150 bits for a 100-letter
message. And we know we cannot do better than that. This is the idea
behind ``Huffman Coding''. (The information content is called
``entropy'', and this is the same ``entropy'' as in statistical
mechanics, and if the rumor is true that von Neumann proposed the term
``entropy'' somewhat jokingly, he must have been aware of this. ``Number
of different messages of that length, given this code'' corresponds to
``number of realizations of this ensemble'' - and the identification
carries over to the continuous case. The definitions only differ by a
sign.)

Now, what do we do when probabilities do not align so nicely with powers
of 2? Let's say they are 1/3-1/3-1/3-0? We could of course stick with
the previous encoding scheme, requiring 1 bit for 1/3 of the letters,
and 2 bits for the other cases, for a total of 5/3 bits/letter. There is
a way to do better, though: Let us form words made of 5 letters: Each of
them has probability \(1/3^5=1/243\). With 8 bits, we can discriminate
\(2^8=256\) cases, so, attributing one 8-bit word per each of these
5-letter words, we even have a few codes to spare. (In a practical
scheme, we might perhaps want to use of the extra 5 codes to express
``end of message, and the previous word needs to be trimmed after
1,2,3,4,5 letters''.) So, amortized, we get 8 bits / 5 letters = 1.6
bits/letter, which is somewhat better. Of course, we could look for even
longer words, where \(3^n/2^m\) comes closer to an integer (from below),
such as encoding 41-letter words with 64-bit sequences for 1.585
bits/letter. The limit is, of course,
\({\rm log}_{2}\,3\approx1.5849625\) bits/letter.

Now, is there another perspective on this that readily generalizes to
cases where probabilities are not nice fractions such as
\(1/2, 1/3, 1/4\)? Given that we are talking about tunable models, we
ideally would want to have a perspective where no probability is really
special?

Such a perspective does indeed exist, and is known as ``arithmetic
coding'' (see \url{https://en.wikipedia.org/wiki/Arithmetic_coding}).
Technically speaking, this is not quite 100\% true, since
implementations of AC generally contain one deviation from the ``pure''
idea to make them practical, but basically, the idea is that we can map
a long message that uses an alphabet with given letter-probabilities to
a small sub-interval of the unit interval \([0, 1)\) as follows: We
first partition this interval according to per-letter probabilities, in
our 4-letter case,
\([0, p_A), [p_A, p_A+p_B), [p_A+p_B,p_A+p_B+p_C), [p_A+p_B+p_C, 1)\).
The first letter selects one of these intervals. We then partition the
selected interval with the same proportions, and use the 2nd letter to
select a sub-interval, and-so-on. At the end of our message, long as it
may be, we have a small interval remaining that contains all real
numbers which we could take as valid representations of this message. If
\(b\) is an integer such that \(1/2^b\) is smaller than the length of
this interval, then this final interval is guaranteed to contain an
integer multiple \(m\) of \(1/2^b\), with \(0\le m\lt2^b\), and this
way, we can encode our message with \(b\) bits. In the limit of very
long messages, we need \({\rm log}_{1/2}\, p_X\) bits to encode a letter
\(X\) with probability \(p\).

So, if sender and receiver have a shared collection of to-be-classified
examples, and access to the same (trained) classifier, and the sender
also knows the ``ground truth'' classifications and wants to transmit
that information via a compactly encoded message, it would only need to
send corrections/amendments to the classifier's output. Let us study a
hypothetical case. We encode a ``Yes'' as ``picking the left part of a
subdivided interval''.

\begin{itemize}
\item
  Example 0 has ground truth label ``Yes'', and shared classifier says:
  ``\(p({\rm Yes})=80\mbox{\%}\)''.

  Sender narrows the {[}0, 1)-interval to {[}0.0, 0.8).
\item
  Example 1 has ground truth label ``No'', and shared classifier says:
  ``\(p({\rm Yes})=50\mbox{\%}\)'' (i.e.~classifier says ``I have a hard
  time saying anything here'').

  Sender narrows the {[}0, 0.8)-interval to {[}0.4, 0.8).
\item
  Example 2 has ground truth label ``No'', and shared classifier is
  rather confident, saying ``\(p({\rm Yes})=0.1\mbox{\%}\)''.

  Sender narrows the {[}0.4, 0.8)-interval by shaving off
  \(0.1\mbox{\%}\) of the range at the lower end, to {[}0.4004, 0.8).
\item
  Example 3 has ground truth label ``Yes'', but the shared classifier
  somewhat confidently thinks otherwise, telling us
  ``\(p({\rm Yes})=10\mbox{\%}\)''.

  Sender narrows the {[}0.4004, 0.8) range down to {[}0.4004, 0.44036).
\item
  Example 4 has ground truth label ``No'', and the shared classifier is
  leaning towards a ``Yes'', telling us ``\(p({Yes})=75\mbox{\%}\)''

  Sender narrows the {[}0.428372, 0.44036) range down to {[}0.43037,
  0.44036)
\item
  We are out of examples. The sender sees that the final interval has
  length \(\approx 1/100.1\) and sends the 7-bit binary number (most
  significant bit first) 111000, which represents 56/128=0.4375 and lies
  in the final interval.
\item
  Receiver classifies example 0, gets ``\(p({\rm Yes})=80\mbox{\%}\)''
  (like sender), sees ``The number 0.4375 is in the {[}0, 0.8)
  sub-interval'', infers ``label=Yes'', narrows the interval to {[}0.0,
  0.8).
\item
  Receiver classifies example 1, gets ``\(p({\rm Yes})=50\mbox{\%}\)'',
  sees ``The number 0.4375 is in the right half of the 50-50 split of
  the remaining interval'', infers ``label=No'', and narrows the
  interval to {[}0.4, 0.8).
\item
  \ldots and-so-on.
\end{itemize}

Nicely, we can directly convert the length of the remaining interval to
a number of bits by a continuous and smooth function:
\(b={\rm log_{1/2}} p\). Adding a letter shrinks interval-length
multiplicatively, but affects the logarithm additively, so it makes
sense to talk about ``expectation value of the logarithm of the final
interval's length'' - this then is the ``alphabet(/code)'s entropy times
message length''.

Now, what happens if we use ``number of bits (as measured in the
arithmetic-encoding way)'' - if you want so: a quantitative measure of
the ``surprise'' - to measure quality of the predictions of our ML
model? (The overall factor to convert between \({\rm log}_2\) and
\({\rm log}_e\) is if course irrelevant for minimization.) At first, the
largest contributions may well come from cases where the classifier
accidentally expresses confidence but is off. Here, gradients pull it
towards ``I should not be so sure about this case''. Fine-tuning a ``I
can even be more certain about this case'' situations contributes less -
if a classifier can go from ``99\% certain about this case'' to ``99.9\%
certain'' and is right with its assessment, this merely changes the
interval-length by a factor of about 1.0091, removing 0.013 bits of
``surprise''.

Conceptually, this ``loss function'', which is called ``cross-entropy''
is some sort of ``distance between probability distributions'' (but a
non-symmetric distance, \(d(A,B)\neq d(B,A)\) in general - we will come
back to it). The same construction is also used to answer the question
``how many bits extra do we expect to expend when encoding a
random-letters message where the letters follow probability distribution
A using a coding scheme as if they followed probability distribution B''
- and in this context, it is called ``relative entropy'' or also
``Kullback-Leibler divergence'' (see:
\url{https://en.wikipedia.org/wiki/Kullback\%E2\%80\%93Leibler_divergence}).

Naturally, we expect properly set up and trained models to be
``calibrated'' in the sense that if we take 1000 test examples for which
a binary classifier gives us a probability for the example to carry a
positive label in the range 60\%-61\% to have a positive-examples
fraction of 60\%-61\% (of course then also with sampling noise). It
might be that our first classifier is too simple to behave that way, but
calibration is in general a useful canary to check that can flag up
problems early.

Unfortunately, with the advent of ``deep learning'' architectures and
the focus-shift to ``better performance on simple metrics such as
classification accuracy makes work publishable'', proper model
calibration is no longer what one can usually expect.

See Figure 1 of
\href{https://arxiv.org/pdf/1706.04599.pdf}{https://arxiv.org/pdf/1706.04599.pdf
- On Calibration of Modern Neural Networks} \cite{guo2017calibration}.

    \hypertarget{the-logistic-sigmoid-function}{%
\subsubsection{The Logistic (``sigmoid'')
Function}\label{the-logistic-sigmoid-function}}

The other question is why we use the ``sigmoid function''
\(x\mapsto\frac{1}{1+\exp(b-x)}\) (effectively the Fermi-Dirac
distribution function \(\mu\mapsto\frac{1}{1+\exp(\beta(E-\mu))}\)) to
obtain probabilities.

At a simplistic level, ``we want to map evidence to probability, and
probabilities are in the range {[}0, 1{]}'', so we should use a function
with range such as \({\mathbb R}\to(0, 1)\), and the sigmoid fits that
bill. There is however a deeper reason.

A nice example that provides some useful background here is email SPAM
filtering. (SPAM = unsolicited commercial email.) While the questions
``why this problem exists at all'' and also ``whether automated spam
filtering actually is a meaningful approach'' very much are worthwhile
to discuss, we want to concern us with a different aspect: If we wanted
to do automated spam detection - how could one build a useful
classifier?

Until about 2002, email SPAM filtering was dominated by hand-crafted and
curated spam detection rules. This changed in 2002 with an online
article by Lisp Hacker and writer Paul Graham
(\href{http://www.paulgraham.com/spam.html}{http://www.paulgraham.com/spam.html
- A Plan for Spam} \cite{graham2002plan}). In hindsight, the reason this article exists is
that the author was not aware of earlier - flawed - research that looked
into this approach and found it to not work. The gist of Paul Graham's
idea is that we should try a ``naive Bayesian'' approach, look for
simple signals in favor and also against a given email message to be
SPAM, and then combine these signals into a spam-probability.
Interestingly, in 2002, the rather crude model proposed by Paul Graham
did indeed work very well - so well indeed that it was widely adopted
and led to a wave of very weird-looking SPAM that tried to trick
classifiers based on this idea. But what, then, was the idea? Basically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If we have a somewhat large corpus of personal email messages labeled
  as ``SPAM'', respectively ``not-SPAM'', it is simple to build a
  lexicon out of the dictionary words in all these messages and
  estimate, per-word, what the likelihood is for an email containing
  this word to be SPAM. (Different people's mailbox may give very
  different-looking such dictionaries. The term ``supergravity'' is
  generally not present in most people's mailboxes, and where it is, it
  generally is strong ``not SPAM'' evidence - while the term ``prince''
  may well provide SPAM evidence for most people.)
\item
  We bluntly treat all SPAM-relevant words as ``independent experts''.
  Suppose an email message contains only two SPAM-relevant terms, A and
  B, and we have ``\(p_A\)=\{probability estimate for an email message
  containing the term A to be SPAM\},'' as well as
  ``\(p_B=\)\{probability estimate for an email message containing the
  term B to be SPAM\}''. What is then a good probability estimate for
  the email to be SPAM if these simple-to-compute ``experts'' were
  independent (which, of course, is a very crude and doubtful
  assumption).
\end{enumerate}

Let's say we have a large body of email messages that have been sampled
in such a way that half of them are SPAM. A small fraction \(q_A\) of
these messages contain the term A, and an independent small fraction
\(q_B\) of these messages contain the term B. We determine \(p_A\) and
\(p_B\) as above. Now, on the fraction \(q_A\cdot q_B\) of email
messages that contain both the term A and B, some are SPAM and some are
not SPAM. The relative sizes of these fractions are \(p_A\cdot p_B\) and
\((1-p_A)\cdot(1-p_B)\), so the SPAM probability among these messages is
\(p_{AB}=\frac{p_Ap_B}{p_A p_B+(1-p_A)(1-p_B)}\). So, we have the
following binary function
\texttt{\$C:\{\textbackslash{}mathbb\ R\}\textbackslash{}times\{\textbackslash{}mathbb\ R\}\textbackslash{}to\{\textbackslash{}mathbb\ R\}\$}
for combining probabilities: \[
  C(a, b)=\frac{a\cdot b}{a\cdot b + (1-a)\cdot(1-b)}.
\]

Naturally, we have \(C(b, a)=C(a, b)\), as well as \(C(a, 0.5)=a\) and
also \(C(C(a, b), c)=C(a,C(b,c))\) - and as we would expect from this
motivation, a bit of algebra gives us
\(C(C(a, b), c)=\frac{a\cdot b \cdot c}{a\cdot b\cdot c+(1-a)\cdot(1-b)\cdot(1-c)}\).

Now, this structurally resembles a situation we know well from special
relativity: the relativistically correct formula for adding collinear
velocities (using \(c=1\)) is \(s(v, w)=\frac{v+w}{1+vw}\), and we also
have commutativity, associativity, and a neutral element. For
relativistic velocities, we know that the story simplifies a lot if we
regard velocities as ``dressed up other quantities'' which just can be
added linearly. These ``other quantities'' we call ``rapidities'', and
the dressing/undressing is done by \(u={\rm tanh}^{-1}\,v\),
respectively \(v={\rm tanh}\,u\).

So, is there a corresponding pair of a dressing/undressing function that
trivializes our ``addition function'' \(C(a, b)\) above - making
\texttt{C(.,.)} merely dressing-up a sum after un-dressing the summands?
Our neutral element is \(p_A=0.5\), and this should correspond to
``evidence zero''. (Clearly, adding another independent expert who
merely flips a fair coin does not help or hurt us with any
classification task.) Let us call our (not yet known)
evidence-to-probability function \(P:{\mathbb R}\to(0, 1)\). We know
that \(P(0)=0.5\) holds. Also, we expect \(P\) to be smooth. Let us
define \(P'(0):=\gamma\), so that we have an expansion
\(P(\epsilon)=0.5+\gamma \epsilon+{\mathcal O}(\epsilon^2)\). Since we
want \(P(x+y)=P(x)P(y)/(P(x)P(y)+(1-P(x))(1-P(y))\) to hold, we can use
this to expand \(P(x+\epsilon)\) to 1st order in \(\epsilon\) and derive
the differential equation \(P'(x)=4\gamma P(x)(1-P(x))\).

So far, \(\gamma\) is an arbitrary slope-paramter. We naturally would
want \(\gamma\lt 0\) (``more positive evidence increases the
probability''), but its value basically is merely about picking some
nice normalization on the evidence-scale (like ``setting \(c=1\)'' for
special relativity). Here, a natural choice is \(\gamma=1/4\), and with
this we get the ODE \(P'=P(1-P)\).

The next steps are straightforward: Let us define \(Q(x):=1/P(x)\),
which gives us \(Q'/(Q-1)=-1\), so \({\rm log}\,(Q(x)-1)=-x+c\), and
from this we find (for \(\gamma=1/4\), and also using \(P(0)=0.5\)):

\[P(x)=\frac{1}{1+\exp(-x)}\].

In the context of statistics (or ML), this is called the ``logistic''
function. Its inverse is the ``logit'' function:

\[P^{-1}(p)={\rm log}\frac{p}{1-p}=2\,{\rm tanh}^{-1}(2p-1)\].

    \hypertarget{multiclass-classification}{%
\subsubsection{Multiclass
classification}\label{multiclass-classification}}

While we now fully understand every aspect of the construction of our
simple digit-classifier, there is one detail left to look into: So far,
we discussed a simplified ``binary classification'' setting. How about
1-in-10 classification? We could of course train 10 binary classifiers
(``this vs.~everything else''), and then somehow try to merge these. In
terms of fundamental properties, what would we expect the ``loss
function'' to look like? Important desiderata are:

\begin{itemize}
\item
  We still want to be able to map some ``(linearly) accumulated
  independent (per-class) evidence'' to (per-class) probability.
\item
  Using relative entropy / KL divergence as the objective function still
  seems to make sense. For the ``correct'' label, we get a contribution
  of \(-{\rm log}\, p_i\), and for each incorrect label,
  \(-{\rm log}(1-p_k)\).
\item
  We again would like to have some simple ``naive Bayesian''
  interpretation to hold. To give a simple example, for two independent
  1-out-of-3 classifiers that see evidence-vectors (of independent
  evidence) \((e_{1A}, e_{1B}, e_{1C})\), respectively
  \((e_{2A}, e_{2B}, e_{2C})\), and attribute to these the corresponding
  probability-vectors \((p_{1A}, p_{1B}, p_{1C})\),
  \((p_{2A}, p_{2B}, p_{2C})\), we would like identities such as this
  one to hold:

  \[
   p_{(1+2)A} = \frac{p_{1A}p_{2A}}{p_{1A}p_{2A}+p_{1B}p_{2B}+p_{1C}p_{2C}}
  \] In words: combining the assessments from both independent experts,
  out of all examples that expert 1 attributes probabilities
  \((p_{1A}, p_{1B}, p_{1C})\) to, and expert 2 attributes probabilities
  \((p_{2A}, p_{2B}, p_{2C})\) to {[}fine print: where these
  probabilities are representative for some chunk of
  probability-space{]}, the relative odds for ``true label is A'' :
  ``true label is B'' are \(p_{1A}p_{2A}:p_{1B}p_{2B}\), and likewise
  for the other two combinations.
\end{itemize}

If we want probabilities to behave multiplicatively when we add
independent evidence, this means that the probability-ratio for two
classes \(p_A:p_B\) should be given by the exponential of the evidence
difference: \(p_A:p_B=\exp(e_A-e_B)\). This then gives us these
probabilities:

\[
(p_A, p_B, p_C) = (\exp(e_A)/Z, \exp(e_B)/Z, \exp(e_C)/Z),\;\;{\rm with}\;Z:=\exp(e_A)+\exp(e_B)+\exp(e_C).
\]

Or, for \(N\) classes:

\[p_i=\frac{\exp e_i}{\sum_k \exp e_k}.\]

{[}\textbf{Participant Exercise}: Show that in the case of binary
classification, this indeed reduces to the logistic function.{]}

This function, called ``softmax'' in a Machine Learning context, is of
course just the Boltzmann distribution. Intuitively, there is little
surprise to the physics-trained eye here (perhaps depending on how much
of a physical chemist one is). If we think of an individual example's
total classification-probability as some kind of molecular substance
where each individual molecule can be in one out of multiple
discriminable but energetically-equivalent configurations (each
configuration corresponding to a ML label class), then the
thermodynamic-equilibrium proportions of the different configurations
(ML perspective: per-class probabilities for the example) would be
determined by the chemical potential. In this picture, the accumulated
per-class-evidence plays the role of a ``chemical potential'' (up to a
sign).

    \hypertarget{more-about-the-pedestrian-thermodynamics-interpretation}{%
\subsubsection{More about the (``pedestrian''-)thermodynamics
interpretation}\label{more-about-the-pedestrian-thermodynamics-interpretation}}

Suppose we build an image classifier that can discriminate three types
of fruit: apples, oranges, and bananas. Let's say that for each type of
fruit, there is a hidden ``internal'' classification system that an
expert on the fruit could use to discern three independent describing
attributes, such as ``variety'' (with, say, 5 options per fruit-type),
``ripeness'' (typically going from ``still green'' to ``green once
again'' - let's say also with 5 options), and ``imperfections'' (also
with 5 options).

Now, let's say the probability for each combination of
\texttt{(fruit-type,\ fruit-variety,\ fruit-ripeness,\ fruit-imperfections)}
is the same according to the distribution from which we draw our
examples.

What happens if our ML-classifier, when analyzing an example, extracted
a feature that could be described in approximate-human-terms as ``this
fruit has a brown spot''? Let's say two of the five
imperfection-categories for bananas and apples are compatible with
``brown spot observed'', but only one for ``oranges''. Then, this one
feature-observation opens up a one-of-two-choices degree of freedom for
apples and bananas, but we only have a one-of-five-choice for oranges.
Then, there are twice as many ways how the observations could be
compatible with ``we are looking at an apple'' in comparison to ``we are
looking at an orange''. Likewise for banana-vs-orange. So, the ``space
of possibilities'' has acquired a relative factor 2 each for two fruit
types, and ``evidence'', which should be a linear additive quantity
measuring the space of options (like entropy) hence should increase by
the log of that factor. Knowing nothing else, having observed a brown
spot overall makes it less likely that we are looking at an orange, but
this is of course a pure gambler's argument that does not involve any
``understanding'' whatsoever.

Now, let us compare this to a pedagogically-constructed chemical
situation. Suppose we have a gas of 5-atomic molecules, a carbon-12
``stereocenter'' in the middle and four different atoms around it -
let's say F, Cl, Br, I. Each molecule can be in left-handed (``S'') or
right-handed (``R'') configuration. Let's say we also provide a catalyst
that allows removing and reattaching an I-atom, and when reacting with
the catalyst, the left-handed form can turn into the right-handed form
and vice versa. Furthermore, let's assume each of these
odd-number-of-protons nuclei had a total nuclear spin of 1/2. Then,
focusing on nuclear spin, we could describe that part of the quantum
state of such a molecule as an element of a 2x2x2x2-dimensional Hilbert
space, one dimension per nucleus. If we started from 100\% left-handed
molecules, interaction with the catalyst would get us into an
equilibrium that is a 50\%-left-50\%-right mixture. Now, suppose we had
some ``thermodynamic daemon'' type magical catalyst which, whenever a
left-handed molecule gets turned into a right-handed one, it replaced
the iodine atom with one that has nuclear spin zero, and replaced the
spin-zero iodine on a right-handed molecule with a spin-1/2 one as it
gets turned into a left-handed molecule again? Then, the
nuclear-quantum-state Hilbert space for the right handed form would be
2x2x2x1-dimensional, ``having one degree of freedom less''. In terms of
``logarithm of the size of the space of opportunities'', there is a
log(2)-difference between these cases, and this then is the contribution
to the chemical potential that favors the ``more configurations
available'' left handed form. Going back from chemical potential to
probabilities, we exponentiate that difference again, and get a
2/3-left-to-1/3-right ratio, which we can attribute to ``left form
having one degree of freedom more''.

    \hypertarget{some-concepts-and-terminology}{%
\section{Some Concepts and
Terminology}\label{some-concepts-and-terminology}}

We built a rather simplistic ``logistic regression based'' handwritten
digit classifier. Before we explore how to improve this, we should look
into some concepts and terminology that allow us to form an idea of what
``classifier A performs better than classifier B'' means.

For a simple benchmark problem such as ``handwritten digit
classification'' where each digit is equally
likely\href{Note\%20however\%20that\%20among\%20actually-handwritten-by-people\%20digits,\%20we\%20likely\%20would\%20see\%20more\%201-s\%20than\%209-s.(See:\%20https://en.wikipedia.org/wiki/Benford\%27s_law).}{1},
a very basic performance metric, measured on some ``test set'' that was
set aside so that it could not possibly have affected decisions entering
the design or training of the classifier, is \textbf{accuracy}, i.e.~the
fraction of correctly classified examples. This, however, can be
misleading, especially\ldots{}

\begin{itemize}
\tightlist
\item
  \ldots if some mis-classifications are substantially more painful than
  others, and also
\item
  \ldots if there is substantial class imbalance, such as: ``99\% of
  examples are Negative anyhow''.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let us first focus on some terms related to \textbf{binary
classification} tasks. The difference between ``type I error'' and
``type II error'' is commonly discussed in secondary education: A ``type
I error'' is a ``False Positive'' - ``we cry wolf but there is no
wolf''. A ``type II error'' is a ``False Negative'' - ``we fail to cry
wolf but should have''. Overall, we have:

\begin{itemize}
\tightlist
\item
  True Positive Fraction (TP) - fraction of true positives (ground truth
  label = yes; classification = yes) in the example set.
\item
  True Negative Fraction (TN) - fraction of label=no, classification=no.
\item
  False Positive Fraction (FP) - fraction of label=no,
  classification=yes.
\item
  False Negative Fraction (FN) - fraction of label=yes,
  classification=no.
\end{itemize}

In terms of these, ``accuracy'' is TP+TN, i.e.~the fraction of correctly
classified cases. We also define ``recall'' as ``fraction of label=yes
examples we manage to classify as positive'', i.e.~TP/(TP+FN), as well
as ``precision'' as ``fraction of label=yes examples among those we
classify as positive'', i.e.~TP/(TP+FP). There are other - derived -
metrics such as F1-score, which are however less commonly reported.

In general, for binary classification tasks, our classifier will produce
some sort of ``confidence'' for a classification, and with proper
calibration, this may take the form of ``I am 80\% confident this is a
Yes''. We would then expect about 20\% of all those example to which a
(properly calibrated!) classifier attributes a probability in the
interval \([80\%-\epsilon, 80\%+\epsilon]\) to actually be negative
cases.

We obviously have a choice of a threshold here: Do we want to play it
safe and only classify examples as ``yes'' where the classifier is
\(\ge 99\%\) certain of that classification, accepting that we will miss
some positive cases, or do we want to catch as much as we can,
classifying even ``slightly suspicious cases'' as ``interesting (such
as: for subsequent investigation)'' as ``positive'', perhaps using even
a threshold of only \(1\%\) - or something in between?

Let us consider a fixed set of labeled examples and see what happens if
we start from a threshold of \(100\%\) and gradually lower it.

Each example has been given a score by the classifier, and all that
matters is the ordering of examples by score, and what happens when
lowering the threshold crosses the score of the next example. These
properties are invariant under mapping classifier score to some other
score as long as we use a strictly monotonically increasing mapping.

For thresholds higher than the maximal example score, we classify
nothing as positive, so have TP = FP = 0.

For thresholds lower than the minimal example score, we classify
everything as positive, and have TP = FP = 1.

What happens if we start at high threshold, TP=FP=0 and keep lowering
the threshold? We keep crossing examples which change their
classification from ``negative'' to ``positive''. If the label was
``positive'', this increases TP by 1/\{number of positive examples\}. If
the label was ``negative'', this increases FP by 1/\{number of negative
examples\}. So, we either take a small step in +x or in +y direction on
the (x=FP, y=TP) plot.

If the ``classifier'' just randomly guessed scores, and we have a
fraction p of positive examples, then at every score-crossing, we will,
with probability p, take a step of size 1/p in y=TP direction, and with
probability 1-p take a step of size 1/(1-p) in x=FP direction. So, the
TP/FP-for-all-thresholds curve will be a ``noisy diagonal line''. One
relevant metric is ROC-AUC, the ``receiver operating characteristic
area-under-curve''. Here, the TP/FP-curve just described is called the
``receiver operating characteristic'' (ROC). This is a
threshold-invariant measure of classifier performance. For a
good-quality classifier, we expect that lowering the threshold starts
catching many correctly-classified examples, so TP should go up with FP
ideally only increasing a little - while for low thresholds, we
ultimately also mis-classify all negative examples as positive, and the
curve reaches (x=TP=1, y=FP=1).

The ROC-AUC can be interpreted directly as the probability for a
randomly picked positive-label and negative-label example to have
classifier-scores ``in good order'', i.e.~positive-label example has
higher score than negative-label example. This is so for the following
reason: If we lower the threshold until we make the next False-Positive
mis-classification, we move a stretch of 1/(1-p) along the x=FPR axis,
at y=TPR-at-this-new-threshold. So, to each negative example, we can
attribute a rectangle on the graph that measures the fraction of
positive examples among all positive examples which have a score lower
than this one.

Details and plots can be found on:
\url{https://en.wikipedia.org/wiki/Receiver_operating_characteristic}.

    \hypertarget{a-dl-classifier-from-scratch}{%
\subsection{A DL Classifier ``from
scratch''}\label{a-dl-classifier-from-scratch}}

Let us see if we can build a full-fledged ``Deep'' Neural Network based
handwritten digit classifier entirely from scratch (only a few hidden
layers, mostly to illustrate the key ideas).

This is indeed feasible, but we need to look into a few tricks.

Pre-DL, the situation was that, even if we managed to train a
fully-connected deep neural network with more than 2, perhaps 3, layers,
this generally would not have managed to add any benefit. Nowadays, we
know how to even get performance improvements from extra layers at 100+
layers deep.

Why do this ``from scratch''? As physicists, ``we understand what we can
create'', and we may sometimes find ourselves in a situation where we
want to make changes to something that just about every ML framework
would not allow us to change.

What is the basic idea behind ``deep learning'' architectures? So far,
we have built a simple logistic regression classifier, ``where every
pixel was an input feature''. We could consider improving this by adding
``feature products'', i.e.~also add tunable weights for
products-of-intensities-of-two-pixels. We might want to simplify this by
first performing a (tuneable) linear projection from \(N\times N\)
pixels to some \(D\)-dimensional space, \(D\ll N\times N\), and then
taking our features from perhaps
\(D\oplus D\otimes D\oplus D\otimes D\otimes D\oplus\ldots\) - and this
would indeed ``allow learning to become sensitive to more complicated
high-dimensional distribution structure''.

Depending on the problem, we might even hand-craft some of the features,
perhaps after doing extensive research on ``where the evidence sits''.
Such ``feature engineering'' was an important part of pre-DL ML, and a
common situation was that ML turned out to be ``merely a tool to guide
problem analysis, and once we understand the problem, it falls apart and
we see how to do this without using ML''.

Post-DL-revolution, the question of which model architectures (/
architecture elements or ideas) are especially useful for which types of
problems is still relevant, but the focus has shifted away from
engineering extraction of individual features. Often, we are in a
situation where ``the entire tree gets pushed in on one side, and a
table comes out on the other side, and we can leave it to ML training to
figure out on its own what part is useful to make what part of the
result.''

For handwritten digit classification, making this possible is mostly
about avoiding some earlier mistakes.

The basic architectural choice for this exercise will be a rather simple
fully connected Neural Network, where each layer has a certain number of
units each of which receives a trainable linear combination of the
outputs of the previous layer as input, and applies a simple
nonlinear-but-differentiable \({\mathbb R}\to{\mathbb R}\) function to
this linear combination to produce its output.

Specifically, let us use these tricks (there are a few more which we
will, for now, deliberately ignore):

\begin{itemize}
\tightlist
\item
  We normalize our input-vectors such that features have mean zero and
  variance 1.
\item
  We use many training examples (tens of thousands).
\item
  We use a blunt approach to fast training (``stochastic gradient
  descent''):

  \begin{itemize}
  \tightlist
  \item
    Estimate, rather than compute, the training set loss (rather, its
    gradient) by only considering a small random sample of all training
    set exercises (such as: 32) for each parameter-update.
  \item
    Multiply the
    \(\partial(\mbox{loss})/\partial(\mbox{parameter k})\)-gradient with
    a small (negative) step-size, the ``learning rate'', and update
    parameters by walking in that direction.
  \end{itemize}
\item
  We pay some attention to random parameter-initialization, the basic
  idea being: ``If for every input, activations are roughly
  normal-distributed, this should also hold for every output - if inputs
  are independent.''
\end{itemize}

This then leads us to the code shown below - here, for the last time,
with hand-crafted backpropagation.

For this example, we will need some nonlinearity. The actual choice does
not matter too much, but let us pick a function that is symmetric around
zero, has a near-zero linear region where it is approximated well by the
identity, stays very roughly linear for a value of up to about 1 or so,
has a somewhat simple analytic derivative, and does not flatten
exponentially, so further-out regions still do contribute somewhat to
gradients. One function that fits the bill here is the inverse tangent
(arctangent) function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Importing the relevant Python libraries (\PYZdq{}modules\PYZdq{}).}
\PY{k+kn}{import} \PY{n+nn}{math}
\PY{k+kn}{import} \PY{n+nn}{pprint}
\PY{k+kn}{import} \PY{n+nn}{time}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot}
\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}

\PY{c+c1}{\PYZsh{} Larger figures.}
\PY{n}{matplotlib}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}nn\PYZus{}classifier\PYZus{}func}\PY{p}{(}\PY{n}{parameters}\PY{o}{=}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a neural network classifier function.}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    parameters: A sequence of 2\PYZhy{}index numpy.ndarray\PYZhy{}s `wb`, each of shape}
\PY{l+s+sd}{      [layer\PYZus{}width + 1, next\PYZus{}layer\PYZus{}width] that represent the weights}
\PY{l+s+sd}{      (indexed `wb[input\PYZus{}node\PYZus{}index, output\PYZus{}node\PYZus{}index]`) and biases}
\PY{l+s+sd}{      (indexed `wb[layer\PYZus{}width, output\PYZus{}node\PYZus{}index]`) for the inter\PYZhy{}layer}
\PY{l+s+sd}{      connections.}

\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    A function `f` with signature}
\PY{l+s+sd}{    ```def f(batch\PYZus{}features, batch\PYZus{}labels, want\PYZus{}gradient=False):}
\PY{l+s+sd}{         ...}
\PY{l+s+sd}{         return loss, softmax\PYZus{}probs, opt\PYZus{}grad\PYZus{}param\PYZus{}pieces}
\PY{l+s+sd}{    ```}
\PY{l+s+sd}{    where `batch\PYZus{}features` is a [batch\PYZus{}size, num\PYZus{}features] numpy\PYZhy{}array\PYZhy{}like}
\PY{l+s+sd}{    value providing batch\PYZhy{}features (which will internally get converted}
\PY{l+s+sd}{    to a numpy array), `batch\PYZus{}labels` is a [batch\PYZus{}size, num\PYZus{}classes]}
\PY{l+s+sd}{    numpy\PYZhy{}array\PYZhy{}like value providing per\PYZhy{}example labels in one\PYZhy{}hot encoded}
\PY{l+s+sd}{    form, batch\PYZhy{}indexed in parallel to `batch\PYZus{}features`, and `want\PYZus{}gradient`}
\PY{l+s+sd}{    determines whether gradients should be computed. If this is logically}
\PY{l+s+sd}{    false, the `opt\PYZus{}grad\PYZus{}param\PYZus{}pieces` part of the returned tuple is None,}
\PY{l+s+sd}{    otherwise it is a sequence of numpy\PYZhy{}arrays indexed in parallel to}
\PY{l+s+sd}{    `parameters` which provides the corresponding gradient\PYZhy{}component}
\PY{l+s+sd}{    of the loss with respect to the parameters. Correspondingly, the `loss`}
\PY{l+s+sd}{    part of the returned tuple is the averaged\PYZhy{}across\PYZhy{}batch\PYZhy{}examples}
\PY{l+s+sd}{    cross\PYZhy{}entropy loss of the prediction vs. the given labels on the batch,}
\PY{l+s+sd}{    and `softmax\PYZus{}probs` is a vector of predicted probabilities.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{def} \PY{n+nf}{loss\PYZus{}pred\PYZus{}grad}\PY{p}{(}\PY{n}{batch\PYZus{}features}\PY{p}{,} \PY{n}{batch\PYZus{}labels}\PY{p}{,}
                     \PY{n}{want\PYZus{}gradient}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Runs inference and optionally computes gradients.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} Labels must be one\PYZhy{}hot!}
    \PY{n}{batch\PYZus{}features} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{batch\PYZus{}features}\PY{p}{)}
    \PY{n}{batch\PYZus{}labels} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{batch\PYZus{}labels}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{batch\PYZus{}features}\PY{o}{.}\PY{n}{dtype}\PY{p}{)}
    \PY{n}{intermediate\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{batch\PYZus{}features}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{n}{batch\PYZus{}labels}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{c+c1}{\PYZsh{} Per batch\PYZhy{}example, turn input features into a vector.}
    \PY{n}{activations\PYZus{}now} \PY{o}{=} \PY{n}{batch\PYZus{}features}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{batch\PYZus{}features}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{for} \PY{n}{num\PYZus{}layer}\PY{p}{,} \PY{n}{layer\PYZus{}params} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{parameters}\PY{p}{)}\PY{p}{:}
      \PY{n}{weights} \PY{o}{=} \PY{n}{layer\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
      \PY{n}{biases} \PY{o}{=} \PY{n}{layer\PYZus{}params}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}
      \PY{n}{pre\PYZus{}nonlinearity} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bi,io\PYZhy{}\PYZgt{}bo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                      \PY{n}{activations\PYZus{}now}\PY{p}{,}
                                      \PY{n}{weights}\PY{p}{)} \PY{o}{+} \PY{n}{biases}
      \PY{k}{if} \PY{n}{num\PYZus{}layer} \PY{o}{+} \PY{l+m+mi}{1} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{parameters}\PY{p}{)}\PY{p}{:}
        \PY{n}{post\PYZus{}nonlinearity} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{arctan}\PY{p}{(}\PY{n}{pre\PYZus{}nonlinearity}\PY{p}{)}
      \PY{k}{else}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} No activation on final layer, which inputs into softmax.}
        \PY{n}{post\PYZus{}nonlinearity} \PY{o}{=} \PY{n}{pre\PYZus{}nonlinearity}
      \PY{n}{intermediate\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{activations\PYZus{}now}\PY{p}{,}
                                \PY{n}{pre\PYZus{}nonlinearity}\PY{p}{,}
                                \PY{n}{post\PYZus{}nonlinearity}\PY{p}{)}\PY{p}{)}
      \PY{n}{activations\PYZus{}now} \PY{o}{=} \PY{n}{post\PYZus{}nonlinearity}
    \PY{c+c1}{\PYZsh{} We still have to convert the final activations to probabilities \PYZhy{}\PYZhy{}}
    \PY{c+c1}{\PYZsh{} which we do via softmax. We use a trick to avoid large arguments}
    \PY{c+c1}{\PYZsh{} to exp(), shifting all activations to max=0.}
    \PY{n}{max\PYZus{}activation\PYZus{}indices} \PY{o}{=} \PY{n}{activations\PYZus{}now}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{max\PYZus{}activations} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{fromiter}\PY{p}{(}
        \PY{p}{(}\PY{n}{example}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{k}{for} \PY{n}{example}\PY{p}{,} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{activations\PYZus{}now}\PY{p}{,} \PY{n}{max\PYZus{}activation\PYZus{}indices}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{dtype}\PY{o}{=}\PY{n}{activations\PYZus{}now}\PY{o}{.}\PY{n}{dtype}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
    \PY{n}{max\PYZus{}activations} \PY{o}{=} \PY{n}{activations\PYZus{}now}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{shifted\PYZus{}activations} \PY{o}{=} \PY{n}{activations\PYZus{}now} \PY{o}{\PYZhy{}} \PY{n}{max\PYZus{}activations}
    \PY{n}{softmax\PYZus{}weights} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{shifted\PYZus{}activations}\PY{p}{)}
    \PY{n}{softmax\PYZus{}denoms} \PY{o}{=} \PY{n}{softmax\PYZus{}weights}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{softmax\PYZus{}probs} \PY{o}{=} \PY{n}{softmax\PYZus{}weights} \PY{o}{/} \PY{n}{softmax\PYZus{}denoms}
    \PY{n}{entropy\PYZus{}contribs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{batch\PYZus{}labels}\PY{p}{,}
                                   \PY{o}{\PYZhy{}}\PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}12}\PY{o}{+}\PY{n}{softmax\PYZus{}probs}\PY{p}{)}\PY{p}{,}
                                   \PY{o}{\PYZhy{}}\PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{softmax\PYZus{}probs}\PY{o}{+}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{)}\PY{p}{)}
    \PY{n}{loss} \PY{o}{=} \PY{n}{entropy\PYZus{}contribs}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} per\PYZhy{}batch\PYZhy{}example}
    \PY{c+c1}{\PYZsh{}}
    \PY{k}{if} \PY{o+ow}{not} \PY{n}{want\PYZus{}gradient}\PY{p}{:}
      \PY{k}{return} \PY{n}{loss}\PY{p}{,} \PY{n}{softmax\PYZus{}probs}\PY{p}{,} \PY{k+kc}{None}
    \PY{c+c1}{\PYZsh{} We want a gradient.}
    \PY{c+c1}{\PYZsh{} Backpropagation code below.}
    \PY{c+c1}{\PYZsh{} [We already know how this works, so this may perhaps be only be of}
    \PY{c+c1}{\PYZsh{}  interest for participants who want to see more code examples. We need}
    \PY{c+c1}{\PYZsh{}  not discuss this in detail. \PYZdq{}No really new tricks here.\PYZdq{}]}
    \PY{n}{s\PYZus{}entropy\PYZus{}contribs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{p}{[}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{batch\PYZus{}size}\PY{p}{)}
    \PY{n}{s\PYZus{}log\PYZus{}softmax\PYZus{}probs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{batch\PYZus{}labels}\PY{p}{,}
                                      \PY{o}{\PYZhy{}}\PY{n}{s\PYZus{}entropy\PYZus{}contribs}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
    \PY{n}{s\PYZus{}log\PYZus{}complementary\PYZus{}softmax\PYZus{}probs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{batch\PYZus{}labels}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,}
                                                    \PY{o}{\PYZhy{}}\PY{n}{s\PYZus{}entropy\PYZus{}contribs}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Here, we recompute some simple intermediate quantities which we did not}
    \PY{c+c1}{\PYZsh{} bother to hold on to.}
    \PY{n}{s\PYZus{}softmax\PYZus{}probs} \PY{o}{=} \PY{n}{s\PYZus{}log\PYZus{}softmax\PYZus{}probs} \PY{o}{/} \PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}12} \PY{o}{+} \PY{n}{softmax\PYZus{}probs}\PY{p}{)}
    \PY{n}{s\PYZus{}softmax\PYZus{}probs} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{s\PYZus{}log\PYZus{}complementary\PYZus{}softmax\PYZus{}probs} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{softmax\PYZus{}probs}\PY{o}{+}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{)}
    \PY{n}{s\PYZus{}softmax\PYZus{}denoms} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{s\PYZus{}softmax\PYZus{}probs} \PY{o}{*} \PY{n}{softmax\PYZus{}weights} \PY{o}{/}
                         \PY{n}{softmax\PYZus{}denoms}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{s\PYZus{}softmax\PYZus{}weights} \PY{o}{=} \PY{n}{s\PYZus{}softmax\PYZus{}probs} \PY{o}{/} \PY{n}{softmax\PYZus{}denoms}
    \PY{n}{s\PYZus{}softmax\PYZus{}weights} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}softmax\PYZus{}denoms}
    \PY{n}{s\PYZus{}shifted\PYZus{}activations} \PY{o}{=} \PY{n}{softmax\PYZus{}weights} \PY{o}{*} \PY{n}{s\PYZus{}softmax\PYZus{}weights}
    \PY{n}{s\PYZus{}max\PYZus{}activations} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{s\PYZus{}shifted\PYZus{}activations}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{s\PYZus{}activations\PYZus{}now} \PY{o}{=} \PY{n}{s\PYZus{}shifted\PYZus{}activations}
    \PY{c+c1}{\PYZsh{} This step is a bit messy \PYZhy{} we here have to do in Python what ought}
    \PY{c+c1}{\PYZsh{} to be handled in NumPy.}
    \PY{k}{for} \PY{n}{num\PYZus{}example}\PY{p}{,} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{max\PYZus{}activation\PYZus{}indices}\PY{p}{)}\PY{p}{:}
      \PY{n}{s\PYZus{}activations\PYZus{}now}\PY{p}{[}\PY{n}{num\PYZus{}example}\PY{p}{,} \PY{n}{k}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}max\PYZus{}activations}\PY{p}{[}\PY{n}{k}\PY{p}{]}
    \PY{c+c1}{\PYZsh{} Here, `s\PYZus{}activations\PYZus{}now` are the sensitivities on the final inputs}
    \PY{c+c1}{\PYZsh{} to softmax.}
    \PY{n}{grad\PYZus{}param\PYZus{}pieces} \PY{o}{=} \PY{p}{[}\PY{k+kc}{None}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{parameters}\PY{p}{)}
    \PY{k}{for} \PY{n}{num\PYZus{}layer\PYZus{}transition} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{parameters}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
      \PY{n}{k} \PY{o}{=} \PY{n}{num\PYZus{}layer\PYZus{}transition} \PY{c+c1}{\PYZsh{} Abbrev.}
      \PY{n}{s\PYZus{}post\PYZus{}nonlinearity} \PY{o}{=} \PY{n}{s\PYZus{}activations\PYZus{}now}
      \PY{n}{activations}\PY{p}{,} \PY{n}{pre\PYZus{}nonlinearity}\PY{p}{,} \PY{n}{post\PYZus{}nonlinearity} \PY{o}{=} \PY{p}{(}
          \PY{n}{intermediate\PYZus{}data}\PY{p}{[}\PY{n}{num\PYZus{}layer\PYZus{}transition}\PY{p}{]}\PY{p}{)}
      \PY{k}{if} \PY{n}{num\PYZus{}layer\PYZus{}transition} \PY{o}{+} \PY{l+m+mi}{1} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{parameters}\PY{p}{)}\PY{p}{:}
        \PY{n}{s\PYZus{}pre\PYZus{}nonlinearity} \PY{o}{=} \PY{n}{s\PYZus{}post\PYZus{}nonlinearity} \PY{o}{/} \PY{p}{(}
            \PY{l+m+mi}{1} \PY{o}{+} \PY{n}{numpy}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{pre\PYZus{}nonlinearity}\PY{p}{)}\PY{p}{)}
      \PY{k}{else}\PY{p}{:}
        \PY{n}{s\PYZus{}pre\PYZus{}nonlinearity} \PY{o}{=} \PY{n}{s\PYZus{}post\PYZus{}nonlinearity}
      \PY{c+c1}{\PYZsh{} Forward code was:}
      \PY{c+c1}{\PYZsh{} weights = layer\PYZus{}params[:\PYZhy{}1, :]}
      \PY{c+c1}{\PYZsh{} biases = layer\PYZus{}params[\PYZhy{}1:, :]}
      \PY{c+c1}{\PYZsh{} pre\PYZus{}nonlinearity = numpy.einsum(\PYZsq{}bi,io\PYZhy{}\PYZgt{}bo\PYZsq{},}
      \PY{c+c1}{\PYZsh{}                                 activations\PYZus{}now,}
      \PY{c+c1}{\PYZsh{}                                 weights) + biases}
      \PY{n}{layer\PYZus{}params} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{n}{num\PYZus{}layer\PYZus{}transition}\PY{p}{]}
      \PY{n}{weights} \PY{o}{=} \PY{n}{layer\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
      \PY{n}{biases} \PY{o}{=} \PY{n}{layer\PYZus{}params}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}
      \PY{n}{s\PYZus{}layer\PYZus{}params} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{layer\PYZus{}params}\PY{p}{)}
      \PY{c+c1}{\PYZsh{} Bias\PYZhy{}sensitivities.}
      \PY{n}{s\PYZus{}layer\PYZus{}params}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{s\PYZus{}pre\PYZus{}nonlinearity}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
      \PY{n}{s\PYZus{}layer\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo,bi\PYZhy{}\PYZgt{}io}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s\PYZus{}pre\PYZus{}nonlinearity}\PY{p}{,} \PY{n}{activations}\PY{p}{)}
      \PY{n}{s\PYZus{}activations\PYZus{}now} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo,io\PYZhy{}\PYZgt{}bi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s\PYZus{}pre\PYZus{}nonlinearity}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
      \PY{n}{grad\PYZus{}param\PYZus{}pieces}\PY{p}{[}\PY{n}{num\PYZus{}layer\PYZus{}transition}\PY{p}{]} \PY{o}{=} \PY{n}{s\PYZus{}layer\PYZus{}params}
    \PY{k}{return} \PY{n}{loss}\PY{p}{,} \PY{n}{softmax\PYZus{}probs}\PY{p}{,} \PY{n}{grad\PYZus{}param\PYZus{}pieces}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{return} \PY{n}{loss\PYZus{}pred\PYZus{}grad}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us first try out a really simple \PYZdq{}random\PYZdq{} example,}
\PY{c+c1}{\PYZsh{} checking if our gradients work. (They do!)}
\PY{n}{rng} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{demo\PYZus{}params} \PY{o}{=} \PY{p}{[}\PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{,}
               \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{,}
               \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{]}
\PY{n}{demo\PYZus{}batch\PYZus{}features}\PY{p}{,} \PY{n}{demo\PYZus{}batch\PYZus{}labels} \PY{o}{=} \PY{p}{(}
    \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
    \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{f\PYZus{}demo} \PY{o}{=} \PY{n}{get\PYZus{}nn\PYZus{}classifier\PYZus{}func}\PY{p}{(}\PY{n}{demo\PYZus{}params}\PY{p}{)}
\PY{n}{f\PYZus{}demo\PYZus{}val}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{f\PYZus{}demo\PYZus{}grad} \PY{o}{=} \PY{n}{f\PYZus{}demo}\PY{p}{(}\PY{n}{demo\PYZus{}batch\PYZus{}features}\PY{p}{,} \PY{n}{demo\PYZus{}batch\PYZus{}labels}\PY{p}{,}
                                 \PY{n}{want\PYZus{}gradient}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}demo\PYZus{}val:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{f\PYZus{}demo\PYZus{}val}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{p}{[}\PY{n}{t}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{f\PYZus{}demo\PYZus{}grad}\PY{p}{]}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{get\PYZus{}numerical\PYZus{}gradient}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{idx1}\PY{p}{,} \PY{n}{idx2}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} We are mutating params[n][idx1, idx2] by eps and evaluate the difference.}
  \PY{n}{demo\PYZus{}params\PYZus{}eps} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{demo\PYZus{}params}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Copy.}
  \PY{n}{demo\PYZus{}params\PYZus{}eps}\PY{p}{[}\PY{n}{n}\PY{p}{]} \PY{o}{=} \PY{n}{demo\PYZus{}params\PYZus{}eps}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
  \PY{n}{demo\PYZus{}params\PYZus{}eps}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{[}\PY{n}{idx1}\PY{p}{,} \PY{n}{idx2}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{eps}
  \PY{n}{f\PYZus{}demo\PYZus{}eps} \PY{o}{=} \PY{n}{get\PYZus{}nn\PYZus{}classifier\PYZus{}func}\PY{p}{(}\PY{n}{demo\PYZus{}params\PYZus{}eps}\PY{p}{)}
  \PY{n}{f\PYZus{}demo\PYZus{}eps\PYZus{}val}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{f\PYZus{}demo\PYZus{}eps}\PY{p}{(}\PY{n}{demo\PYZus{}batch\PYZus{}features}\PY{p}{,}
                                    \PY{n}{demo\PYZus{}batch\PYZus{}labels}\PY{p}{,}
                                    \PY{n}{want\PYZus{}gradient}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}demo\PYZus{}eps\PYZus{}val:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{f\PYZus{}demo\PYZus{}eps\PYZus{}val}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vs.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{f\PYZus{}demo\PYZus{}val}\PY{p}{)}
  \PY{k}{return} \PY{p}{(}\PY{n}{f\PYZus{}demo\PYZus{}eps\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{f\PYZus{}demo\PYZus{}val}\PY{p}{)} \PY{o}{/} \PY{n}{eps}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{get\PYZus{}numerical\PYZus{}gradient}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
f\_demo\_val: 2.175762
[[[0.0019, 0.0026, -0.01194, 0.01522, -0.01891],
  [-0.00507, -0.00759, 0.03308, -0.04231, 0.05237],
  [0.06119, 0.09462, -0.40677, 0.5206, -0.64192]],
 [[0.02846, 0.01043, 0.087, 0.03194],
  [0.1313, 0.04752, 0.4027, 0.14725],
  [0.08044, 0.02917, 0.24657, 0.09022],
  [0.02027, 0.00746, 0.06195, 0.02275],
  [0.04768, 0.01734, 0.14611, 0.05349],
  [0.25897, 0.09375, 0.79419, 0.29044]],
 [[-0.14684, 0.14684],
  [0.26458, -0.26458],
  [0.27931, -0.27931],
  [0.01699, -0.01699],
  [0.89155, -0.89155]]]
\#\#\#
f\_demo\_eps\_val: 2.1757617947269394 vs. 2.175761813641268
-0.018914328769881195
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us get the MNIST training set...}

\PY{k+kn}{import} \PY{n+nn}{tensorflow\PYZus{}datasets} \PY{k}{as} \PY{n+nn}{tfds}

\PY{p}{(}\PY{n}{ds\PYZus{}train}\PY{p}{,} \PY{n}{ds\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{ds\PYZus{}info} \PY{o}{=} \PY{n}{tfds}\PY{o}{.}\PY{n}{load}\PY{p}{(}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{split}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{shuffle\PYZus{}files}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
    \PY{n}{as\PYZus{}supervised}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{with\PYZus{}info}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
\PY{p}{)}

\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{ds\PYZus{}train}\PY{o}{.}\PY{n}{as\PYZus{}numpy\PYZus{}iterator}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of MNIST training\PYZhy{}set examples:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} We build our training and test set from the MNIST \PYZdq{}training\PYZdq{} examples.}
\PY{c+c1}{\PYZsh{} (If we used the `test` examples in any way here, we would risk making}
\PY{c+c1}{\PYZsh{} decisions about our model based on official test set performance,}
\PY{c+c1}{\PYZsh{} hence end up \PYZdq{}training on the for\PYZhy{}evaluation test set\PYZdq{}. We must avoid this!)}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} ...but let us shuffle them randomly\PYZhy{}but\PYZhy{}reproducibly.}
\PY{n}{rng} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{rng}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{)}
\PY{n}{num\PYZus{}training} \PY{o}{=} \PY{l+m+mi}{4} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{5}  \PY{c+c1}{\PYZsh{} 80\PYZpc{} training examples.}
\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}training} \PY{o}{=} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}training}\PY{p}{]}
\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}ourtest} \PY{o}{=} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{[}\PY{n}{num\PYZus{}training}\PY{p}{:}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of examples in our training set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{num\PYZus{}training}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of MNIST training-set examples: 60000

\#\#\#\#\#\#

Number of examples in our training set: 48000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We need to map the labels to one\PYZhy{}hot form.}
\PY{n}{labels\PYZus{}1hot\PYZus{}all} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\PY{n}{labels\PYZus{}1hot\PYZus{}all}\PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                \PY{n}{numpy}\PY{o}{.}\PY{n}{fromiter}\PY{p}{(}
                    \PY{p}{(}\PY{n}{label} \PY{k}{for} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{)}\PY{p}{,}
                    \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{int32}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Labels}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{labels\PYZus{}1hot\PYZus{}all}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}

\PY{n}{labels\PYZus{}1hot\PYZus{}training} \PY{o}{=} \PY{n}{labels\PYZus{}1hot\PYZus{}all}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}training}\PY{p}{]}
\PY{n}{labels\PYZus{}1hot\PYZus{}ourtest} \PY{o}{=} \PY{n}{labels\PYZus{}1hot\PYZus{}all}\PY{p}{[}\PY{n}{num\PYZus{}training}\PY{p}{:}\PY{p}{]}

\PY{n}{img0\PYZus{}all} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}
    \PY{p}{[}\PY{p}{(}\PY{n}{img}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{255.0}\PY{p}{)} \PY{k}{for} \PY{n}{img}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{images\PYZus{}and\PYZus{}labels\PYZus{}all}\PY{p}{]}\PY{p}{,}
    \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{img0\PYZus{}training} \PY{o}{=} \PY{n}{img0\PYZus{}all}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}training}\PY{p}{]}
\PY{n}{img0\PYZus{}ourtest} \PY{o}{=} \PY{n}{img0\PYZus{}all}\PY{p}{[}\PY{n}{num\PYZus{}training}\PY{p}{:}\PY{p}{]}
\PY{c+c1}{\PYZsh{} We could imagine doing per\PYZhy{}pixel\PYZhy{}feature input normalization, but this}
\PY{c+c1}{\PYZsh{} would have some problems due to rarely activated pixels. Let us instead}
\PY{c+c1}{\PYZsh{} simply make the mean pixel activation zero and standard deviation 1.}
\PY{n}{img\PYZus{}offset} \PY{o}{=} \PY{n}{img0\PYZus{}training}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\PY{n}{img\PYZus{}stddev} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{img0\PYZus{}training}\PY{p}{)}
\PY{n}{img\PYZus{}training} \PY{o}{=} \PY{p}{(}\PY{n}{img0\PYZus{}training} \PY{o}{\PYZhy{}} \PY{n}{img\PYZus{}offset}\PY{p}{)} \PY{o}{/} \PY{n}{img\PYZus{}stddev}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{img\PYZus{}training}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img\PYZus{}training}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} We already shuffled our images. So, we can define batches by merely reshaping}
\PY{c+c1}{\PYZsh{} the arrays.}
\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{64}
\PY{n}{num\PYZus{}batches} \PY{o}{=} \PY{n}{num\PYZus{}training} \PY{o}{/}\PY{o}{/} \PY{n}{batch\PYZus{}size}
\PY{n}{num\PYZus{}training\PYZus{}used} \PY{o}{=} \PY{n}{num\PYZus{}batches} \PY{o}{*} \PY{n}{batch\PYZus{}size}
\PY{n}{batched\PYZus{}labels\PYZus{}1hot\PYZus{}training} \PY{o}{=} \PY{p}{(}
    \PY{n}{labels\PYZus{}1hot\PYZus{}training}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}training\PYZus{}used}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}
        \PY{n}{num\PYZus{}batches}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Our classifier is built in such a way that it reshapes every single}
\PY{c+c1}{\PYZsh{} example\PYZsq{}s features to a vector anyhow, so we may as well feed them}
\PY{c+c1}{\PYZsh{} in as 28x28.}
\PY{n}{batched\PYZus{}img\PYZus{}training} \PY{o}{=} \PY{n}{img\PYZus{}training}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}training\PYZus{}used}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}
    \PY{n}{num\PYZus{}batches}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Let us actually build a first blunt classifier.}
\PY{n}{rng1} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{model\PYZus{}params} \PY{o}{=} \PY{p}{[}
          \PY{c+c1}{\PYZsh{} First, we (nonlinearly) map pixels to a 25\PYZhy{}vector.}
          \PY{n}{rng1}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{28}\PY{o}{*}\PY{l+m+mi}{28}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,}
          \PY{c+c1}{\PYZsh{} Then, this 25\PYZhy{}vector to another 25\PYZhy{}vector.}
          \PY{n}{rng1}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{25}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
          \PY{c+c1}{\PYZsh{} Then, once more.}
          \PY{n}{rng1}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{25}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
          \PY{c+c1}{\PYZsh{} Then, to a 10\PYZhy{}vector for softmax.}
          \PY{n}{rng1}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{25}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}

\PY{n}{classifier\PYZus{}func} \PY{o}{=} \PY{n}{get\PYZus{}nn\PYZus{}classifier\PYZus{}func}\PY{p}{(}\PY{n}{model\PYZus{}params}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{train\PYZus{}one\PYZus{}epoch}\PY{p}{(}\PY{n}{rng}\PY{p}{,}
                    \PY{n}{params}\PY{p}{,}
                    \PY{n}{classifier\PYZus{}func}\PY{p}{,}
                    \PY{n}{batched\PYZus{}labels}\PY{p}{,}
                    \PY{n}{batched\PYZus{}imgs}\PY{p}{,}
                    \PY{n}{learning\PYZus{}rate}\PY{p}{,}
                    \PY{n}{print\PYZus{}every}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}\PY{p}{:}
  \PY{n}{num\PYZus{}batches} \PY{o}{=} \PY{n}{batched\PYZus{}labels}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{n}{batch\PYZus{}sequence} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}batches}\PY{p}{)}\PY{p}{)}
  \PY{n}{rng}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{batch\PYZus{}sequence}\PY{p}{)}
  \PY{k}{for} \PY{n}{num\PYZus{}batch}\PY{p}{,} \PY{n}{batch\PYZus{}index} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{batch\PYZus{}sequence}\PY{p}{)}\PY{p}{:}
    \PY{n}{loss}\PY{p}{,} \PY{n}{pred}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n}{classifier\PYZus{}func}\PY{p}{(}\PY{n}{batched\PYZus{}imgs}\PY{p}{[}\PY{n}{num\PYZus{}batch}\PY{p}{]}\PY{p}{,}
                                       \PY{n}{batched\PYZus{}labels}\PY{p}{[}\PY{n}{num\PYZus{}batch}\PY{p}{]}\PY{p}{,}
                                       \PY{n}{want\PYZus{}gradient}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{k}{if} \PY{n}{num\PYZus{}batch} \PY{o}{\PYZpc{}} \PY{n}{print\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Batch }\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}batch}\PY{l+s+si}{:}\PY{l+s+s1}{4d}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}batches}\PY{l+s+si}{:}\PY{l+s+s1}{4d}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{: Loss=}\PY{l+s+si}{\PYZob{}}\PY{n}{loss}\PY{l+s+si}{:}\PY{l+s+s1}{10.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{for} \PY{n}{p}\PY{p}{,} \PY{n}{delta\PYZus{}p} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{grad}\PY{p}{)}\PY{p}{:}
      \PY{n}{p} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{delta\PYZus{}p}

\PY{k}{for} \PY{n}{num\PYZus{}epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{num\PYZus{}epoch}\PY{p}{)}
  \PY{n}{train\PYZus{}one\PYZus{}epoch}\PY{p}{(}\PY{n}{rng1}\PY{p}{,}
                  \PY{n}{model\PYZus{}params}\PY{p}{,}
                  \PY{n}{classifier\PYZus{}func}\PY{p}{,}
                  \PY{n}{batched\PYZus{}labels\PYZus{}1hot\PYZus{}training}\PY{p}{,}
                  \PY{n}{batched\PYZus{}img\PYZus{}training}\PY{p}{,}
                  \PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} For a better classifier: Let\PYZsq{}s train for a bit longer.}
\PY{c+c1}{\PYZsh{} (Feel free to skip this.)}

\PY{k}{for} \PY{n}{num\PYZus{}epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{} Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{num\PYZus{}epoch}\PY{p}{)}
  \PY{n}{train\PYZus{}one\PYZus{}epoch}\PY{p}{(}\PY{n}{rng1}\PY{p}{,}
                  \PY{n}{model\PYZus{}params}\PY{p}{,}
                  \PY{n}{classifier\PYZus{}func}\PY{p}{,}
                  \PY{n}{batched\PYZus{}labels\PYZus{}1hot\PYZus{}training}\PY{p}{,}
                  \PY{n}{batched\PYZus{}img\PYZus{}training}\PY{p}{,}
                  \PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us try out our trained classifier on a few examples from our test set.}
\PY{n}{loss1}\PY{p}{,} \PY{n}{predictions1}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{classifier\PYZus{}func}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} Need to use the same normalization function.}
    \PY{p}{(}\PY{n}{img0\PYZus{}ourtest} \PY{o}{\PYZhy{}} \PY{n}{img\PYZus{}offset}\PY{p}{)} \PY{o}{/} \PY{n}{img\PYZus{}stddev}\PY{p}{,}
    \PY{n}{labels\PYZus{}1hot\PYZus{}ourtest}\PY{p}{)}

\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{predictions1}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{pprint}\PY{o}{.}\PY{n}{pprint}\PY{p}{(}\PY{n}{labels\PYZus{}1hot\PYZus{}ourtest}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}
    \PY{n+nb}{zip}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predictions1}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
        \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{labels\PYZus{}1hot\PYZus{}ourtest}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} After 500 training epochs, we get an accuracy of \PYZti{}94.37\PYZpc{}.}
\PY{c+c1}{\PYZsh{} Not too great, but somewhat better than what a linear model}
\PY{c+c1}{\PYZsh{} can hope to achieve here. After 1000 epochs, we see an}
\PY{c+c1}{\PYZsh{} accuracy of \PYZti{}95.10\PYZpc{} \PYZhy{} the model is still improving with more training.}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{[}\PY{n}{x} \PY{o}{==} \PY{n}{y} \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}
          \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predictions1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
          \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{labels\PYZus{}1hot\PYZus{}ourtest}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[0.97, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0],
 [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98, 0.0],
 [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.98],
 [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.96],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97, 0.0, 0.03],
 [0.15, 0.0, 0.05, 0.55, 0.04, 0.01, 0.02, 0.01, 0.16, 0.02],
 [0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
 [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.99, 0.0, 0.0, 0.0]]
[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
 [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
 [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
 [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]]
[(0, 0), (8, 8), (9, 3), (9, 9), (9, 9), (9, 9), (7, 7), (3, 0), (1, 1), (6, 6),
(6, 6), (3, 3), (4, 4), (0, 0), (4, 4), (1, 1), (6, 6), (0, 0), (9, 9), (6, 6)]
Accuracy: 0.94375
    \end{Verbatim}

    \hypertarget{summary}{%
\subsection{Summary}\label{summary}}

\begin{itemize}
\tightlist
\item
  We have a basic trainable `deep NN classifier' for handwritten digits.
\item
  This setup has many of the important generic elements of a
  fully-connected DNN approach. Some performance-improving tricks and
  tweaks are still missing, though. (We can explore quite a few of these
  via TensorFlow Playground.)
\item
  Here, we trained a model with more than 20k parameters to do something
  useful, using stochastic gradient descent.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total trainable model parameters:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{size} \PY{k}{for} \PY{n}{m} \PY{o+ow}{in} \PY{n}{model\PYZus{}params}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Total trainable model parameters: 21185
    \end{Verbatim}

    \hypertarget{a-closer-look}{%
\subsection{A Closer Look}\label{a-closer-look}}

Let us explore one specific detail about our current set-up. Suppose we
build a MNIST-classifier much like this, where we use even more hidden
layers and all make them same-with, also sticking with our
atan-nonlinearity. What do the gradients then actually look like?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{explore\PYZus{}gradients}\PY{p}{(}\PY{n}{num\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,}
                      \PY{n}{layer\PYZus{}width}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,}
                      \PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                      \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,}
                      \PY{n}{std\PYZus{}weights\PYZus{}layer0}\PY{o}{=}\PY{l+m+mf}{0.03}\PY{p}{,}
                      \PY{n}{std\PYZus{}biases\PYZus{}layer0}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,}
                      \PY{n}{std\PYZus{}weights}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                      \PY{n}{std\PYZus{}biases}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                      \PY{n}{std\PYZus{}weights\PYZus{}layer\PYZus{}last}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,}
                      \PY{n}{std\PYZus{}biases\PYZus{}layer\PYZus{}last}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,}
                      \PY{n}{num\PYZus{}batches}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
                      \PY{n}{rng}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
  \PY{k}{if} \PY{n}{rng} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
    \PY{n}{rng} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} The very first layer is a bit special \PYZhy{} here, we go from 28x28 down}
  \PY{c+c1}{\PYZsh{} to layer\PYZus{}width.}
  \PY{n}{model\PYZus{}params} \PY{o}{=} \PY{p}{[}
    \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{28}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{layer\PYZus{}width}\PY{p}{]}\PY{p}{,}
                                  \PY{n}{scale}\PY{o}{=}\PY{n}{std\PYZus{}weights\PYZus{}layer0}\PY{p}{)}\PY{p}{,}
                       \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{layer\PYZus{}width}\PY{p}{]}\PY{p}{,}
                                  \PY{n}{scale}\PY{o}{=}\PY{n}{std\PYZus{}biases\PYZus{}layer0}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                      \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}
  \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}layers}\PY{p}{)}\PY{p}{:}
    \PY{n}{model\PYZus{}params}\PY{o}{.}\PY{n}{append}\PY{p}{(}
        \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{[}\PY{n}{layer\PYZus{}width}\PY{p}{,} \PY{n}{layer\PYZus{}width}\PY{p}{]}\PY{p}{,}
                                      \PY{n}{scale}\PY{o}{=}\PY{n}{std\PYZus{}weights}\PY{p}{)}\PY{p}{,}
                           \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{layer\PYZus{}width}\PY{p}{]}\PY{p}{,}
                                      \PY{n}{scale}\PY{o}{=}\PY{n}{std\PYZus{}biases}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                          \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} The very last layer also is special. We go down to width\PYZhy{}10.}
  \PY{n}{model\PYZus{}params}\PY{o}{.}\PY{n}{append}\PY{p}{(}
         \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{[}\PY{n}{layer\PYZus{}width}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}
                                       \PY{n}{scale}\PY{o}{=}\PY{n}{std\PYZus{}weights}\PY{p}{)}\PY{p}{,}
                            \PY{n}{rng}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}
                                       \PY{n}{scale}\PY{o}{=}\PY{n}{std\PYZus{}biases}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                           \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
  \PY{n}{classifier\PYZus{}func} \PY{o}{=} \PY{n}{get\PYZus{}nn\PYZus{}classifier\PYZus{}func}\PY{p}{(}\PY{n}{model\PYZus{}params}\PY{p}{)}
  \PY{k}{for} \PY{n}{num\PYZus{}epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{num\PYZus{}epoch}\PY{p}{)}
    \PY{n}{train\PYZus{}one\PYZus{}epoch}\PY{p}{(}\PY{n}{rng}\PY{p}{,}
                    \PY{n}{model\PYZus{}params}\PY{p}{,}
                    \PY{n}{classifier\PYZus{}func}\PY{p}{,}
                    \PY{n}{batched\PYZus{}labels\PYZus{}1hot\PYZus{}training}\PY{p}{,}
                    \PY{n}{batched\PYZus{}img\PYZus{}training}\PY{p}{,}
                    \PY{n}{learning\PYZus{}rate}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Let us collect data for the first 10 training set batches.}
  \PY{n}{some\PYZus{}loss\PYZus{}probs\PYZus{}grad} \PY{o}{=} \PY{p}{[}\PY{n}{classifier\PYZus{}func}\PY{p}{(}\PY{n}{batched\PYZus{}img\PYZus{}training}\PY{p}{[}\PY{n}{num\PYZus{}batch}\PY{p}{]}\PY{p}{,}
                                          \PY{n}{batched\PYZus{}labels\PYZus{}1hot\PYZus{}training}\PY{p}{[}\PY{n}{num\PYZus{}batch}\PY{p}{]}\PY{p}{,}
                                          \PY{n}{want\PYZus{}gradient}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                          \PY{k}{for} \PY{n}{num\PYZus{}batch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}batches}\PY{p}{)}\PY{p}{]}
  \PY{c+c1}{\PYZsh{} We are interested in the per\PYZhy{}layer length\PYZhy{}of\PYZhy{}the\PYZhy{}gradient.}
  \PY{k}{def} \PY{n+nf}{gradient\PYZus{}lengths}\PY{p}{(}\PY{n}{loss\PYZus{}probs\PYZus{}grad}\PY{p}{)}\PY{p}{:}
    \PY{n}{loss}\PY{p}{,} \PY{n}{probs}\PY{p}{,} \PY{n}{grad\PYZus{}components} \PY{o}{=} \PY{n}{loss\PYZus{}probs\PYZus{}grad}
    \PY{k}{return} \PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{g}\PY{p}{)} \PY{k}{for} \PY{n}{g} \PY{o+ow}{in} \PY{n}{grad\PYZus{}components}\PY{p}{]}
  \PY{n}{per\PYZus{}batch\PYZus{}gradient\PYZus{}lengths} \PY{o}{=} \PY{p}{[}
    \PY{n}{gradient\PYZus{}lengths}\PY{p}{(}\PY{n}{l\PYZus{}p\PYZus{}g}\PY{p}{)} \PY{k}{for} \PY{n}{l\PYZus{}p\PYZus{}g} \PY{o+ow}{in} \PY{n}{some\PYZus{}loss\PYZus{}probs\PYZus{}grad}\PY{p}{]}
  \PY{c+c1}{\PYZsh{} Let us also return the classifier.}
  \PY{k}{return} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{per\PYZus{}batch\PYZus{}gradient\PYZus{}lengths}\PY{p}{)}\PY{p}{,} \PY{n}{classifier\PYZus{}func}

\PY{n}{grads1}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{explore\PYZus{}gradients}\PY{p}{(}\PY{n}{rng}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{grads2}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{explore\PYZus{}gradients}\PY{p}{(}\PY{n}{rng}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{std\PYZus{}weights}\PY{o}{=}\PY{l+m+mf}{0.09}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{grads1}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{grads2}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[0.01798 0.00512 0.00585 0.00722 0.01071]
 [0.02071 0.00673 0.00826 0.01179 0.01788]
 [0.02092 0.00658 0.00808 0.01062 0.01652]
 [0.0192  0.00607 0.0076  0.01018 0.01771]
 [0.0174  0.00566 0.00836 0.01009 0.01314]]
\#\#\#
[[0.00794 0.00252 0.00294 0.00398 0.00681]
 [0.00909 0.00328 0.00416 0.00656 0.01123]
 [0.0092  0.00321 0.00408 0.00587 0.01035]
 [0.00843 0.00296 0.00383 0.0056  0.01111]
 [0.00771 0.00281 0.00423 0.00541 0.00802]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We are interested in the lengths of the in\PYZhy{}between gradients, so,}
\PY{c+c1}{\PYZsh{} ignoring the first and final layer.}
\PY{c+c1}{\PYZsh{} Let us plot these (logarithmically).}
\PY{c+c1}{\PYZsh{} x\PYZhy{}coordinate = \PYZob{}number of weights\PYZhy{}layer\PYZcb{} + \PYZob{}number of example\PYZcb{} * 0.02,}
\PY{c+c1}{\PYZsh{} y\PYZus{}coordinate = log(gradient\PYZus{}length)}

\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
    \PY{p}{[}\PY{n}{num\PYZus{}layer} \PY{o}{+} \PY{n}{num\PYZus{}example} \PY{o}{*} \PY{l+m+mf}{0.02}
     \PY{k}{for} \PY{n}{num\PYZus{}layer} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{grads1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
     \PY{k}{for} \PY{n}{num\PYZus{}example} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{grads1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}
    \PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{grads1}\PY{p}{[}\PY{n}{num\PYZus{}example}\PY{p}{,} \PY{n}{num\PYZus{}layer}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
     \PY{k}{for} \PY{n}{num\PYZus{}layer} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{grads1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
     \PY{k}{for} \PY{n}{num\PYZus{}example} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{grads1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}
     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ok}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_03_MachineLearning_files/ML_03_MachineLearning_19_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This plot shows that, \PYZdq{}as we backprop towards earlier layers\PYZdq{},}
\PY{c+c1}{\PYZsh{} \PYZdq{}the length of the gradient decays\PYZdq{}, so}
\PY{c+c1}{\PYZsh{} \PYZdq{}late layers move a lot in training, but earlier layers only a little\PYZdq{}.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} Let us wrap this up and study this as a function of the weight\PYZhy{}}
\PY{c+c1}{\PYZsh{} and bias\PYZhy{}noise scaling.}

\PY{k}{def} \PY{n+nf}{plot\PYZus{}wb}\PY{p}{(}\PY{n}{seq\PYZus{}tag\PYZus{}std\PYZus{}weights\PYZus{}std\PYZus{}biases}\PY{p}{,} \PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{rng\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
  \PY{k}{for} \PY{n}{plot\PYZus{}tag}\PY{p}{,} \PY{n}{std\PYZus{}weights}\PY{p}{,} \PY{n}{std\PYZus{}biases} \PY{o+ow}{in} \PY{n}{seq\PYZus{}tag\PYZus{}std\PYZus{}weights\PYZus{}std\PYZus{}biases}\PY{p}{:}
    \PY{n}{grads}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{explore\PYZus{}gradients}\PY{p}{(}\PY{n}{rng}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{rng\PYZus{}seed}\PY{p}{)}\PY{p}{,}
                                \PY{n}{std\PYZus{}weights}\PY{o}{=}\PY{n}{std\PYZus{}weights}\PY{p}{,}
                                \PY{n}{std\PYZus{}biases}\PY{o}{=}\PY{n}{std\PYZus{}biases}\PY{p}{,}
                                \PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{o}{=}\PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
      \PY{p}{[}\PY{n}{num\PYZus{}layer} \PY{o}{+} \PY{n}{num\PYZus{}example} \PY{o}{*} \PY{l+m+mf}{0.02}
       \PY{k}{for} \PY{n}{num\PYZus{}layer} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{grads}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
       \PY{k}{for} \PY{n}{num\PYZus{}example} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{grads}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{n}{num\PYZus{}example}\PY{p}{,} \PY{n}{num\PYZus{}layer}\PY{p}{]}\PY{p}{)}
       \PY{k}{for} \PY{n}{num\PYZus{}layer} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{grads}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
       \PY{k}{for} \PY{n}{num\PYZus{}example} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{grads}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}
       \PY{n}{plot\PYZus{}tag}\PY{p}{)}
  \PY{n}{pyplot}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
  \PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}


\PY{n}{plot\PYZus{}wb}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ok}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ob}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{or}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.03}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{og}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{]}\PY{p}{)}

\PY{n}{plot\PYZus{}wb}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ob}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{or}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{og}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.15}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.20}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ok}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.25}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{]}\PY{p}{)}

\PY{n}{plot\PYZus{}wb}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{or}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.20}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ok}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_03_MachineLearning_files/ML_03_MachineLearning_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_03_MachineLearning_files/ML_03_MachineLearning_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_03_MachineLearning_files/ML_03_MachineLearning_20_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us also see what happens after a few epochs.}

\PY{n}{plot\PYZus{}wb}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{or}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{0.20}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ok}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
         \PY{p}{]}\PY{p}{,} \PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    How to interpret this last plot?

For a large amount if initial-weight noise (black), the different
batches (all of size 64) we use to estimate the model-gradient produce
gradients whose lenghts, for the first hidden layer, are ``all over the
place'' (lengths from about from exp(-0.8) to about exp(2.7) or so). We
introduced batches to estimate a ``roughly OK'' low-effort
useful-for-training approximation to the model's gradient on the entire
training set.

Seeing that gradient-lengths differ by a factor of 30 or so across
batches makes the idea that these per-batch gradients would do anything
useful look doubtful. It is plausible that we are merely adding noise at
this layer in every training step.

Correspondingly, if initial-weight noise is initially very small, we get
small activations which take is into the mostly-linear region of our
\({\rm tan}^{-1}\) nonlinearity. Every such small-initial-weights layer
then ``also scales down overall activation by some factor'', and it is
no surprise that gradient-lengths decay as we backpropagate further away
from the result and towards earlier layers.

So, it would be a mistake to assume that ``we can start this with a tiny
amount of noise, merely to break the symmetry'', and then hope for
backpropagation to adjust weights to get us something useful even with
many layers - remote-from-the-loss layers will move only very little due
to this gradient-decay.

Noting that in a fully-connected network with many nodes per layer,
``every node is a neighbor of every node from the next layer'', i.e.~we
have a degree of neighbor-connectivity that is higher than what we could
get from neighborhood in a 4d embedding, we may wonder whether this
situation would be accessible to Mean Field Theory. This has been done
in
\href{https://arxiv.org/abs/1611.01232}{https://arxiv.org/abs/1611.01232
- ``Deep Information Propagation''} \cite{schoenholz2016deep} -- a (in my view) seminal but wildly
under-appreciated paper which likely has not received as much attention
as it deserves since most members of the ML community unfortunately do
not have a strong background in statistical mechanics. (It should
however be noted that there would be more to say about the
infinite-width limit of neural networks than what is discussed in that
paper.)

A key insight here is that in order to successfully train a deep
architecture, we have to pay close attention to how random weight / bias
initialization is done.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

If all this is actually true, then we should be able to train this
``mini monster'' architecture we have here into a good-quality
classifier by using an informed choice of the inner-layer weight
standard deviation. If we do this, we still have not really tuned
initialization of random parameters for the very first and last layer,
but perhaps what we have is good enough\ldots?

We still might have to do some exploration around ``finding a good
learning rate'', though. (I did a tiny bit of manual tweaking to find a
good choice. A useful recipe is to do half-order-of-magnitude
adjustments to find the sweet spot between ``loss barely moves at all''
and ``loss often increases rather than decreasing''.)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{grads\PYZus{}x}\PY{p}{,} \PY{n}{classifier\PYZus{}func\PYZus{}x} \PY{o}{=} \PY{n}{explore\PYZus{}gradients}\PY{p}{(}
    \PY{n}{rng}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}
    \PY{n}{std\PYZus{}weights}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
    \PY{n}{std\PYZus{}biases}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
    \PY{n}{num\PYZus{}training\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,}
    \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{3e\PYZhy{}4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{loss\PYZus{}x}\PY{p}{,} \PY{n}{predictions\PYZus{}x}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{classifier\PYZus{}func\PYZus{}x}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} Need to use the same normalization function.}
    \PY{p}{(}\PY{n}{img0\PYZus{}ourtest} \PY{o}{\PYZhy{}} \PY{n}{img\PYZus{}offset}\PY{p}{)} \PY{o}{/} \PY{n}{img\PYZus{}stddev}\PY{p}{,}
    \PY{n}{labels\PYZus{}1hot\PYZus{}ourtest}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{[}\PY{n}{x} \PY{o}{==} \PY{n}{y} \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}
          \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predictions\PYZus{}x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
          \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{labels\PYZus{}1hot\PYZus{}ourtest}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Prints:}
\PY{c+c1}{\PYZsh{} Accuracy: 0.95725}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.95725
    \end{Verbatim}

    So, with just a few minutes of training time on a CPU, we got somewhat
reasonable performance (accuracy 95.7\%) out of a
8-stacked-nonlinearities (not counting softmax at the end) ``deep''
neural network by showing to it a collection of 48000 training examples
200 times over.

And we did it all without using any ``ML framework''. We merely used
NumPy in a way we might just as well have used Matlab or Octave instead.
If we wanted to, we could translate everything we saw here straightaway
to C code. None of the computational steps involved here use any
sophisticated processing or algorithms.

Naturally, this is of course still quite far from state-of-the-art. Even
pre-DL revolution, error rates well below 1\% were achievable for this
specific task. Nevertheless, quite a few of the main ideas that allow us
to perform even better can be discussed in simpler settings where we do
not even have to look at code.

Specifically, these notions are readily explorable via the
\url{https://playground.tensorflow.org/} web page:

\begin{itemize}
\tightlist
\item
  ReLU activation function.
\item
  L1 and L2 regularization.
\item
  Tweaking ``learning rate''.
\end{itemize}

In our toy classifier, we used \({\rm tan}^{-1}\) as our nonlinearity.
There are quite a few other choices which give about-similar
performance. Does the activation function matter? ``Yes and No'' - in
the sense that problematic properties of some particular choice of
activation function can often be fixed by other ``compensating'' design
decisions.

Historically, the logistic (``sigmoid'') function was popolar in the
early neural-networks literature. Here, we have been using a choice that
``behaves symmetrically around zero''.

It came to quite a surprise that, relative to these choices, the ReLU
function (``rectified linear unit''), with its very simple (and scale
invariant!) behavior often performs quite a bit better. This is the
function \(x \mapsto (x+|x|)/2\). There have been various tweaks to
this, such as leaky-relu, and another refinement to this idea was ELU
(replacing the negative side with an exponential).

A particularly interesting choice has been presented in
\href{https://arxiv.org/abs/1706.02515v5}{https://arxiv.org/abs/1706.02515v5
- Self-normalizing neural networks} \cite{klambauer2017self}: This choice tries to ``by design''
(under reasonable assumptions) drive activations towards a ``zero mean,
unit variance'' fixed point. However, there is a lot of literature of
the ``This paper demonstrates that my nonlinearity performs better than
yours'' type. Pragmatically, ReLU still is a good default choice for a
first shot at getting useful results.

    \hypertarget{addendum-thoughts-on-universal-approximation-theorems}{%
\subsection{Addendum: Thoughts on universal approximation
theorems}\label{addendum-thoughts-on-universal-approximation-theorems}}

(This was not covered on the IMPRS course given at the Albert Einstein
Institute in 2022, and expresses some views of the present author that
may not have the status of being consensus in the ML community.)

With neural networks, two obvious questions are: ``Is some particular
aspect of interest `learnable' from the set of examples we have
available'' (it may well be that we would need many more examples to be
sufficiently dense in space to make the aspect we are interested in
approximable by our model), and ``does our chosen architecture perhaps
not permit learning some functions''? Clearly, a linear model would fail
to learn the behavior of an exclusive-or logic gate. In general, we have
``\href{https://en.wikipedia.org/wiki/Universal_approximation_theorem}{Universal
Approximation Theorems}'' that basically state that for common popular
Deep Neural Network architectures, they can learn to approximate every
``reasonable'' function, given sufficient capacity. (Clearly, we would
however not expect any finite-size model with a simple architecture to
be able to train on examples for \texttt{label\ =\ sin(1/feature)} in
such a way that it would produce a main characteristic of this relation
- infinitely many zeroes.) With ReLU units, this is quite intuitive - as
a function of the activations that get summed and then put into a ReLU
unit, this unit is linear on one side of a hyperplane and zero on the
other. This means two things: First, for any given point not on any
hyperplane where a ReLU starts to activate (which are finitely many, so
this is a set of measure zero), the output is linear in the inputs. In a
sense, a 100-layers deep ReLU network, when observed locally,
``collapses to a 1-layer linear model''. Second, by summing such ReLU
functions - and taking ReLU outputs as inputs of other ReLU units, we
effectively partition up feature-space in a mosaic of polyhedral cells
(some of which will extend to infinity). Clearly, given sufficient
capacity, every function that we can think of as being in some sense
approximable by a continuous function that is piecewise-affine on mosaic
cells is then ``learnable'' by a ReLU network with sufficient capacity.

This however does not mean that it would not make sense to look deeper
into the general structure of data-processing inside neural networks.
One interesting aspect is that we are only discussing one-dimensional
nonlinearities, so there is a hidden claim here that every
higher-dimensional non-linear function can be approximated by only
one-dimensional nonlinearities and addition as the only binary operation
(i.e.~operation which ``combines'' data). This is true, and commonly
known as the
\href{https://en.wikipedia.org/wiki/Kolmogorov\%E2\%80\%93Arnold_representation_theorem}{Kolmogorov-Arnold
Representation Theorem} (which in itself has an interesting back story
and relation to Hilbert's 13th problem). There is however a problem
which is especially relevant in physics-related applications where
problems often have some interesting symmetry (such as: rotational
symmetry).

The issue is best illustrated by example: Suppose we wanted to learn
computing the determinant of a real \(5\times 5\)-matrix. This is a
conceptually rather simple mapping from 25 real input parameters (matrix
elements) to a 1-dimensional output (the determinant) - clearly, we can
write this as a 5th-degree homogeneous polynomial with \(5!=120\)
summands. How would this be learned? Ultimately, this would involve
learning to multiply two numbers (in many places), but since we only
ever combine different internal activations by adding them,
multiplication of two positive numbers would have to be synthesized from
a learned approximation to a logarithm, learned independently for both
inputs, then summing, followed by a learned approximation to an
exponential. For signed quantities, this then becomes even more
complicated. Intuitively, it is clear that ``learning to compute a
determinant'' will be possible in principle, but one would also expect
that having to learn all these - from the perspective of a neural
network ``unnatural'' - multiplication operations can easily make the
size of the training set that would be required to properly learn
computing a determinant (and hence, also training time) prohibitively
large. The determinant is of course only one example of a polynomial
geometric invariant, and the story would be similar for many a problem
where we would expect learning to be able to ``discover'' that some
derived quantity such as - say - Mandelstam variables might be useful to
predict an outcome. One possible approach would be to design
multiplication into the architecture, as is done with Sigma-Pi networks.
Overall, these are one of many ``fringe architectures'' that can be
employed to address very specific problems. A perhaps even more fringe
(but, in the present author's view, useful) idea is to take this a step
further and try to use an architecture which like a Sigma-Pi network
already has an inherent concept of multiplication, but avoids much of
the need for architecture-search - simply by going from a
one-dimensional nonlinearity to using matrix exponentiation as a
nonlinearity - our paper
\href{https://arxiv.org/abs/2008.03936}{https://arxiv.org/abs/2008.03936
- Intelligent Matrix Exponentiation} \cite{fischbacher2020intelligent} characterizes properties of this
nonlinearity (and in particular shows that this is indeed able to learn
determinants, unlike conventional architectures - as participants are
invited to verify by trying), but deliberately not going as deep into
Lie group theory as one could do here, given the intended ML audience.


    % Add a bibliography block to the postdoc
    
    
    

    
    
    
    

    
    \hypertarget{machine-learning-with-tensorflow}{%
\section{Machine Learning with
TensorFlow}\label{machine-learning-with-tensorflow}}

\begin{itemize}
\item
  In the previous unit, we have seen how to build a DNN classifier
  ``from scratch''.
\item
  Knowing how to do this is knowledge at the level of ``knowing how to
  derive the Euler-Lagrange equations'':

  We may well find ourselves in some unusual situation where we have to
  ``go back to first principles'' to reason something out.
\item
  Much of current ML work (model-defining, model-training) is
  framework-based, but there is both a blessing and a curse here:

  \begin{itemize}
  \tightlist
  \item
    Frameworks cover many common cases.
  \item
    They also may limit our perspective to the framework's narrow
    field-of-view.
  \item
    Interesting work remains to be done that will require extending
    frameworks, or even ``working outside existing frameworks since we
    have to break some rules''.
  \end{itemize}
\item
  Here, we will mostly focus on Google's TensorFlow library. Later, we
  will also look a bit into JAX.

  \begin{itemize}
  \tightlist
  \item
    TensorFlow is a well-supported evolving (but somewhat
    under-documented) library that is popular for ML, and has been
    designed for ML, but can be used for many other tasks as well. ``A
    frigate''.
  \item
    JAX is a smaller and more experimental project that ``tries to take
    the good bits and pieces from TensorFlow'', such as ``fast
    gradients'' and ``accelerated linear algebra''. ``A skiff''.
  \end{itemize}
\end{itemize}

    \hypertarget{tensorflow-for-the-ml-practitioner}{%
\subsection{TensorFlow for the ML
Practitioner}\label{tensorflow-for-the-ml-practitioner}}

Suppose we wanted to redo what we did by hand using the infrastructure
provided by TensorFlow. Here is an ``easy sailing'' version.

First, let us train a model (we will come back to discussing all the
nice things that we are getting for free here thanks to TensorFlow).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We will need this PyPI module later}
\PY{err}{!}\PY{n}{pip} \PY{n}{install} \PY{n}{tf2onnx}


\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{import} \PY{n+nn}{time}

\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\PY{k+kn}{import} \PY{n+nn}{tensorflow\PYZus{}datasets} \PY{k}{as} \PY{n+nn}{tfds}


\PY{c+c1}{\PYZsh{} Loading the training and test set. We split the training\PYZhy{}set into \PYZsq{}training\PYZsq{},}
\PY{c+c1}{\PYZsh{} \PYZsq{}validation\PYZsq{}, and \PYZsq{}extra\PYZsq{} for ad\PYZhy{}hoc purposes.}
\PY{k}{def} \PY{n+nf}{get\PYZus{}training\PYZus{}datasets}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{p}{(}\PY{n}{ds\PYZus{}train\PYZus{}raw}\PY{p}{,}
   \PY{n}{ds\PYZus{}validation\PYZus{}raw}\PY{p}{,}
   \PY{n}{ds\PYZus{}extra\PYZus{}raw}\PY{p}{,}
   \PY{n}{ds\PYZus{}test\PYZus{}raw}\PY{p}{)}\PY{p}{,} \PY{n}{ds\PYZus{}info} \PY{o}{=} \PY{n}{tfds}\PY{o}{.}\PY{n}{load}\PY{p}{(}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{split}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train[:75}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train[75}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{:99}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train[99}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{:]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
      \PY{n}{shuffle\PYZus{}files}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
      \PY{n}{as\PYZus{}supervised}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
      \PY{n}{with\PYZus{}info}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
  \PY{n}{total\PYZus{}num\PYZus{}examples} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{s}\PY{o}{.}\PY{n}{num\PYZus{}examples} \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{ds\PYZus{}info}\PY{o}{.}\PY{n}{splits}\PY{o}{.}\PY{n}{values}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{def} \PY{n+nf}{normalize\PYZus{}image}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{label}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Normalizes images.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{cast}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{255.}\PY{p}{,} \PY{n}{label}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{def} \PY{n+nf}{transform}\PY{p}{(}\PY{n}{ds}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}\PY{n}{ds}
            \PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{normalize\PYZus{}image}\PY{p}{,} \PY{n}{num\PYZus{}parallel\PYZus{}calls}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}
            \PY{o}{.}\PY{n}{cache}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Shuffle buffer size needs to be at least as large as the number}
            \PY{c+c1}{\PYZsh{} of examples \PYZhy{} but does not matter much otherwise.}
            \PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{total\PYZus{}num\PYZus{}examples}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}
            \PY{o}{.}\PY{n}{prefetch}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{return} \PY{p}{(}\PY{n}{transform}\PY{p}{(}\PY{n}{ds\PYZus{}train\PYZus{}raw}\PY{p}{)}\PY{p}{,}
          \PY{n}{transform}\PY{p}{(}\PY{n}{ds\PYZus{}validation\PYZus{}raw}\PY{p}{)}\PY{p}{,}
          \PY{n}{transform}\PY{p}{(}\PY{n}{ds\PYZus{}extra\PYZus{}raw}\PY{p}{)}\PY{p}{,}
          \PY{n}{transform}\PY{p}{(}\PY{n}{ds\PYZus{}test\PYZus{}raw}\PY{p}{)}\PY{p}{)}


\PY{n}{ds\PYZus{}train}\PY{p}{,} \PY{n}{ds\PYZus{}validation}\PY{p}{,} \PY{n}{ds\PYZus{}extra}\PY{p}{,} \PY{n}{ds\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}training\PYZus{}datasets}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Training and test sets are iterators.}
\PY{c+c1}{\PYZsh{} For later, we save part of the `ds\PYZus{}extra` dataset to the filesystem.}

\PY{c+c1}{\PYZsh{} The os.access() check makes this cell idempotent.}
\PY{k}{if} \PY{n}{os}\PY{o}{.}\PY{n}{access}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training\PYZus{}examples.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{os}\PY{o}{.}\PY{n}{R\PYZus{}OK}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Small sample dataset already was saved to filesystem.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
  \PY{n}{sample\PYZus{}batches} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{ds\PYZus{}extra}\PY{o}{.}\PY{n}{take}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{o}{.}\PY{n}{as\PYZus{}numpy\PYZus{}iterator}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{n}{sample\PYZus{}batches\PYZus{}images} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}
    \PY{p}{[}\PY{n}{image\PYZus{}data} \PY{k}{for} \PY{n}{image\PYZus{}data}\PY{p}{,} \PY{n}{labels} \PY{o+ow}{in} \PY{n}{sample\PYZus{}batches}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
  \PY{n}{sample\PYZus{}batches\PYZus{}labels} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}
    \PY{p}{[}\PY{n}{labels} \PY{k}{for} \PY{n}{image\PYZus{}data}\PY{p}{,} \PY{n}{labels} \PY{o+ow}{in} \PY{n}{sample\PYZus{}batches}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} `sample\PYZus{}batches\PYZus{}images` is a `[num\PYZus{}batches, 32, 28, 28, 1]`\PYZhy{}array:}
  \PY{c+c1}{\PYZsh{} `num\PYZus{}batches` batches of 32 images each which are 28x28 with one}
  \PY{c+c1}{\PYZsh{}  color\PYZhy{}channel.}
  \PY{c+c1}{\PYZsh{} `training\PYZus{}batches\PYZus{}labels` is a `[num\PYZus{}batches, 32]`\PYZhy{}array:}
  \PY{c+c1}{\PYZsh{}  One label per batch per image in the batch.}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shapes:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sample\PYZus{}batches\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{sample\PYZus{}batches\PYZus{}labels}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Labels:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sample\PYZus{}batches\PYZus{}labels}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{n}{numpy}\PY{o}{.}\PY{n}{savez\PYZus{}compressed}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training\PYZus{}examples.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                         \PY{n}{images}\PY{o}{=}\PY{n}{sample\PYZus{}batches\PYZus{}images}\PY{p}{,}
                         \PY{n}{labels}\PY{o}{=}\PY{n}{sample\PYZus{}batches\PYZus{}labels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Shapes: (10, 32, 28, 28, 1) (10, 32)
Labels:
[[6 2 3 1 6 9 3 6 1 2 5 4 2 1 5 8 5 7 6 9 3 8 6 4 1 5 9 8 3 2 6 9]
 [7 2 9 9 9 5 4 4 0 8 2 8 7 4 1 2 6 8 3 4 5 8 0 5 2 8 9 7 5 8 5 7]
 [3 9 6 9 1 3 2 3 0 7 7 2 4 6 1 7 4 3 3 9 4 1 1 2 1 4 6 2 2 8 6 4]
 [2 8 4 3 3 5 6 4 1 1 1 5 7 7 5 7 4 5 1 5 7 3 0 1 1 2 8 8 5 1 7 1]
 [0 1 3 3 5 6 0 3 9 1 7 0 7 3 1 9 4 5 5 8 8 6 1 7 3 7 2 6 7 1 7 3]
 [7 9 0 8 8 7 4 3 6 5 8 8 9 8 1 7 3 4 1 9 5 7 8 1 9 4 0 7 2 3 4 5]
 [2 0 7 2 8 6 2 3 6 1 9 2 7 4 4 8 4 1 5 2 7 2 0 8 8 3 9 3 3 0 3 4]
 [2 3 8 5 0 1 6 5 5 0 8 5 9 9 4 8 8 1 4 4 9 8 4 4 6 5 3 0 8 8 1 7]
 [6 0 1 6 8 2 3 9 4 8 1 1 0 4 4 2 3 4 1 0 5 1 9 0 0 6 2 7 5 2 7 9]
 [1 8 4 9 2 1 9 1 0 8 0 1 4 4 7 5 3 8 2 9 9 4 1 6 2 5 8 1 1 3 7 3]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Training a model.}

\PY{n}{model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
  \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}\PY{p}{,}
  \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
  \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
  \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
  \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
  \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Participant Exercise: Try out other architectures, such as...:}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} model = tf.keras.models.Sequential([}
\PY{c+c1}{\PYZsh{}   tf.keras.layers.Flatten(input\PYZus{}shape=(28, 28)),}
\PY{c+c1}{\PYZsh{}   tf.keras.layers.Dense(80, activation=\PYZsq{}relu\PYZsq{}),}
\PY{c+c1}{\PYZsh{}   tf.keras.layers.Dense(80, activation=\PYZsq{}relu\PYZsq{}),}
\PY{c+c1}{\PYZsh{}   tf.keras.layers.Dense(80, activation=\PYZsq{}relu\PYZsq{}),}
\PY{c+c1}{\PYZsh{}   tf.keras.layers.Dense(10)}
\PY{c+c1}{\PYZsh{} ])}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} How simple can we make this and still get \PYZgt{}95\PYZpc{} accuracy?}


\PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}\PY{p}{,}
    \PY{n}{loss}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{SparseCategoricalCrossentropy}\PY{p}{(}\PY{n}{from\PYZus{}logits}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{metrics}\PY{o}{.}\PY{n}{SparseCategoricalAccuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,}
\PY{p}{)}

\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{n}{ds\PYZus{}train}\PY{p}{,}
    \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}
    \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{ds\PYZus{}validation}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/10
1407/1407 [==============================] - 33s 10ms/step - loss: 0.7655 -
sparse\_categorical\_accuracy: 0.8082 - val\_loss: 0.4042 -
val\_sparse\_categorical\_accuracy: 0.8922
Epoch 2/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.3325 -
sparse\_categorical\_accuracy: 0.9089 - val\_loss: 0.3063 -
val\_sparse\_categorical\_accuracy: 0.9135
Epoch 3/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.2651 -
sparse\_categorical\_accuracy: 0.9238 - val\_loss: 0.2643 -
val\_sparse\_categorical\_accuracy: 0.9244
Epoch 4/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.2270 -
sparse\_categorical\_accuracy: 0.9344 - val\_loss: 0.2390 -
val\_sparse\_categorical\_accuracy: 0.9310
Epoch 5/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.1996 -
sparse\_categorical\_accuracy: 0.9425 - val\_loss: 0.2149 -
val\_sparse\_categorical\_accuracy: 0.9373
Epoch 6/10
1407/1407 [==============================] - 7s 5ms/step - loss: 0.1778 -
sparse\_categorical\_accuracy: 0.9479 - val\_loss: 0.1988 -
val\_sparse\_categorical\_accuracy: 0.9422
Epoch 7/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.1597 -
sparse\_categorical\_accuracy: 0.9541 - val\_loss: 0.1862 -
val\_sparse\_categorical\_accuracy: 0.9474
Epoch 8/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.1447 -
sparse\_categorical\_accuracy: 0.9580 - val\_loss: 0.1788 -
val\_sparse\_categorical\_accuracy: 0.9469
Epoch 9/10
1407/1407 [==============================] - 7s 5ms/step - loss: 0.1326 -
sparse\_categorical\_accuracy: 0.9613 - val\_loss: 0.1684 -
val\_sparse\_categorical\_accuracy: 0.9513
Epoch 10/10
1407/1407 [==============================] - 6s 4ms/step - loss: 0.1218 -
sparse\_categorical\_accuracy: 0.9649 - val\_loss: 0.1631 -
val\_sparse\_categorical\_accuracy: 0.9525
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<keras.callbacks.History at 0x7dd008367700>
\end{Verbatim}
\end{tcolorbox}
        
    We now have a trained model. Perhaps not one with quite state-of-the-art
performance, but at least a model that clearly seems to know something
about the task it was being built for.

Let us see how to: * Get information about the trained model. * Save it
to a file and load it again. * Actually use it to make predictions.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Saving to a `*.h5` file will make tf\PYZhy{}Keras use the HDF5 format}
\PY{c+c1}{\PYZsh{} for the saved model.}
\PY{n}{model}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mnist\PYZus{}model.h5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{err}{!}\PY{n}{ls} \PY{o}{\PYZhy{}}\PY{n}{lah} \PY{n}{mnist\PYZus{}model}\PY{o}{*}

\PY{n}{reloaded\PYZus{}model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{load\PYZus{}model}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mnist\PYZus{}model.h5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Let us use one batch of examples we extracted earlier:}
\PY{n}{examples\PYZus{}images}\PY{p}{,} \PY{n}{examples\PYZus{}labels} \PY{o}{=} \PY{p}{(}
    \PY{n}{sample\PYZus{}batches\PYZus{}images}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{,} \PY{n}{sample\PYZus{}batches\PYZus{}labels}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
  \PY{k}{if} \PY{n}{image}\PY{o}{.}\PY{n}{size} \PY{o}{!=} \PY{l+m+mi}{28}\PY{o}{*}\PY{l+m+mi}{28}\PY{p}{:}
    \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Expecting input data to provide 28x28 pixels.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{logits} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{image}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{n}{verbose}\PY{p}{)}
  \PY{k}{if} \PY{n}{verbose}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Logits:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{logits}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{logits}\PY{p}{)}

\PY{c+c1}{\PYZsh{} We could run predictions on an entire batch, but here process}
\PY{c+c1}{\PYZsh{} individual images.}
\PY{k}{for} \PY{n}{num\PYZus{}example}\PY{p}{,} \PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{label}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{examples\PYZus{}images}\PY{p}{,}
                                                 \PY{n}{examples\PYZus{}labels}\PY{p}{)}\PY{p}{)}\PY{p}{:}
  \PY{n}{predicted} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Feel free to set verbose=True}
  \PY{n}{ok} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{OK}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{predicted} \PY{o}{==} \PY{n}{label} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BAD}\PY{l+s+s1}{\PYZsq{}}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Example Image }\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}example}\PY{l+s+si}{:}\PY{l+s+s1}{2d}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{: }\PY{l+s+s1}{\PYZsq{}}
        \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted=}\PY{l+s+si}{\PYZob{}}\PY{n}{predicted}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, actual=}\PY{l+s+si}{\PYZob{}}\PY{n}{label}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{  \PYZhy{} }\PY{l+s+si}{\PYZob{}}\PY{n}{ok}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Model: "sequential"
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
 Layer (type)                Output Shape              Param \#
=================================================================
 flatten (Flatten)           (None, 784)               0

 dense (Dense)               (None, 50)                39250

 dense\_1 (Dense)             (None, 50)                2550

 dense\_2 (Dense)             (None, 50)                2550

 dense\_3 (Dense)             (None, 50)                2550

 dense\_4 (Dense)             (None, 10)                510

=================================================================
Total params: 47,410
Trainable params: 47,410
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\#\#\#\#\#\#
-rw-r--r-- 1 root root 605K Aug 22 11:34 mnist\_model.h5
Example Image  0: predicted=6, actual=6  - OK
Example Image  1: predicted=2, actual=2  - OK
Example Image  2: predicted=3, actual=3  - OK
Example Image  3: predicted=1, actual=1  - OK
Example Image  4: predicted=6, actual=6  - OK
Example Image  5: predicted=9, actual=9  - OK
Example Image  6: predicted=3, actual=3  - OK
Example Image  7: predicted=6, actual=6  - OK
Example Image  8: predicted=1, actual=1  - OK
Example Image  9: predicted=7, actual=2  - BAD
Example Image 10: predicted=5, actual=5  - OK
Example Image 11: predicted=4, actual=4  - OK
Example Image 12: predicted=2, actual=2  - OK
Example Image 13: predicted=1, actual=1  - OK
Example Image 14: predicted=5, actual=5  - OK
Example Image 15: predicted=8, actual=8  - OK
Example Image 16: predicted=5, actual=5  - OK
Example Image 17: predicted=7, actual=7  - OK
Example Image 18: predicted=6, actual=6  - OK
Example Image 19: predicted=7, actual=9  - BAD
Example Image 20: predicted=3, actual=3  - OK
Example Image 21: predicted=8, actual=8  - OK
Example Image 22: predicted=6, actual=6  - OK
Example Image 23: predicted=4, actual=4  - OK
Example Image 24: predicted=1, actual=1  - OK
Example Image 25: predicted=5, actual=5  - OK
Example Image 26: predicted=9, actual=9  - OK
Example Image 27: predicted=8, actual=8  - OK
Example Image 28: predicted=3, actual=3  - OK
Example Image 29: predicted=2, actual=2  - OK
Example Image 30: predicted=6, actual=6  - OK
Example Image 31: predicted=9, actual=9  - OK
    \end{Verbatim}

    While we are at it: A `trained ML model' is a bit like an `electronics
module': We would typically like to know how to use this as a component
in some larger engineering design (/ product).

It is quite possible to use trained TensorFlow models on smartphones,
advanced microcontrollers, or merely make them part of some compiled
application. Often, a good approach is to convert the model to
``TensorFlow Lite'' form for deployment.

There are TFLite libraries for various systems/architectures to then
load a model, feed input to it, and obtain predictions from it. These
exist for: microcontrollers, tiny computers such as the Raspberry Pi,
Android apps, iOS apps, compiled binaries, etc.

Here, we will just sketch how this looks like using again Python-TFLite
- so, we are not quite cutting the Python umbilical cord yet. We will
first save our model in TFLite form, and then switch over from using
TensorFlow to using only the TFLite module.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Converting the model to TFLite (\PYZdq{}TensorFlow Lite\PYZdq{}) form.}

\PY{n}{model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{load\PYZus{}model}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mnist\PYZus{}model.h5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{converter} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{lite}\PY{o}{.}\PY{n}{TFLiteConverter}\PY{o}{.}\PY{n}{from\PYZus{}keras\PYZus{}model}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\PY{n}{tflite\PYZus{}model} \PY{o}{=} \PY{n}{converter}\PY{o}{.}\PY{n}{convert}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Save the model.}
\PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
  \PY{n}{f}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{tflite\PYZus{}model}\PY{p}{)}

\PY{err}{!}\PY{n}{ls} \PY{o}{\PYZhy{}}\PY{n}{lah} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
WARNING:absl:Found untraced functions such as \_update\_step\_xla while saving
(showing 1 of 1). These functions will not be directly callable after loading.
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
-rw-r--r-- 1 root root 189K Aug 22 11:34 mnist\_model.tflite
    \end{Verbatim}

    For later, we can also convert the trained model to \texttt{.onnx}
format, which is understood by other projects as a representation of a
computational graph. For graphs that do not use too exotic computational
operations, this should generally work. At the time of this writing, we
cannot convert from a model saved in HDF5 format, so we need to save our
model again in TensorFlow's own format. Rather than importing the
\texttt{tf2onnx} module here, we perform conversion in a subprocess,
executed via a shell escape.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mnist\PYZus{}model\PYZus{}tf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{err}{!}\PY{n}{python} \PY{o}{\PYZhy{}}\PY{n}{m} \PY{n}{tf2onnx}\PY{o}{.}\PY{n}{convert} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{saved}\PY{o}{\PYZhy{}}\PY{n}{model} \PY{n}{mnist\PYZus{}model\PYZus{}tf} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{output} \PY{n}{mnist\PYZus{}model}\PY{o}{.}\PY{n}{onnx}
\PY{err}{!}\PY{n}{ls} \PY{o}{\PYZhy{}}\PY{n}{lah} \PY{n}{mnist\PYZus{}model}\PY{o}{.}\PY{o}{*}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
WARNING:absl:Found untraced functions such as \_update\_step\_xla while saving
(showing 1 of 1). These functions will not be directly callable after loading.
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in
sys.modules after import of package 'tf2onnx', but prior to execution of
'tf2onnx.convert'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
2023-08-22 11:42:04,202 - WARNING - '--tag' not specified for saved\_model. Using
--tag serve
2023-08-22 11:42:04,486 - INFO - Signatures found in model: [serving\_default].
2023-08-22 11:42:04,486 - WARNING - '--signature\_def' not specified, using first
signature: serving\_default
2023-08-22 11:42:04,486 - INFO - Output names: ['dense\_4']
2023-08-22 11:42:04,566 - INFO - Using tensorflow=2.12.0, onnx=1.14.0,
tf2onnx=1.15.0/6d6b6c
2023-08-22 11:42:04,566 - INFO - Using opset <onnx, 15>
2023-08-22 11:42:04,572 - INFO - Computed 0 values for constant folding
2023-08-22 11:42:04,586 - INFO - Optimizing ONNX model
2023-08-22 11:42:04,629 - INFO - After optimization: Cast -1 (1->0), Identity -2
(2->0)
2023-08-22 11:42:04,631 - INFO -
2023-08-22 11:42:04,631 - INFO - Successfully converted TensorFlow model
mnist\_model\_tf to ONNX
2023-08-22 11:42:04,631 - INFO - Model inputs: ['flatten\_input']
2023-08-22 11:42:04,631 - INFO - Model outputs: ['dense\_4']
2023-08-22 11:42:04,631 - INFO - ONNX model is saved at mnist\_model.onnx
-rw-r--r-- 1 root root 605K Aug 22 11:34 mnist\_model.h5
-rw-r--r-- 1 root root 190K Aug 22 11:42 mnist\_model.onnx
-rw-r--r-- 1 root root 189K Aug 22 11:34 mnist\_model.tflite
    \end{Verbatim}

    Let us actually download the \texttt{.tflite} and also \texttt{.onnx}
file locally\ldots{}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{google}\PY{n+nn}{.}\PY{n+nn}{colab}\PY{n+nn}{.}\PY{n+nn}{files}
\PY{n}{google}\PY{o}{.}\PY{n}{colab}\PY{o}{.}\PY{n}{files}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{google}\PY{o}{.}\PY{n}{colab}\PY{o}{.}\PY{n}{files}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.onnx}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} NOTE: For some versions of TensorFlow and TFLite, it is not possible}
\PY{c+c1}{\PYZsh{} to import `tflite` and `tensorflow` into the same Python process.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} This normally is not even needed, given that `tf` has a `tf.lite` sub\PYZhy{}module,}
\PY{c+c1}{\PYZsh{} but here we want to demonstrate using TFlite only and not the full\PYZhy{}blown}
\PY{c+c1}{\PYZsh{} TensorFlow module.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} If executing this cell fails, then restarting the Colab runtime will}
\PY{c+c1}{\PYZsh{} replace the running Python interpreter with a new one while retaining}
\PY{c+c1}{\PYZsh{} on\PYZhy{}filesystem state of the virtual machine. So, it might be necessary to}
\PY{c+c1}{\PYZsh{} do [Runtime] \PYZhy{}\PYZgt{} [Restart Runtime] (Short\PYZhy{}cut: Control\PYZhy{}M dot) before}
\PY{c+c1}{\PYZsh{} continuing with this notebook by executing this cell.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} Since the runtime system may have been restarted here,}
\PY{c+c1}{\PYZsh{} we re\PYZhy{}import the modules that we will need going forward.}

\PY{c+c1}{\PYZsh{} Note: On non\PYZhy{}colab systems, if `tflite\PYZhy{}runtime` is not yet available}
\PY{c+c1}{\PYZsh{} for too\PYZhy{}recent a CPython version, this might require e.g.}
\PY{c+c1}{\PYZsh{} python3.10 \PYZhy{}m pip install tflite\PYZhy{}runtime}
\PY{err}{!}\PY{n}{pip} \PY{n}{install} \PY{n}{tflite}\PY{o}{\PYZhy{}}\PY{n}{runtime}

\PY{k+kn}{import} \PY{n+nn}{time}
\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{tflite\PYZus{}runtime}\PY{n+nn}{.}\PY{n+nn}{interpreter} \PY{k}{as} \PY{n+nn}{tflite}

\PY{n}{reloaded\PYZus{}examples} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training\PYZus{}examples.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{sample\PYZus{}batches\PYZus{}images} \PY{o}{=}  \PY{n}{reloaded\PYZus{}examples}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{sample\PYZus{}batches\PYZus{}labels} \PY{o}{=}  \PY{n}{reloaded\PYZus{}examples}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Requirement already satisfied: tflite-runtime in /usr/local/lib/python3.10/dist-
packages (2.13.0)
Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-
packages (from tflite-runtime) (1.23.5)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}mnist\PYZus{}tflite\PYZus{}predictor}\PY{p}{(}\PY{n}{model\PYZus{}path}\PY{p}{)}\PY{p}{:}
  \PY{n}{interpreter} \PY{o}{=} \PY{n}{tflite}\PY{o}{.}\PY{n}{Interpreter}\PY{p}{(}\PY{n}{model\PYZus{}path}\PY{o}{=}\PY{n}{model\PYZus{}path}\PY{p}{)}
  \PY{n}{interpreter}\PY{o}{.}\PY{n}{allocate\PYZus{}tensors}\PY{p}{(}\PY{p}{)}
  \PY{n}{input\PYZus{}details} \PY{o}{=} \PY{n}{interpreter}\PY{o}{.}\PY{n}{get\PYZus{}input\PYZus{}details}\PY{p}{(}\PY{p}{)}
  \PY{n}{output\PYZus{}details} \PY{o}{=} \PY{n}{interpreter}\PY{o}{.}\PY{n}{get\PYZus{}output\PYZus{}details}\PY{p}{(}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{fn\PYZus{}predict}\PY{p}{(}\PY{n}{in\PYZus{}data}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Note that this is not reentrant! Different invocations of the current}
    \PY{c+c1}{\PYZsh{} function use the same `interpreter`, and mutate its state by}
    \PY{c+c1}{\PYZsh{} \PYZdq{}setting the input\PYZdq{}. So, if multithreading executed called a function}
    \PY{c+c1}{\PYZsh{} that does this concurrently more\PYZhy{}than\PYZhy{}once\PYZhy{}at\PYZhy{}the\PYZhy{}same\PYZhy{}time, things}
    \PY{c+c1}{\PYZsh{} would go wrong.}
    \PY{n}{interpreter}\PY{o}{.}\PY{n}{set\PYZus{}tensor}\PY{p}{(}\PY{n}{input\PYZus{}details}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                           \PY{n}{in\PYZus{}data}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
    \PY{n}{interpreter}\PY{o}{.}\PY{n}{invoke}\PY{p}{(}\PY{p}{)}
    \PY{n}{t1} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
    \PY{n}{output\PYZus{}data} \PY{o}{=} \PY{n}{interpreter}\PY{o}{.}\PY{n}{get\PYZus{}tensor}\PY{p}{(}\PY{n}{output\PYZus{}details}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{result} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{output\PYZus{}data}\PY{p}{)}
    \PY{k}{if} \PY{n}{verbose}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(t=}\PY{l+s+si}{\PYZob{}}\PY{p}{(}\PY{n}{t1}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{1000}\PY{l+s+si}{:}\PY{l+s+s1}{.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ msec):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{result}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{result}
  \PY{k}{return} \PY{n}{fn\PYZus{}predict}

\PY{n}{mnist\PYZus{}tflite\PYZus{}predictor} \PY{o}{=} \PY{n}{get\PYZus{}mnist\PYZus{}tflite\PYZus{}predictor}\PY{p}{(}
    \PY{n}{model\PYZus{}path}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{mnist\PYZus{}tflite\PYZus{}predictor}\PY{p}{(}\PY{n}{sample\PYZus{}batches\PYZus{}images}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{,}
                                          \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ \PYZhy{}  Actual: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sample\PYZus{}batches\PYZus{}labels}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(t=0.906 msec): [-4.890e-01 -1.157e+00 -2.000e-03 -3.020e+00  8.140e-01
9.350e-01
  8.952e+00 -8.248e+00 -7.890e-01 -3.224e+00]
Predicted:  6  -  Actual:  6
    \end{Verbatim}

    \hypertarget{what-tensorflow-did-for-us-here}{%
\subsubsection{What TensorFlow did for us
here}\label{what-tensorflow-did-for-us-here}}

\begin{itemize}
\item
  For simple multi-layer architectures:

  \begin{itemize}
  \tightlist
  \item
    Provide us with an extremely simple way to specify a neural network.
  \item
    Automatically pick reasonable defaults for the distribution of
    randomized initial weights and biases.
  \item
    Hide all the subtleties around computing fast gradients.
  \end{itemize}
\item
  Offer a convenient way to specify how we want to utilize gradients for
  optimization.

  Earlier: `multiply gradient with a small factor and take a small step
  in the opposite direction (going down)'.

  Here: pick a very simple yet quite effective alternative strategy
  known as `Adam Optimization'.
\item
  Allow us a convenient way to specify a loss function - here directly
  from logits.

  (Earlier, we had to go from logits to probabilities, do softmax, and
  then hand-backpropagate it all).
\item
  Provide convenience functions for loading and transforming input data,
  which include very substantial performance optimizations.
\item
  Handle setting up the computation in such a way that training can
  optionally run on CPU, GPU, or TPU(!)
\item
  Provide us with a straightforward way to save and deploy a trained
  model.
\item
  Allow us to specify performance metrics to track during training.
\end{itemize}

\hypertarget{what-we-have-not-seen-yet}{%
\subsubsection{What we have not seen
yet}\label{what-we-have-not-seen-yet}}

\begin{itemize}
\tightlist
\item
  ``The scaffolding between this high-level perspective and the
  low-level approach we discussed earlier.''
\item
  How to put unusual data-processing into TensorFlow so that we can
  still use high level infrastructure like model-saving.
\item
  How to use the ``tensor arithmetics with fast gradients'' machinery
  inside TensorFlow to do non-ML physics.
\item
  More tuning and tweaks we can apply to improve performance (L2
  regularization, architectural elements such as: dropout layers,
  convolutional+max-pooling layers).
\end{itemize}

\hypertarget{additional-remarks}{%
\subsubsection{Additional remarks}\label{additional-remarks}}

\begin{itemize}
\item
  On `weight initialization': TensorFlow by default uses
  ``\href{https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}{Glorot} \cite{glorot2010understanding}
  (or `Xavier') initialization'' - basic idea: if N normal distributed
  inputs get summed over, use a 1/sqrt(N) scaling factor for random
  weight initialization.
\item
  Here, we are using the `test set' to obtain performance metrics. This
  is generally a bad idea, since it leads to applying tweaks based on
  information obtained by ``peeking at test set performance''.

  \textbf{If we want to report proper ability-to-generalize, we must not
  use observations on the test set to make model tuning decisions.}

  (The right thing to do here is to lock away the ``test set'' and split
  the for-training examples into an actual `training set for computing
  gradients' and a `validation set' for making tuning decisions.)

  Overall, this example is mostly about showing ``how we can wire things
  up'', and in this context, this is tolerable, since we are not out to
  ``break the world record on MNIST classification''.
\item
  Nowadays, a ML system doing MNIST-classification is a bit like a TV
  screen showing a test image - it merely indicates that we built
  something which allows data to flow the way it should, and we might be
  able to identify some glaring problems.
\end{itemize}

    \hypertarget{architecture-improvements}{%
\subsubsection{Architecture
Improvements}\label{architecture-improvements}}

Before we ``open the engine hood'' and take a deeper look at the
technology, let us discuss a few easy-to-explain ideas around ``what can
we do here to further improve image classifier performance?''

There are some simple general ideas that are reasonably easy-to-explain
in just one or two paragraphs - if we accept that the ``cheap heuristic
explanations'' we are giving might actually be a little bit off. At this
level, this is more ``cooking advice'' than a thorough discussion of
deeper characteristics.

\hypertarget{l2-regularization}{%
\subparagraph{\texorpdfstring{\textbf{L2
Regularization}}{L2 Regularization}}\label{l2-regularization}}

Now that we have seen how to train deep architectures: What would we
expect to happen if we substantially over-sized or under-sized a model
(in terms of number of layers and also units per layer) relative to what
experimentation tells us is the point where increasing model size brings
no further benefit?

Under-sizing is easy to understand: The model will not be able to
extract and process all the features that are in principle available,
and will have to make hard choices what to ignore.

Over-sizing is more interesting: If we massively over-sized the model,
and used a fixed-size training set, it would at some point have enough
capacity to just memorize every training example - and it may memorize
such examples in weird ways, such as ``If the top right pixel is green
and the top left pixel is brighter than its neighbors, then we know we
are looking at example \#1735''. Naturally, this then messes up the
model's ability to generalize. \emph{In particular}, if there is any
classification error in the training set labels, the model will not do
what an under-sized model would do - ``if I cannot do well and have to
take a hit, I perhaps should take one on this crazy outlier in the
data''. Rather, it would try to come up with a very fancy
``explanation'' why the outlier (and some region around it) is ``really
special''.

If we built a model with only linear activation functions, there would
be no point going to two or more layers - since ``all transformations
are linear''. So, the power of deep architectures is closely related to
their ability to put nonlinearities to good use. If we used an
architecture with tanh-activation as the nonlinearity on hidden layers,
and a final linear scaling layer, then tanh-s that receive
small-magnitude input would behave mostly-linear.

With this in mind, it appears natural to try an approach where ``we give
the network the ability to use many nonlinearities, but we attach a
small price to it, so the network will only decide to actually use that
power if that brings a benefit''.

Now, we are in an ``optimize performance, but also do X'' (i.e.~try not
to use large weights) situation. We still want to consider ML training
as a numerical optimization problem - but that then means that we have
to slightly deviate from the idea of ``what we optimize for is
performance'' towards ``our loss function is a weighted sum of
contributions, where one measures performance, and the others are about
other relevant aspects, such as minimizing some quantity that we believe
to be a useful proxy for the model inclination to avoid unnecessarily
complex explanations''.

The basic idea behind ``L2 regularization'' is: Let us add a term to the
total loss that is the sum-squared of weights. The model can use
nonlinearity, and ``smallish weights are still cheap'', but making a
weight large is considered costly.

The effect is nicely illustrated in these two TensorFlow Playground
training runs: Two large-capacity models trained on the same ``simple
explanation but noisy data'' dataset, having the same outliers. Without
L2 regularization, we see a tendency to ``draw a complicated border to
explain the training set''. With L2 regularization, we get a nicer
border. As expected, the complex border is related to ``overfitting'',
and we do see an accuracy gap between training and test set.

(See screenshots ``L2 Regularization'' on ``supplementary material''
document.)

\hypertarget{relu-activation}{%
\paragraph{\texorpdfstring{\textbf{ReLU
activation}}{ReLU activation}}\label{relu-activation}}

Traditionally, the earliest nonlinearity to be used widely in neural
network research was the `sigmoid' \(x\mapsto 1/(1+\exp(-x))\) (this
again is of course just the logistic function). The thinking here was
that this should serve as a crude model for biological neurons that
still allows us to do backpropagation: Zero output for low activation,
saturation for high activation, but we want a continuous function.

It so turns out that one can in principle use just about every ``not too
wild'' nonlinearity for Deep Neural networks, even functions such as
\(x\mapsto x^2\) or \(x\mapsto\exp(-x^2/2)\). Overall, this might not be
too surprising at least for smooth such functions - ``if we zoom in at
some point, we will see linear behavior, if we zoom out a bit, we will
see quadratic corrections, but higher corrections will still be
mostly-negligible''.

It somewhat came to a surprise to discover that an extremely simple
nonlinearity, which furthermore happens to be scale invariant, usually
performs really well in comparison - the ``rectified linear unit'',
\(x\mapsto (x+|x|)/2\). There was a natural drive towards such a very
simple function from the desire to avoid complicated numerical
calculations such as doing an exponential - and it stuck when it showed
good performance.

More recently, there have been variations on the topic, such as ELU and
SELU, but ReLU remains an important workhorse.

It is interesting to ponder what the functions described by
exclusively-ReLU deep networks look like: these are continuous functions
that are affine-linear on (generalized) polyhedral cells.
(``Generalized'' since they may extend to infinity.) Naturally,
``gradients just propagate through, even to very deep layers''.

The power of a deep ReLU network is nicely illustrated in the TensorFlow
Playground by doing a ``swiss roll with noisy data'' problem using only
the x/y coordinate as input features, and going for a maximum-capacity
network, even without any regularization - but one really has to watch
the training process to appreciate this:

(See screenshot ``ReLU Activation'' on ``supplementary material''
document.)

\hypertarget{dropout-regularization}{%
\paragraph{\texorpdfstring{\textbf{Dropout
Regularization}}{Dropout Regularization}}\label{dropout-regularization}}

Looking at some TensorFlow Playground training runs for the ``swiss
roll'' problem that use the same quite noisy input data, but started
from differently initialized random weights, it is not surprising that
we get roughly-comparable classifier performance, but both models made
different doubtful decisions about where to make the contour look
complicated to net a few more examples and get slightly lower loss on
the training set.

So, each model will have ``quirks''. Naturally, we would expect that if
we just did what we discussed earlier, combining the assessments of
different (perhaps not quite independent) ``experts'', such as
majority-vote-of-5-differently-trained-models, we should be able to
average out such ``structure hallucinations''. Of course, training and
deploying multiple models is expensive, so: can we perhaps use some
variant of this idea which retains some of the ``averaging over
different models'' property while not being so expensive?

One idea here is to use ``dropout'' layers: During the training process,
we keep randomly disabling some units (but correspondingly scaling up
the total input to a unit that has temporarily lost some of its input
units) in changing patterns. Effectively, we are training an
exponentially large family of networks (since with \(N\) ``faulty''
units, we have \(\sim 2^N\) ways to disable half of them) which are all
obtained from a ``master network'' by turning off some units.

Another way to think about this is that ``dropout'' punishes complex
co-adaptation of many units. If a complex hypothesis is realized by
having some specific set of 10 units activate in a particular pattern,
``dropout'' makes it unlikely that they all are available at the same
time, so, less complex hypotheses that only require two or three units
to cooperate are favored over intricate explanations.

(See screenshots ``Dropout'' on ``supplementary material'' document.)

\hypertarget{early-stopping}{%
\paragraph{\texorpdfstring{\textbf{Early
Stopping}}{Early Stopping}}\label{early-stopping}}

This is in the category of ``sometimes it helps, but sometimes we
observe the opposite, and it helps more to keep training even when the
model stopped learning anything''.

The basic idea is that it can happen that a model first learns the
overall structure of the problem, those aspects that generalize well,
but as it keeps being fed the training examples, it will increasingly
fine-tune on accidental properties of the training set - so, we should
be able to limit overfitting by stopping the training process early,
typically according to some heuristics.

\hypertarget{small-batch-sizes}{%
\paragraph{\texorpdfstring{\textbf{Small Batch
Sizes}}{Small Batch Sizes}}\label{small-batch-sizes}}

This point is controversial - there are indications that, while there is
some truth to this advice, the opposite also holds.

Overall, neural network training amounts to finding some minimum (in
weight/bias parameter space) of a ``loss: function that has many local
minima (at the very least since we can always permute/relabel units and
get an equivalent network). If we use smallish batch sizes (perhaps 32
or 64) to estimate gradients, that makes gradients inherently noisy, and
this noisiness makes us not see narrow basins in the loss function,
going for larger,''more robust'' minima instead.

\hypertarget{convolutional-architectures}{%
\paragraph{\texorpdfstring{\textbf{Convolutional
Architectures}}{Convolutional Architectures}}\label{convolutional-architectures}}

So far, we fed our classifiers one-dimension-per-pixel data vectors.

Now, if we would in advance pick an image-scrambling permutation of
pixels and applied this in the same way to all training and test
examples, this would make the problem no more difficult for the ML
architectures we have seen so far, but would make the problem
\emph{much} harder for a human. So, clearly, the problem has extra
structure which we do not even remotely exploit yet.

The MNIST dataset is normalized to always have the ink-center-of-gravity
in the middle of the image, and also in some other ways. This makes this
item somewhat a case of ``this happens to work also on MNIST, despite
the major reasons why this is a good thing applying to a limited
extent''. Sticking with MNIST, one way to think about the problem is
that we might be able to reduce ``overfitting'' (so, mistaking
accidental structure observed in the training set for relevant) by
applying some data-reduction that we would expect to reduce such
accidental features. Obviously, digits are a quite anthropomorphic
solution to the problem of communicating data between humans - in the
sense that doing the same between computers, we would likely go for a
more barcode or QR-code like solution. So, unsurprisingly, digits work
reasonably well even for people with slightly bad eyesight - they do not
contain relevant fine detail that must be exactly right.

So, one might wonder whether blurring the image a bit could help with
classification. A ``blurring'' transform basically amounts to performing
a convolution with some kernel, such as a Gaussian. This leaves the
question: What kernel to use? If we were to answer ``this is ML, so we
might simply treat the kernel's parameters as learnable and use gradient
descent to find out what a good kernel might be'', and perhaps add
``let's blur in more than one way at the same time'', we basically have
invented ``Convolutional Neural Networks''.

Another way to think about this: If we humans look at an object, it
mostly does not matter much whether it sits right in the center of our
field of vision, or is a tiny bit displaced to the left, right, or up,
or down. So, for many image classifiaction problems, there is some
inherent translational symmetry in the task: If I want to know whether
there is a frog in an image, then it should not matter for detecting the
frog if the camera ``pointed two pixels further to the left'' when the
picture was taken. So, we might want to do localized feature-extraction
that is only sensitive to some small windowed region in the image, and
slide that window over the entire image, both horizontally and
vertically. Here, ``each window gets treated the same'', in the sense
that we use the same weights independent of window-position. So, in
training, if we have frogs in 4 out of 16 batch-examples, then such a
set-up will try to tweak the convolution kernel(s) in such a way that
they become sensitive to the differences between images-with-frogs and
images-without-frogs, irrespective of how we have to place the window(s)
to best fit the frogs.

Right after a convolutional layer, which convolves window-wise with a
collection of learnable kernels, one typically puts a
resolution-reducing layer, often max-pooling. About max-pooling,
Geoffrey Hinton has (in)famously said: ``The pooling operation used in
convolutional neural networks is a big mistake and the fact that it
works so well is a disaster.'' Here, we contend ourselves with observing
that ``it often works quite well.''

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Before we proceed with `tf` code again, we might have to re\PYZhy{}start the runtime}
\PY{c+c1}{\PYZsh{} system and re\PYZhy{}run the very first cell of this notebook, which re\PYZhy{}imports}
\PY{c+c1}{\PYZsh{} TF and other modules, and loads the dataset.}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us put some of these ideas to the test.}
\PY{c+c1}{\PYZsh{} Example adjusted from: https://keras.io/examples/vision/mnist\PYZus{}convnet/}


\PY{n}{cnn\PYZus{}model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}
    \PY{p}{]}
\PY{p}{)}

\PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}\PY{p}{,}
    \PY{n}{loss}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{SparseCategoricalCrossentropy}\PY{p}{(}\PY{n}{from\PYZus{}logits}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{metrics}\PY{o}{.}\PY{n}{SparseCategoricalAccuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,}
\PY{p}{)}

\PY{k}{if} \PY{k+kc}{True}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} This can take about half an hour or so. Feel free to disable.}
  \PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{ds\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{ds\PYZus{}validation}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{if} \PY{k+kc}{True}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} Some extra training, for refinement \PYZhy{} feel free to disable.}
  \PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{ds\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{ds\PYZus{}validation}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/2
1407/1407 [==============================] - 58s 41ms/step - loss: 0.0293 -
sparse\_categorical\_accuracy: 0.9906 - val\_loss: 0.0311 -
val\_sparse\_categorical\_accuracy: 0.9904
Epoch 2/2
1407/1407 [==============================] - 57s 41ms/step - loss: 0.0286 -
sparse\_categorical\_accuracy: 0.9906 - val\_loss: 0.0306 -
val\_sparse\_categorical\_accuracy: 0.9906
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Model: "sequential\_1"
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
 Layer (type)                Output Shape              Param \#
=================================================================
 conv2d\_2 (Conv2D)           (None, 26, 26, 32)        320

 max\_pooling2d\_2 (MaxPooling  (None, 13, 13, 32)       0
 2D)

 conv2d\_3 (Conv2D)           (None, 11, 11, 64)        18496

 max\_pooling2d\_3 (MaxPooling  (None, 5, 5, 64)         0
 2D)

 flatten\_1 (Flatten)         (None, 1600)              0

 dropout\_1 (Dropout)         (None, 1600)              0

 dense\_2 (Dense)             (None, 50)                80050

 dense\_3 (Dense)             (None, 10)                510

=================================================================
Total params: 99,376
Trainable params: 99,376
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
    \end{Verbatim}

    What we have here may well be superhuman ability at reading handwritten
digits which we can readily deploy as a 400 KB model file on even quite
cheap microcontrollers. This certainly is interesting.

However, given that we still did not even try hard yet, this illustrates
another point: ``MNIST is in general too simple to demonstrate
superiority of some ML method'' and ``just about every idea works well
on MNIST''.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us download/save HDF5 and tflite forms of this model.}

\PY{k+kn}{from} \PY{n+nn}{google}\PY{n+nn}{.}\PY{n+nn}{colab} \PY{k+kn}{import} \PY{n}{files}

\PY{err}{!}\PY{n}{rm} \PY{o}{\PYZhy{}}\PY{n}{rf} \PY{n}{mnist\PYZus{}cnn\PYZus{}model}\PY{o}{.}\PY{n}{h5}

\PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SAVE\PYZhy{}TRAINED\PYZhy{}MODEL}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{and} \PY{k+kc}{False}\PY{p}{:}  \PY{c+c1}{\PYZsh{} Remove \PYZsq{}and False\PYZsq{} to save the model.}
  \PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}cnn\PYZus{}model.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{err}{!}\PY{n}{ls} \PY{o}{\PYZhy{}}\PY{n}{la} \PY{n}{mnist\PYZus{}cnn\PYZus{}model}\PY{o}{.}\PY{o}{*}
  \PY{n}{cnn\PYZus{}converter} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{lite}\PY{o}{.}\PY{n}{TFLiteConverter}\PY{o}{.}\PY{n}{from\PYZus{}keras\PYZus{}model}\PY{p}{(}\PY{n}{cnn\PYZus{}model}\PY{p}{)}
  \PY{n}{tflite\PYZus{}cnn\PYZus{}model} \PY{o}{=} \PY{n}{cnn\PYZus{}converter}\PY{o}{.}\PY{n}{convert}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} Save the model.}
  \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}cnn\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
    \PY{n}{f}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{tflite\PYZus{}cnn\PYZus{}model}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{n}{files}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}cnn\PYZus{}model.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{files}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}cnn\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{UPLOAD\PYZhy{}MODEL}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{and} \PY{k+kc}{True}\PY{p}{:}  \PY{c+c1}{\PYZsh{} Upload a model instead.}
  \PY{n}{model\PYZus{}data} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{n}{files}\PY{o}{.}\PY{n}{upload}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}cnn\PYZus{}model.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h\PYZus{}model}\PY{p}{:}
    \PY{n}{h\PYZus{}model}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{model\PYZus{}data}\PY{p}{)}
  \PY{n}{reloaded\PYZus{}cnn\PYZus{}model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{load\PYZus{}model}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mnist\PYZus{}cnn\PYZus{}model.h5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{n+nb}{print}\PY{p}{(}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test set accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{reloaded\PYZus{}cnn\PYZus{}model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{ds\PYZus{}test}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{l+m+mi}{100}\PY{l+s+si}{:}\PY{l+s+s1}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}
      \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
313/313 [==============================] - 5s 15ms/step - loss: 0.0244 -
sparse\_categorical\_accuracy: 0.9917
Test set accuracy: 99.17\%
    \end{Verbatim}

    \hypertarget{other-major-architectural-ideas}{%
\section{Other major architectural
ideas}\label{other-major-architectural-ideas}}

We discussed convolutional networks for image recognition tasks, and
also some of the common approaches to improve model performance (which
typically means: generalization).

Our next major focus will be ``using the TensorFlow machinery to do
Physics with it - perhaps without any ML involved''. So, this may be a
good opportunity to discuss a few other important general ML-related
ideas.

    \hypertarget{embeddings}{%
\subsection{Embeddings}\label{embeddings}}

Much of ``supervised Machine Learning'' (i.e.~we have target labels)
typically is about:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Representing some aspect(s) of the real world we care about as a
  high-dimensional vector. (This might be: an image, a molecular
  structure, a sentence, a medical record, etc.)
\item
  Defining a ``model'' \(m_{\vec \theta}\) with trainable parameters
  \(\vec\theta\in\mathbb{R}^D\) that can produce the desired output.
\item
  Obtaining training examples and tweaking model parameters (typically
  via some variant of stochastic gradient descent) to handle training
  set examples well.
\item
  Measuring performance on a validation-set.
\item
  Once everything looks good, measuring how well the model generalizes
  on the test set, and deploying the model.
\end{enumerate}

We want to focus on 1.: How do we turn something like a sentence into a
vector?

The ``\href{https://en.wikipedia.org/wiki/Netflix_Prize}{Netflix Prize
Problem}'' provides an interesting setting to study this question. The
Wikipedia article has details about the back story, but in a nutshell,
this was about the Netflix video streaming company setting up a \$1M
competition back in 2006 for building a video recommendation system that
outperforms their own system by at least 10\%.

Academically, what was interesting about this problem was that Netflix
provided a very large training dataset. Getting training data labels is
generally expensive, and here the research community had an opportunity
to use a dataset much larger than what they normally had available, at
the order of 100M training examples.

The problem was as follows: Netflix customers watch movies, and get an
opportunity to rate how well they enjoyed a movie right after they
watched it. These ratings do obviously contain personal preferences
about movies, and so it clearly is attractive for Netflix to have a
recommendation system that produces individualized suggestions about
what other movies a customer might enjoy watching.

For our purposes, we can imagine the overall setting to be as follows:
We have a large ``user movie ratings'' matrix \(R_{um}\), where user
\(u\) rates movie \(m\) - let's say with score
\(R_{um}\in [-1.. 1] \cup {\rm NaN}\), where \({\rm NaN}\) is supposed
to mean: ``this user has not rated (perhaps not watched) that movie''.
We might have additional information about movies, but here, we want to
focus exclusively on this matrix.

The matrix that we are given has some entries blanked out - Netflix
knows how the given user rated the given movie, but for some (perhaps a
million or so) ratings, they do not tell us and instead have put a
\({\rm NaN}\) entry there, just as if the user had not yet rated the
movie. We do not know which entries are the missing ones, but we want to
build a system that can predict these ratings well.

\textbf{Let us have a break here and give everybody 10 minutes to think
about the problem.}

In the end, the prize was won in 2009, in a ``photo finish''. The
winning team did a hundred different things, but we want to focus on the
core idea.

This was simply to try to find two matrices \(U, M\) such that
\(R\approx UM\) holds for the known entries. With indices,
\(R_{um}=\tilde U_{uk}\,\tilde M_{km}\). Here, \(u\) is still a
user-index (going up to perhaps \(100\,000\)), and \(m\) is still a
movie index (going up to maybe \(10\,000\) or so). The range of the
index \(k\) is small-ish, perhaps going up to \(K=50\).

We see how we can regard this as an optimization problem. We also
immediately see how we can use such a factorization to make predictions.
But why is doing this useful?

Suppose individual customers have movie preferences that could be
described as ``generally likes Jackie Chan movies'', ``likes comedy'',
``dislikes horror movies''. Now, if we had a fixed list of perhaps 200
such categories, we might try to get to a quantitative prediction if we
find every user's and every movie's ``profile'' as a vector of
quantitified alignment with each category. Then, the alignment between a
given user's and movie's profile should allow us to predict an unknown
rating.

The beauty of this ``matrix factorization'' approach is that it
determines these categories for us, in such a way that they are most
useful for the problem at hand!

Now, of course, as stated, the problem has a \(GL(K)\) ambiguity, since
we can always transform \(\tilde U\to \tilde U \Lambda\),
\(\tilde M\to \Lambda^{-1} \tilde M\), with \(\Lambda\in GL(K)\). But
let's say we deal with this somehow, perhaps by additionally postulating
that every row-vector of \(\tilde U\) and every column-vector of
\(\tilde M\) must be length-1. This would then still leave us with some
ambiguity (we could still do an \(O(K)\) rotation of the coordinate
basis), but apart from such details, we are left with a linear
preference model where the problem itself seeks out the relevant
directions.

Another way to view this problem: If all entries of \(R\) were known, it
very likely would not be a ``random'' matrix but have extra structure.
So, the question is: if we wanted to do data reduction and store far
fewer elements, what would be a good proposal to still get a reasonably
good approximation to the original matrix? Intuitively, ``the best thing
we can usually do'' is to perform a Principal Component Analysis. This
amounts to writing the matrix as a product \(R=USV^T\) where here, since
\(R\) is real, \(U\) and \(V\) are orthogonal, and \(S\) is rectangular
with entries only on the diagonal\textasciitilde{}\(S_{(j)(j)}\) - and
then trimming \(S\) by only retaining the \(K\) largest-magnitude
``generalized eigenvalues''. So, the approach is: ``Perform a Singular
Value Decomposition, and project out all smallish generalized
eigenvalues''. This is, of course, a tried and tested pre-Deep-Learning
ML approach that is widely used in many disciplines, also in financial
analysis.

Can we then interpret these ``principal axes''? It turns out that,
indeed, if we work this out for the Netflix matrix, we find that the two
most relevant dimensions are ``Drama-vs-Comedy'' and
``Unsurprising-Plot-vs-Plot-Twists''.

A nicely readable article about this is:
\href{https://datajobs.com/data-science-repo/Recommender-Systems-\%5BNetflix\%5D.pdf}{Matrix
Factorization Techniques for Recommender Systems}.

Now, what does this mean for Deep Learning? A basic insight is that if
we have tokenized input (such as for text: each word or multi-word term
in the dictionary is one token), we often can simply map each token to
some \(K\)-dimensional vector with a trainable mapping (which we call an
``embedding''), and leave it to the problem at hand to adjust the
token-to-vector mapping in a way that maximizes
usefulness-for-the-problem.

This approach has produced some interesting surprises. Leaving out some
(not quite irrelevant) detail, training a word embedding model on a
problem that involves examples from news articles, for example, one may
well find that the embedding evolved some lattice-like structure. If
\(E: {\rm token}\to{\mathbb R}^K\) is the trained token-embedding
function, we may find that equations such as
\(E({\rm Paris}) - E({\rm France}) + E({\rm Germany}) \approx E({\rm Berlin})\)
hold (in the sense that among the nearest neighbors of the
left-hand-side vector, we find the embedding vector for ``Berlin'' on
typically the 1st, but generally maybe at most 2nd or 3rd place).

Google put some code from 2013 online that allows exploring this
phenomenon. It meanwhile has been affected by some moderate ``bit rot'',
but for those who are curious, the web page is
\url{https://code.google.com/archive/p/word2vec/}, and the code archive
is:
\url{https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip}

\hypertarget{the-wider-ml-landscape}{%
\subsection{The wider ML Landscape}\label{the-wider-ml-landscape}}

Some important notions and ideas from the wider Machine Learning
landscape that we at least should have mentioned, but will not have any
time to go into detail about.

\begin{itemize}
\item
  Unsupervised Learning

  In general: Learning where we have no ground truth labels.

  \begin{itemize}
  \tightlist
  \item
    Autoencoders (Variations on the ``input \(\to\) \{compact
    representation\} \(\to\) original'' idea.)
  \item
    t-SNE as a ``useful heuristic for visualizing in 2d or 3d how data
    clusters in high dimensions''.
  \end{itemize}
\item
  Semi-Supervised Learning

  Learning where we have some labeled examples, and many more unlabeled
  examples. Idea: ``Knowing how stuff generally looks like will
  typically help''.
\item
  Reinforcement Learning

  Learning behavior from reward observations.
\item
  Regression Problems

  Estimating a non-discrete quantity. Key questions revolve around ``how
  much spread is there in the `labels'\,'', and `what features allow me
  to explain how much of that spread?' I.e. ``Standard deviation of the
  height of all children attending primary school is \(X\), but if I
  know their age, I can predict their height with standard deviation
  \(Y<X\).
\item
  Non-Neural-Network Approaches

  \begin{itemize}
  \tightlist
  \item
    K-Means Classifiers
  \item
    Support Vector Machines and Kernel Machines
  \end{itemize}
\item
  Specialized NN architectures

  \begin{itemize}
  \item
    Sequence-to-Sequence learning: LSTM (`ancient' but quite powerful).
  \item
    Graph-NNs(/-CNNs)
  \item
    ``Transformer Architectures'' More recent, quite powerful, we are
    still in the process of properly understanding what they can do and
    how they work. Basis for large language models such as
    \href{https://en.wikipedia.org/wiki/GPT-3}{GPT-3}.
  \item
    Generative Adversarial Networks - for generating e.g.
    realistic-looking art.

    Also, for example: ``Neural Photo Editing''
    (\href{https://www.youtube.com/watch?v=FDELBFSeqQs}{Example YouTube
    video} - \href{https://arxiv.org/abs/1609.07093}{paper} \cite{brock2016neural}).
  \end{itemize}
\item
  Tweaking pre-trained models;
  \href{https://www.tensorflow.org/hub}{TensorFlow Hub}
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    

    
    
    
    

    
    \hypertarget{using-tensorflow-for-physics}{%
\section{Using TensorFlow for
Physics}\label{using-tensorflow-for-physics}}

We will now, for the time being, leave Machine Learning behind and
explore the deeper levels of TensorFlow, the
fast-gradients-on-GPU-and-TPU machinery, and see how we can use this to
our advantage when doing physics.

    \hypertarget{accelerating-computations-with-graphics-hardware}{%
\subsection{Accelerating computations with Graphics
Hardware}\label{accelerating-computations-with-graphics-hardware}}

Comparing ``how our numpy-based and TensorFlow-based MNIST code feel
like'', I think it is fair to say that ``TensorFlow is a fast beast''.

When we run on GPU, the rules of the game are that the GPU contains many
small processor-like units, which however cannot do too complex control
flow operations, and work in groups where in each group, all must
perform the same steps (generally going through different data). Bit
like rowers on multiple rowing eights.

Want to do tensor arithmetics in such a way that we can put the
computation onto a CPU, or alternatively onto such an architecture. It
helps to understand the overall design if we first look into earlier
ideas and approaches towards ``making some computations execute either
on CPU or on (say, even pre-GPU) graphics hardware''. The first such
applications were, unsurprisingly, about making computer graphics fast.

The main abstraction that evolved (first in Silicon Graphics's
``\href{https://en.wikipedia.org/wiki/IRIS_GL}{IRIS GL}'' system, which
became the OpenGL standard) was that of a ``display list''. A ``display
list'' basically is a sequence of 3d graphics related drawing commands
such as ``draw a triangle with these vertex 3d coordinates'' which form
a unit that can either be processed by code running on CPU, or more
directly by silicon in the graphics processing unit.

In principle, we can do the following thing:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tell the OpenGL library to start recording all subsequent drawing
  commands on a display list.
\item
  Go through a lot of complicated code which might do things such as
  running a fractal generator to generate some realistic-looking
  landscape on-demand, but also will - interspersed in that calculation
  - at some point execute 3d drawing commands.
\item
  Tell the OpenGL library to stop recording and hand back the sequence
  of recorded drawing commands as a ``display list object''.
\item
  Perhaps change perspective, lighting, or other details, and re-execute
  the recorded list of 3d drawing commands to re-render the scene,
  perhaps from another viewpoint.
\end{enumerate}

Pretty much the same strategy also would work for recording and redoing
tensor arithmetics, if we made all tensor-arithmetics operations use a
library that can switch into ``recording mode''. The question is: Why
would one want to do that? With 3d rendering, it's clear that we might
want to redo rendering a given scene from a different perspective. But
what would we want to do differently when redoing a tensor arithmetic
calculation? After all, if we did feed slightly different input data,
chances are that we might branch differently at some point, and the
actual calculation would then follow a path that is not the one recorded
on the tape. Still, there actually is much benefit in having such a
tensor-arithmetic code, just because it has all the go-left-or-right
decisions baked in that have been made ``on the forward calculation''.
This is straight code, without branching, that tells us what numerics
operations actually have happened, and so we would have a very easy time
doing sensitivity-backpropagation on this.

But let us take an actual look at an OpenGL display list, to show that
this is no dark magic.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{err}{!}\PY{n}{pip} \PY{n}{install} \PY{n}{pyopengl}
\PY{c+c1}{\PYZsh{} One issue with `pyopengl` is that unless GLUT and GLU libraries}
\PY{c+c1}{\PYZsh{} are installed, this will fail with cryptic error messages, so we need}
\PY{c+c1}{\PYZsh{} to ensure we have GLUT and GLU packages \PYZhy{} plus an X virtual framebuffer}
\PY{c+c1}{\PYZsh{} to render X in headless mode.}
\PY{err}{!}\PY{n}{apt} \PY{n}{install} \PY{n}{xvfb} \PY{n}{freeglut3} \PY{n}{libglu1}\PY{o}{\PYZhy{}}\PY{n}{mesa}
\PY{c+c1}{\PYZsh{} Starting a virtual\PYZhy{}framebuffer X server and setting this as our default}
\PY{c+c1}{\PYZsh{} X11 display.}
\PY{err}{!}\PY{n}{nohup} \PY{o}{\PYZgt{}}\PY{o}{/}\PY{n}{dev}\PY{o}{/}\PY{n}{null} \PY{l+m+mi}{2}\PY{o}{\PYZgt{}}\PY{o}{\PYZam{}}\PY{l+m+mi}{1} \PY{n}{Xvfb} \PY{p}{:}\PY{l+m+mi}{7} \PY{o}{\PYZhy{}}\PY{n}{screen} \PY{l+m+mi}{0} \PY{l+m+mi}{1024}\PY{n}{x1024x24} \PY{o}{\PYZam{}}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{n}{os}\PY{o}{.}\PY{n}{environ}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DISPLAY}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:7.0}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} Some additional modules and extensions used further down in this notebook.}
\PY{k+kn}{import} \PY{n+nn}{collections}
\PY{k+kn}{import} \PY{n+nn}{datetime}
\PY{k+kn}{import} \PY{n+nn}{itertools}
\PY{k+kn}{import} \PY{n+nn}{math}
\PY{k+kn}{import} \PY{n+nn}{sys}

\PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k+kn}{import} \PY{n}{Image}
\PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k+kn}{import} \PY{n}{ImageOps}

\PY{k+kn}{from} \PY{n+nn}{OpenGL} \PY{k+kn}{import} \PY{n}{GL} \PY{k}{as} \PY{n}{gl}
\PY{k+kn}{from} \PY{n+nn}{OpenGL} \PY{k+kn}{import} \PY{n}{GLU} \PY{k}{as} \PY{n}{glu}
\PY{k+kn}{from} \PY{n+nn}{OpenGL} \PY{k+kn}{import} \PY{n}{GLUT} \PY{k}{as} \PY{n}{glut}

\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{interpolate}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize}

\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}

\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{image} \PY{k}{as} \PY{n}{mpimage}

\PY{c+c1}{\PYZsh{} Load the TensorBoard notebook extension.}
\PY{o}{\PYZpc{}}\PY{n}{load\PYZus{}ext} \PY{n}{tensorboard}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-
packages (3.1.7)
Reading package lists{\ldots} Done
Building dependency tree{\ldots} Done
Reading state information{\ldots} Done
libglu1-mesa is already the newest version (9.0.2-1).
freeglut3 is already the newest version (2.8.1-6).
xvfb is already the newest version (2:21.1.4-2ubuntu1.7\textasciitilde{}22.04.1).
0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.
The tensorboard extension is already loaded. To reload it, use:
  \%reload\_ext tensorboard
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{opengl\PYZus{}demo}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{gl\PYZus{}delete\PYZus{}lists\PYZus{}calls} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{def} \PY{n+nf}{setup\PYZus{}display\PYZus{}lists}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{gl\PYZus{}list\PYZus{}start} \PY{o}{=} \PY{n}{gl}\PY{o}{.}\PY{n}{glGenLists}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} BEGIN optimized display\PYZhy{}list for rendering a tetrahedron.}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glNewList}\PY{p}{(}\PY{n}{gl\PYZus{}list\PYZus{}start}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}COMPILE}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glBegin}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}TRIANGLES}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{o}{+}\PY{l+m+mf}{2.0}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glEnd}\PY{p}{(}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} BEGIN optimized display\PYZhy{}list for rendering a tetrahedron.}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glEndList}\PY{p}{(}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{gl\PYZus{}delete\PYZus{}lists\PYZus{}calls}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{gl}\PY{o}{.}\PY{n}{glDeleteLists}\PY{p}{(}\PY{n}{gl\PYZus{}list\PYZus{}start}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tetrahedron}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{gl\PYZus{}list\PYZus{}start}\PY{p}{,}
            \PY{c+c1}{\PYZsh{} We could have further display lists here.}
            \PY{p}{\PYZcb{}}
  \PY{c+c1}{\PYZsh{}}
  \PY{n}{width}\PY{p}{,} \PY{n}{height} \PY{o}{=} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{400}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutInit}\PY{p}{(}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutInitDisplayMode}\PY{p}{(}\PY{n}{glut}\PY{o}{.}\PY{n}{GLUT\PYZus{}DOUBLE} \PY{o}{|} \PY{n}{glut}\PY{o}{.}\PY{n}{GLUT\PYZus{}RGB} \PY{o}{|} \PY{n}{glut}\PY{o}{.}\PY{n}{GLUT\PYZus{}DEPTH}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutInitWindowSize}\PY{p}{(}\PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutCreateWindow}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutHideWindow}\PY{p}{(}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glClearColor}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glColor}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}
  \PY{k}{try}\PY{p}{:}
    \PY{n}{display\PYZus{}lists} \PY{o}{=} \PY{n}{setup\PYZus{}display\PYZus{}lists}\PY{p}{(}\PY{p}{)}
    \PY{k}{def} \PY{n+nf}{render}\PY{p}{(}\PY{p}{)}\PY{p}{:}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glEnable}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}TEXTURE\PYZus{}2D}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glEnable}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}DEPTH\PYZus{}TEST}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glClearDepth}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glDepthFunc}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LESS}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glDepthMask}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}TRUE}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glShadeModel}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}SMOOTH}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glViewport}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glMatrixMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}PROJECTION}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glLoadIdentity}\PY{p}{(}\PY{p}{)}
      \PY{n}{glu}\PY{o}{.}\PY{n}{gluPerspective}\PY{p}{(}\PY{l+m+mf}{45.0}\PY{p}{,} \PY{n}{width} \PY{o}{/} \PY{n}{height}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1000.0}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glMatrixMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}MODELVIEW}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glLoadIdentity}\PY{p}{(}\PY{p}{)}
      \PY{c+c1}{\PYZsh{} Lighting}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glLightfv}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LIGHT0}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}AMBIENT}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glLightfv}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LIGHT0}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}DIFFUSE}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glLightfv}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LIGHT0}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}POSITION}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{2.0}\PY{p}{,}\PY{l+m+mf}{23.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{20.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glEnable}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LIGHT0}\PY{p}{)}
      \PY{n}{glu}\PY{o}{.}\PY{n}{gluLookAt}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{20.0}\PY{p}{,} \PY{l+m+mf}{7.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glClear}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}COLOR\PYZus{}BUFFER\PYZus{}BIT} \PY{o}{|} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}DEPTH\PYZus{}BUFFER\PYZus{}BIT}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glPolygonMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}FRONT\PYZus{}AND\PYZus{}BACK}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}FILL}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glColor3f}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glCallList}\PY{p}{(}\PY{n}{display\PYZus{}lists}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tetrahedron}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glColor3f}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glPolygonMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}FRONT\PYZus{}AND\PYZus{}BACK}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LINE}\PY{p}{)}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glCallList}\PY{p}{(}\PY{n}{display\PYZus{}lists}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tetrahedron}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}}
    \PY{n}{render}\PY{p}{(}\PY{p}{)}
    \PY{n}{glut}\PY{o}{.}\PY{n}{glutSwapBuffers}\PY{p}{(}\PY{p}{)}
    \PY{n}{gl}\PY{o}{.}\PY{n}{glPixelStorei}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}PACK\PYZus{}ALIGNMENT}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{data} \PY{o}{=} \PY{n}{gl}\PY{o}{.}\PY{n}{glReadPixels}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,}
                           \PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{,}
                           \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}RGB}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}UNSIGNED\PYZus{}BYTE}\PY{p}{)}
    \PY{n}{image} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{frombytes}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RGB}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{p}{(}\PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{)}\PY{p}{,} \PY{n}{data}\PY{p}{)}
    \PY{n}{image}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GL.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PNG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{k}{finally}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Ensure deallocation of all display lists before we leave}
    \PY{c+c1}{\PYZsh{} this call frame.}
    \PY{k}{for} \PY{n}{f} \PY{o+ow}{in} \PY{n}{gl\PYZus{}delete\PYZus{}lists\PYZus{}calls}\PY{p}{:} \PY{n}{f}\PY{p}{(}\PY{p}{)}


\PY{n}{opengl\PYZus{}demo}\PY{p}{(}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{mpimage}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GL.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_3_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    So\ldots{} ``is this how TensorFlow works''? Not quite - but this
``arithmetic tape'' approach to backpropagation naturally is popular for
many such systems, and it sometimes shows in names such as
``\href{https://dl.acm.org/doi/abs/10.1145/2450153.2450158}{TAPENADE} \cite{hascoet2013tapenade}''
(which, incidentally, uses a somewhat more clever approach than a bare
tape). The idea has however entered the TensorFlow2 API in the form of a
``Gradient Tape''.

So, what then does TensorFlow do? Any given intermediate (tensor) result
in a tensor-arithmetic computation is part of a directed acyclic graph
that represents the calculation, and the tensors ``flow'' along the
edges of this graph, which show how the output of one operation feeds
into other ``downstream'' operations.

Having this graph has many advantages: One may think that even with a
graph, when we actually do the computation, we have to go through all
nodes in some particular order - but that is actually not true on
massively parallel hardware, where we can make decisions to have
different parts of the graph evaluated concurrently, on different
devices. So, given some particular computing architecture with parallel
capabilites, there is an interesting problem around ``how to make this
particular tensor-arithmetic calculation fast on that specific
hardware?'' - and this is what TensorFlow (i.e.~the lower architectural
layers of TensorFlow) do for us. Also, of course, it understands
sensitivity-backpropagation in terms of a graph-transformation.

Let's jump straight in and actually see an example. We will be using
TensorFlow2 with TensorBoard for visualization. Overall, TensorBoard is
more often used to visualize higher level things than function-graphs,
but here this is appropriate. Let's do some very simple physics:
computing the energy of N electrons placed on a unit sphere in 3d.

For simplicity, we use a redundant description where every electron is
described by 3 coordinates, but we unit-normalize the vector to put them
all on a sphere.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{thomson\PYZus{}energy}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes the electrostatic energy of electrons on a sphere.}

\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    electron\PYZus{}coords: [num\PYZus{}electrons, 3] float tf.Tensor.}
\PY{l+s+sd}{      Per\PYZhy{}electron 3d position. This gets projected onto the unit sphere before}
\PY{l+s+sd}{      working out electrostatic energy.}
\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    float tf.Tensor, the total electrostatic energy.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{projected\PYZus{}electron\PYZus{}coords} \PY{o}{=} \PY{p}{(}
      \PY{n}{electron\PYZus{}coords} \PY{o}{/} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} There is TensorFlow library code for computing a pairwise\PYZhy{}distance matrix,}
  \PY{c+c1}{\PYZsh{} but here, let us show how to do that manually,}
  \PY{c+c1}{\PYZsh{} using reshaping and broadcasting.}
  \PY{n}{distance\PYZus{}vecs} \PY{o}{=} \PY{p}{(}\PY{n}{projected\PYZus{}electron\PYZus{}coords}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{\PYZhy{}}
                   \PY{n}{projected\PYZus{}electron\PYZus{}coords}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
  \PY{n}{distances} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{distance\PYZus{}vecs}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} We also need inverse\PYZhy{}distances. The problem is that every electron has}
  \PY{c+c1}{\PYZsh{} distance zero to itself. We could use tf.math.divide\PYZus{}no\PYZus{}nan() here,}
  \PY{c+c1}{\PYZsh{} but then we would have some explaining to do how this works with}
  \PY{c+c1}{\PYZsh{} gradients, and why we can take it as a guarantee to indeed always see exact}
  \PY{c+c1}{\PYZsh{} zeros on the matrix diagonal, despite doing approximate numerical}
  \PY{c+c1}{\PYZsh{} calculations. Let us instead show another trick}
  \PY{c+c1}{\PYZsh{} (mostly to illustrate some techniques).}
  \PY{c+c1}{\PYZsh{} If, for example, num\PYZus{}electrons = 3, our distance matrix is of the form:}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} [[d00, d01, d12],}
  \PY{c+c1}{\PYZsh{}  [d10, d11, d12],}
  \PY{c+c1}{\PYZsh{}  [d20, d21, d22]]}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} The problematic elements are along the diagonal.}
  \PY{c+c1}{\PYZsh{} If we trim `d22` and reshape the remaining N\PYZca{}2\PYZhy{}1 elements to [N\PYZhy{}1, N+1],}
  \PY{c+c1}{\PYZsh{} we get this form:}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} [[d00, d01, d02, d10],}
  \PY{c+c1}{\PYZsh{}  [d11, d12, d20, d21]]}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} ...where we can just drop the initial column.}
  \PY{n}{num\PYZus{}electrons} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{n}{new\PYZus{}shape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{num\PYZus{}electrons} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}electrons} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
  \PY{n}{reshaped\PYZus{}dists} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{distances}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{new\PYZus{}shape}\PY{p}{)}
  \PY{k}{return} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{tf}\PY{o}{.}\PY{n}{math}\PY{o}{.}\PY{n}{reduce\PYZus{}sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{/} \PY{n}{reshaped\PYZus{}dists}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Participant Exercise: Use this alternative strategy instead:}
  \PY{c+c1}{\PYZsh{} \PYZhy{} Adding a unit matrix to the distances\PYZhy{}matrix}
  \PY{c+c1}{\PYZsh{} \PYZhy{} Inverting and summing}
  \PY{c+c1}{\PYZsh{} \PYZhy{} Subtracting the known contribution coming from the diagonal.}


\PY{c+c1}{\PYZsh{} Let\PYZsq{}s try this out and compute the energy of electrons in tetrahedral}
\PY{c+c1}{\PYZsh{} configuration.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} Here, we will always feed tf\PYZhy{}Tensors to tf\PYZhy{}functions, but having a numpy}
\PY{c+c1}{\PYZsh{} representation of input data is useful, since this simplifies some operations.}
\PY{n}{tetrahedral\PYZus{}arrangement} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}
    \PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,}
    \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{float64}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E(tetrahedral):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{thomson\PYZus{}energy}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{tetrahedral\PYZus{}arrangement}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} If we scale all coordinates, this should not affect the energy, since}
\PY{c+c1}{\PYZsh{} all electron positions get projected onto the unit sphere.}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E(10*tetrahedral):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{thomson\PYZus{}energy}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{10} \PY{o}{*} \PY{n}{tetrahedral\PYZus{}arrangement}\PY{p}{,}
                                 \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Here, we got \PYZdq{}eager\PYZhy{}mode TensorFlow tensors\PYZdq{} back.}
\PY{c+c1}{\PYZsh{} These objects have a .numpy() method which extracts numpy data:}

\PY{n}{numpy\PYZus{}energy} \PY{o}{=}  \PY{n}{thomson\PYZus{}energy}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{tetrahedral\PYZus{}arrangement}\PY{p}{,}
                               \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{repr}\PY{p}{(}\PY{n}{numpy\PYZus{}energy}\PY{p}{)}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Let us actually check that if we perturb the geometry a bit, the energy}
\PY{c+c1}{\PYZsh{} goes up.}

\PY{k}{def} \PY{n+nf}{get\PYZus{}tetrahedral\PYZus{}energy\PYZus{}change\PYZus{}for\PYZus{}random\PYZus{}perturbation}\PY{p}{(}\PY{n}{pert}\PY{p}{)}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} Helpers}
  \PY{n}{tc} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Strictly speaking, unit\PYZus{}normalized() is unnecessary here.}
  \PY{n}{e0} \PY{o}{=} \PY{n}{thomson\PYZus{}energy}\PY{p}{(}\PY{n}{tc}\PY{p}{(}\PY{n}{tetrahedral\PYZus{}arrangement}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
  \PY{n}{e1} \PY{o}{=} \PY{n}{thomson\PYZus{}energy}\PY{p}{(}\PY{n}{tc}\PY{p}{(}\PY{n}{tetrahedral\PYZus{}arrangement} \PY{o}{+} \PY{n}{pert}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
  \PY{k}{return} \PY{n}{e1} \PY{o}{\PYZhy{}} \PY{n}{e0}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{get\PYZus{}tetrahedral\PYZus{}energy\PYZus{}change\PYZus{}for\PYZus{}random\PYZus{}perturbation}\PY{p}{(}
      \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                                              \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Participant Exercise: Check that even with different RNG seeds,}
\PY{c+c1}{\PYZsh{} we will always see an increase in energy if we perturb the tetrahedral}
\PY{c+c1}{\PYZsh{} arrangement. Magnitude\PYZhy{}wise, this increase looks as if it were}
\PY{c+c1}{\PYZsh{} \PYZdq{}to 2nd order in epsilon\PYZdq{}.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
E(tetrahedral): tf.Tensor(3.674234614174767, shape=(), dtype=float64)
E(10*tetrahedral): tf.Tensor(3.674234614174767, shape=(), dtype=float64)
3.674234614174767
1.542903582674171e-10
    \end{Verbatim}

    Let us try to get a look at the actual tensor arithmetics graph for this
little computation. We will use the `tensorboard' tool for that.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} The function to be traced.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} This is just the same computation we defined earlier, but with code comments}
\PY{c+c1}{\PYZsh{} removed and some extra \PYZdq{}with tf.name\PYZus{}scope()\PYZdq{} and constant\PYZhy{}names added.}
\PY{c+c1}{\PYZsh{} The purpose of these is to provide more hierarchical structure for}
\PY{c+c1}{\PYZsh{} tensor\PYZhy{}arithmetic graph visualization with TensorBoard.}
\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{thomson\PYZus{}energy\PYZus{}for\PYZus{}visualization}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}\PY{p}{:}
  \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{name\PYZus{}scope}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normalization}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{projected\PYZus{}electron\PYZus{}coords} \PY{o}{=} \PY{p}{(}
        \PY{n}{electron\PYZus{}coords} \PY{o}{/} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
                                         \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
  \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{name\PYZus{}scope}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{distance\PYZus{}vecs} \PY{o}{=} \PY{p}{(}\PY{n}{projected\PYZus{}electron\PYZus{}coords}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{\PYZhy{}}
                     \PY{n}{projected\PYZus{}electron\PYZus{}coords}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
    \PY{n}{distances} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{distance\PYZus{}vecs}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{name\PYZus{}scope}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reshaping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{num\PYZus{}electrons} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{new\PYZus{}shape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{num\PYZus{}electrons} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}electrons} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
    \PY{n}{reshaped\PYZus{}dists} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{distances}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{new\PYZus{}shape}\PY{p}{)}
  \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{name\PYZus{}scope}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{energy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1/2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{*}
            \PY{n}{tf}\PY{o}{.}\PY{n}{math}\PY{o}{.}\PY{n}{reduce\PYZus{}sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{/} \PY{n}{reshaped\PYZus{}dists}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Set up logging.}
\PY{err}{!}\PY{n}{rm} \PY{o}{\PYZhy{}}\PY{n}{rf} \PY{n}{logs}
\PY{n}{stamp} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{m}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{H}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{M}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{S}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{logdir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{logs/func/}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{stamp}
\PY{n}{writer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{create\PYZus{}file\PYZus{}writer}\PY{p}{(}\PY{n}{logdir}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Sample input data. Should be created ahead\PYZhy{}of\PYZhy{}tracing.}
\PY{n}{example\PYZus{}electron\PYZus{}coords} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Bracketing the function call with}
\PY{c+c1}{\PYZsh{} tf.summary.trace\PYZus{}on() and tf.summary.trace\PYZus{}export().}
\PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{trace\PYZus{}on}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{profiler}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Calling only one tf.function when tracing.}
\PY{k}{try}\PY{p}{:}
  \PY{n}{energy} \PY{o}{=} \PY{n}{thomson\PYZus{}energy\PYZus{}for\PYZus{}visualization}\PY{p}{(}\PY{n}{example\PYZus{}electron\PYZus{}coords}\PY{p}{)}
  \PY{k}{with} \PY{n}{writer}\PY{o}{.}\PY{n}{as\PYZus{}default}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{trace\PYZus{}export}\PY{p}{(}
        \PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{demo\PYZus{}func\PYZus{}trace}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{step}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
        \PY{n}{profiler\PYZus{}outdir}\PY{o}{=}\PY{n}{logdir}\PY{p}{)}
\PY{k}{finally}\PY{p}{:}
  \PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{trace\PYZus{}off}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-
packages/tensorflow/python/ops/summary\_ops\_v2.py:1332: start (from
tensorflow.python.eager.profiler) is deprecated and will be removed after
2020-07-01.
Instructions for updating:
use `tf.profiler.experimental.start` instead.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-
packages/tensorflow/python/ops/summary\_ops\_v2.py:1383: stop (from
tensorflow.python.eager.profiler) is deprecated and will be removed after
2020-07-01.
Instructions for updating:
use `tf.profiler.experimental.stop` instead.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-
packages/tensorflow/python/ops/summary\_ops\_v2.py:1383: save (from
tensorflow.python.eager.profiler) is deprecated and will be removed after
2020-07-01.
Instructions for updating:
`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-
packages/tensorflow/python/eager/profiler.py:150: maybe\_create\_event\_file (from
tensorflow.python.eager.profiler) is deprecated and will be removed after
2020-07-01.
Instructions for updating:
`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}}\PY{n}{tensorboard} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{logdir} \PY{n}{logs}\PY{o}{/}\PY{n}{func}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Javascript object>
    \end{Verbatim}

    
    Let us next do something more interesting - using TensorFlow to turn our
function into another function that computes the gradient of the energy.

Let's then also visualize the graph for that.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{thomson\PYZus{}energy\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}\PY{p}{:}
  \PY{n}{tape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{p}{)}
  \PY{k}{with} \PY{n}{tape}\PY{p}{:}
    \PY{n}{tape}\PY{o}{.}\PY{n}{watch}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}
    \PY{n}{energy} \PY{o}{=} \PY{n}{thomson\PYZus{}energy\PYZus{}for\PYZus{}visualization}\PY{p}{(}\PY{n}{electron\PYZus{}coords}\PY{p}{)}
  \PY{k}{return} \PY{n}{energy}\PY{p}{,} \PY{n}{tape}\PY{o}{.}\PY{n}{gradient}\PY{p}{(}\PY{n}{energy}\PY{p}{,} \PY{n}{electron\PYZus{}coords}\PY{p}{)}


\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Tensorboard graph visualization.}
\PY{n}{stamp} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{m}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{H}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{M}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{S}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{logdir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{logs/grad/}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{stamp}
\PY{n}{writer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{create\PYZus{}file\PYZus{}writer}\PY{p}{(}\PY{n}{logdir}\PY{p}{)}

\PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{trace\PYZus{}on}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{profiler}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{k}{try}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{n}{thomson\PYZus{}energy\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{example\PYZus{}electron\PYZus{}coords}\PY{p}{)}\PY{p}{)}
  \PY{k}{with} \PY{n}{writer}\PY{o}{.}\PY{n}{as\PYZus{}default}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{trace\PYZus{}export}\PY{p}{(}
        \PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{demo\PYZus{}grad\PYZus{}trace}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{step}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
        \PY{n}{profiler\PYZus{}outdir}\PY{o}{=}\PY{n}{logdir}\PY{p}{)}
\PY{k}{finally}\PY{p}{:}
  \PY{n}{tf}\PY{o}{.}\PY{n}{summary}\PY{o}{.}\PY{n}{trace\PYZus{}off}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} The visualization may be a little bit messed up here, but we get the idea.}
\PY{o}{\PYZpc{}}\PY{n}{tensorboard} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{logdir} \PY{n}{logs}\PY{o}{/}\PY{n}{grad}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(<tf.Tensor: shape=(), dtype=float64, numpy=28.790527871655378>, <tf.Tensor:
shape=(5, 3), dtype=float64, numpy=
array([[ 12.91151948,  15.61737417, -18.2504733 ],
       [ -2.00207027, -11.13616609,   9.13746111],
       [ -7.64327007,   2.83715471,  49.863972  ],
       [ 23.05478418,  -9.21406972,  -9.10227686],
       [ -0.93899942,   9.80415117,  -3.73420366]])>)
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Javascript object>
    \end{Verbatim}

    
    \hypertarget{tensorflow-2-and-python}{%
\subsection{TensorFlow 2 and Python}\label{tensorflow-2-and-python}}

A \texttt{@tf.function} decorated function will get analyzed by
TensorFlow2 and turned into some ``callable GPU-ready
graph-representation''. This is a rather complex operation that could
not be implemented if \texttt{tf.function} were a ``normal'' Python
decorator which maps a black-box function to some other function that
merely can call the original function. Rather, \texttt{@tf.function} has
to somehow inspect the definition of the function at the code level.

With Lisp, such program-transformations would be naturally part of the
language, and straightforward since the language basically has no
syntax. This is much less the case with Python, where the corresponding
program-transformation has to run a text parser and then analyze the
parse tree.

One important caveat is that \texttt{@tf.function} decorated code might
not execute as Python code, since the underlying ``computation written
in Python syntax'' uses subtly different semantics from Python. This is
most visibly when putting Python code with side effects under
\texttt{@tf.function}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{tf2\PYZus{}execution\PYZus{}example1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CALLED with x:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x}\PY{p}{)}
  \PY{k}{return} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{x}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example1}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example1}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{11}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example1}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example1}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example1}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Step 1
CALLED with x: Tensor("x:0", shape=(), dtype=float64)
tf.Tensor(20.0, shape=(), dtype=float64)
Step 2
tf.Tensor(22.0, shape=(), dtype=float64)
Step 3
CALLED with x: Tensor("x:0", shape=(), dtype=float32)
tf.Tensor(24.0, shape=(), dtype=float32)
Step 4
tf.Tensor(26.0, shape=(), dtype=float32)
Step 5
tf.Tensor(28.0, shape=(), dtype=float64)
    \end{Verbatim}

    The noteworthy details here are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \texttt{Step\ 1}, when the \texttt{@tf.function} gets called,
  Python code that has side effects (the \texttt{print}) clearly gets
  executed.
\item
  The \texttt{tf.Tensor} object that gets printed apparently does not
  carry a value - at least, it does not report one. (This is not an
  ``eager tensor''.)
\item
  Re-evaluating the same function on different numerical input with the
  same shape and data types as used on an earlier evaluation re-uses the
  ``compiled form'' generated earlier - and since Python statement
  execution that does not somehow have a representation on the value
  dependency graph is invisible to this compiled form, statements with
  side effects are not executed here.
\item
  Evaluating the same decorated function with input of some other type
  (single-precision float rather than double-precision float) will
  produce another GPU-optimized graph, and in order to obtain that,
  Python code gets re-traced, and we get to see another side effect from
  a Python statement.
\item
  Compiled graphs are cached on the callable-function object produced by
  \texttt{@tf.function}, so if we switch back to using the previous data
  type, there already is a compiled representation of the graph
  available, and no re-tracing happens.
\end{enumerate}

Let us compare this behavior with that of a not-\texttt{@tf.function}
decorated equivalent function:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{tf2\PYZus{}execution\PYZus{}example2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CALLED with x:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x}\PY{p}{)}
  \PY{k}{return} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{x}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example2}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example2}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{11}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example2}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example2}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step 5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example2}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Step 1
CALLED with x: tf.Tensor(10.0, shape=(), dtype=float64)
tf.Tensor(20.0, shape=(), dtype=float64)
Step 2
CALLED with x: tf.Tensor(11.0, shape=(), dtype=float64)
tf.Tensor(22.0, shape=(), dtype=float64)
Step 3
CALLED with x: tf.Tensor(12.0, shape=(), dtype=float32)
tf.Tensor(24.0, shape=(), dtype=float32)
Step 4
CALLED with x: tf.Tensor(13.0, shape=(), dtype=float32)
tf.Tensor(26.0, shape=(), dtype=float32)
Step 5
CALLED with x: tf.Tensor(14.0, shape=(), dtype=float64)
tf.Tensor(28.0, shape=(), dtype=float64)
    \end{Verbatim}

    As expected, we observe two things here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Side effects clearly happen at every call, just as we would expect it
  for Python code.
\item
  The tensors reported on the \texttt{CALLED\ with:} lines are shown
  with numerical values:
  \texttt{CALLED\ with\ x:\ tf.Tensor(10.0,\ shape=(),\ dtype=float64)}.
  This was not the case before. In this ``eager'' mode, and only there,
  a \texttt{tf.Tensor} has a \texttt{.numpy()} method which allows us to
  extract its data as a \texttt{numpy.ndarray}.
\end{enumerate}

This Python-code-to-GPU-graph translator has some limitations. It tries
to present itself to us as if it had full understanding of a rather
complicated language, but it actually does not work like a proper
compiler. A true compiler would merely inspect source code and would not
have any need for ``tracing an actual function-invocation''.

A function decorated with \texttt{@tf.function} looks like Python code,
can be parsed into a syntax tree by the Python parser, and often can
even be executed by CPython, but the actual rules about executing such a
function differ from Python. TensorFlow (unlike numpy) allows - for
example - putting a \texttt{tf.Tensor} object as a conditional into an
\texttt{if}-statement:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{tf2\PYZus{}execution\PYZus{}example3}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
  \PY{k}{if} \PY{n}{x} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{:}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{x}
  \PY{k}{return} \PY{n}{x}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example3}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{tf2\PYZus{}execution\PYZus{}example3}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor([20.], shape=(1,), dtype=float64)
[5.]
    \end{Verbatim}

    However, if we \texttt{@tf.function}-decorate a function that does this,
we can encounter situations one would normally consider impossible under
generally accepted notions of valid execution semantics. The problem is
that \texttt{@tf.function} needs to trace-execute code rather than
producing compiler output from mere inspection (it does not understand
the complex Python language standard well enough to do that), and
whenever there are conditionals, the corresponding graph representation
``needs to have translations for all branches''. We see the consequence
of this in the next example:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{boom}\PY{p}{(}\PY{n}{tensor1}\PY{p}{)}\PY{p}{:}
  \PY{n}{c0} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{int32}\PY{p}{)}
  \PY{k}{if} \PY{n}{tensor1}\PY{p}{:}
    \PY{k}{if} \PY{o+ow}{not} \PY{n}{tensor1}\PY{p}{:}
       \PY{c+c1}{\PYZsh{} !!! This is the logically impossible case !!!!}
       \PY{k}{assert} \PY{k+kc}{False}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boom!}\PY{l+s+s1}{\PYZsq{}}
       \PY{k}{return} \PY{n}{c0}
    \PY{k}{else}\PY{p}{:}
       \PY{k}{return} \PY{n}{c0}
  \PY{k}{else}\PY{p}{:}
    \PY{k}{return} \PY{n}{c0}

\PY{n}{boom}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{k+kc}{True}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{bool}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}, frame=single, framerule=2mm, rulecolor=\color{outerrorbackground}]
\textcolor{ansi-red}{---------------------------------------------------------------------------}
\textcolor{ansi-red}{AssertionError}                            Traceback (most recent call last)
\textcolor{ansi-green}{<ipython-input-9-91e0dc041dad>} in \textcolor{ansi-cyan}{<cell line: 14>}\textcolor{ansi-blue}{()}
\textcolor{ansi-green-intense}{\textbf{     12}}     \textcolor{ansi-green}{return} c0
\textcolor{ansi-green-intense}{\textbf{     13}} 
\textcolor{ansi-green}{---> 14}\textcolor{ansi-red}{ }boom\textcolor{ansi-blue}{(}tf\textcolor{ansi-blue}{.}constant\textcolor{ansi-blue}{(}\textcolor{ansi-green}{True}\textcolor{ansi-blue}{,} dtype\textcolor{ansi-blue}{=}tf\textcolor{ansi-blue}{.}bool\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{)}

\textcolor{ansi-green}{/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback\_utils.py} in \textcolor{ansi-cyan}{error\_handler}\textcolor{ansi-blue}{(*args, **kwargs)}
\textcolor{ansi-green-intense}{\textbf{    151}}     \textcolor{ansi-green}{except} Exception \textcolor{ansi-green}{as} e\textcolor{ansi-blue}{:}
\textcolor{ansi-green-intense}{\textbf{    152}}       filtered\_tb \textcolor{ansi-blue}{=} \_process\_traceback\_frames\textcolor{ansi-blue}{(}e\textcolor{ansi-blue}{.}\_\_traceback\_\_\textcolor{ansi-blue}{)}
\textcolor{ansi-green}{--> 153}\textcolor{ansi-red}{       }\textcolor{ansi-green}{raise} e\textcolor{ansi-blue}{.}with\_traceback\textcolor{ansi-blue}{(}filtered\_tb\textcolor{ansi-blue}{)} \textcolor{ansi-green}{from} \textcolor{ansi-green}{None}
\textcolor{ansi-green-intense}{\textbf{    154}}     \textcolor{ansi-green}{finally}\textcolor{ansi-blue}{:}
\textcolor{ansi-green-intense}{\textbf{    155}}       \textcolor{ansi-green}{del} filtered\_tb

\textcolor{ansi-green}{/tmp/\_\_autograph\_generated\_filewnozn99i.py} in \textcolor{ansi-cyan}{tf\_\_boom}\textcolor{ansi-blue}{(tensor1)}
\textcolor{ansi-green-intense}{\textbf{     55}}                         do\_return \textcolor{ansi-blue}{=} \textcolor{ansi-green}{False}
\textcolor{ansi-green-intense}{\textbf{     56}}                         \textcolor{ansi-green}{raise}
\textcolor{ansi-green}{---> 57}\textcolor{ansi-red}{                 }ag\_\_\textcolor{ansi-blue}{.}if\_stmt\textcolor{ansi-blue}{(}ag\_\_\textcolor{ansi-blue}{.}ld\textcolor{ansi-blue}{(}tensor1\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{,} if\_body\_1\textcolor{ansi-blue}{,} else\_body\_1\textcolor{ansi-blue}{,} get\_state\_1\textcolor{ansi-blue}{,} set\_state\_1\textcolor{ansi-blue}{,} \textcolor{ansi-blue}{(}\textcolor{ansi-blue}{'do\_return'}\textcolor{ansi-blue}{,} \textcolor{ansi-blue}{'retval\_'}\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{,} \textcolor{ansi-cyan}{2}\textcolor{ansi-blue}{)}
\textcolor{ansi-green-intense}{\textbf{     58}}                 \textcolor{ansi-green}{return} fscope\textcolor{ansi-blue}{.}ret\textcolor{ansi-blue}{(}retval\_\textcolor{ansi-blue}{,} do\_return\textcolor{ansi-blue}{)}
\textcolor{ansi-green-intense}{\textbf{     59}}         \textcolor{ansi-green}{return} tf\_\_boom

\textcolor{ansi-green}{/tmp/\_\_autograph\_generated\_filewnozn99i.py} in \textcolor{ansi-cyan}{if\_body\_1}\textcolor{ansi-blue}{()}
\textcolor{ansi-green-intense}{\textbf{     45}}                             do\_return \textcolor{ansi-blue}{=} \textcolor{ansi-green}{False}
\textcolor{ansi-green-intense}{\textbf{     46}}                             \textcolor{ansi-green}{raise}
\textcolor{ansi-green}{---> 47}\textcolor{ansi-red}{                     }ag\_\_\textcolor{ansi-blue}{.}if\_stmt\textcolor{ansi-blue}{(}ag\_\_\textcolor{ansi-blue}{.}not\_\textcolor{ansi-blue}{(}ag\_\_\textcolor{ansi-blue}{.}ld\textcolor{ansi-blue}{(}tensor1\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{,} if\_body\textcolor{ansi-blue}{,} else\_body\textcolor{ansi-blue}{,} get\_state\textcolor{ansi-blue}{,} set\_state\textcolor{ansi-blue}{,} \textcolor{ansi-blue}{(}\textcolor{ansi-blue}{'do\_return'}\textcolor{ansi-blue}{,} \textcolor{ansi-blue}{'retval\_'}\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{,} \textcolor{ansi-cyan}{2}\textcolor{ansi-blue}{)}
\textcolor{ansi-green-intense}{\textbf{     48}} 
\textcolor{ansi-green-intense}{\textbf{     49}}                 \textcolor{ansi-green}{def} else\_body\_1\textcolor{ansi-blue}{(}\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{:}

\textcolor{ansi-green}{/tmp/\_\_autograph\_generated\_filewnozn99i.py} in \textcolor{ansi-cyan}{if\_body}\textcolor{ansi-blue}{()}
\textcolor{ansi-green-intense}{\textbf{     29}}                     \textcolor{ansi-green}{def} if\_body\textcolor{ansi-blue}{(}\textcolor{ansi-blue}{)}\textcolor{ansi-blue}{:}
\textcolor{ansi-green-intense}{\textbf{     30}}                         \textcolor{ansi-green}{nonlocal} retval\_\textcolor{ansi-blue}{,} do\_return
\textcolor{ansi-green}{---> 31}\textcolor{ansi-red}{                         }\textcolor{ansi-green}{assert} \textcolor{ansi-green}{False}\textcolor{ansi-blue}{,} \textcolor{ansi-blue}{'Boom!'}
\textcolor{ansi-green-intense}{\textbf{     32}}                         \textcolor{ansi-green}{try}\textcolor{ansi-blue}{:}
\textcolor{ansi-green-intense}{\textbf{     33}}                             do\_return \textcolor{ansi-blue}{=} \textcolor{ansi-green}{True}

\textcolor{ansi-red}{AssertionError}: in user code:

    File "<ipython-input-9-91e0dc041dad>", line 7, in boom  *
        assert False, 'Boom!'

    AssertionError: Boom!

    \end{Verbatim}

    When writing low-level TensorFlow code, the author generally follows
these rules which help a lot to avoid problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Do not interactively redefine functions while some TF code is running.
  (Some versions of TensorFlow identify functions by source-location,
  and changing these live can have unintended consequences.)
\item
  Use TensorFlow functions such as \texttt{tf.cond()},
  \texttt{tf.while\_loop()}, etc. rather than putting tensors into
  Python-level control flow statements (even if that would work).
\item
  Never mix tensors and non-tensor quantities, such as by using function
  signatures that state that a function may accept either a
  numpy-ndarray or alternatively also a \texttt{tf.Tensor}. Always
  maintain clarity about what is a \texttt{tf.Tensor} and what not.
\item
  Things having tensor input or output should have very simple call
  signatures.
\item
  Be mindful that \texttt{@tf.function} decorated functions are stateful
  objects that hold on to cached compiled versions. Despite being
  callable and also (generally) functionally-pure, it often is more
  appropriate to think about them as stateful objects rather than ``pure
  functions''.
\item
  Be aware that \texttt{tf.where(c,\ x,\ y)} is implemented as
  equivalent to:
\end{enumerate}

\begin{verbatim}
def tf_where(c, x, y):
  c1 = tf.cast(tf.cast(c, tf.bool), x.dtype)
  return c1 * x + (1 - c1) * y
\end{verbatim}

\ldots which means that a \texttt{NaN}-gradient even on a not-taken
branch will percolate as a \texttt{NaN}.

    \hypertarget{some-physics}{%
\subsection{Some Physics}\label{some-physics}}

Let's actually do something with all this knowledge. Let's wire it all
up such that we can use \texttt{scipy.optimize.fmin\_bfgs} to determine
the minimal-energy configuration of four electrons on a sphere.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}min\PYZus{}energy\PYZus{}config\PYZus{}n\PYZus{}electrons}\PY{p}{(}\PY{n}{num\PYZus{}electrons}\PY{p}{)}\PY{p}{:}
  \PY{n}{x0} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{num\PYZus{}electrons}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{as\PYZus{}tf\PYZus{}input}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x0}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}
  \PY{n}{num\PYZus{}call\PYZus{}to\PYZus{}f} \PY{o}{=} \PY{l+m+mi}{0}
  \PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{nonlocal} \PY{n}{num\PYZus{}call\PYZus{}to\PYZus{}f}
    \PY{n}{result} \PY{o}{=} \PY{n}{thomson\PYZus{}energy\PYZus{}for\PYZus{}visualization}\PY{p}{(}\PY{n}{as\PYZus{}tf\PYZus{}input}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    \PY{k}{if} \PY{n}{num\PYZus{}call\PYZus{}to\PYZus{}f} \PY{o}{\PYZpc{}} \PY{l+m+mi}{5} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E: }\PY{l+s+si}{\PYZob{}}\PY{n}{result}\PY{l+s+si}{:}\PY{l+s+s1}{16.12f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{num\PYZus{}call\PYZus{}to\PYZus{}f} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{result}
  \PY{k}{def} \PY{n+nf}{fprime}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{t\PYZus{}grad} \PY{o}{=} \PY{n}{thomson\PYZus{}energy\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{as\PYZus{}tf\PYZus{}input}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{t\PYZus{}grad}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
  \PY{n}{opt\PYZus{}config} \PY{o}{=} \PY{n}{scipy}\PY{o}{.}\PY{n}{optimize}\PY{o}{.}\PY{n}{fmin\PYZus{}bfgs}\PY{p}{(}
      \PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{fprime}\PY{o}{=}\PY{n}{fprime}\PY{p}{,}
      \PY{n}{gtol}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{,} \PY{n}{maxiter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x0}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Return energy and unit\PYZhy{}normalized vectors.}
  \PY{k}{return} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{opt\PYZus{}config}\PY{p}{)}\PY{p}{,}
          \PY{n}{opt\PYZus{}config} \PY{o}{/} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{opt\PYZus{}config}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}


\PY{n}{opt\PYZus{}energy\PYZus{}8}\PY{p}{,} \PY{n}{opt\PYZus{}config\PYZus{}8} \PY{o}{=} \PY{n}{get\PYZus{}min\PYZus{}energy\PYZus{}config\PYZus{}n\PYZus{}electrons}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{distance\PYZus{}statistics} \PY{o}{=} \PY{n}{collections}\PY{o}{.}\PY{n}{Counter}\PY{p}{(}
    \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{opt\PYZus{}config\PYZus{}8}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{\PYZhy{}}
                      \PY{n}{opt\PYZus{}config\PYZus{}8}\PY{p}{[}\PY{n}{numpy}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
                      \PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{sorted}\PY{p}{(}\PY{p}{(}\PY{n}{d}\PY{p}{,} \PY{n}{n} \PY{o}{/} \PY{l+m+mi}{8}\PY{p}{)} \PY{k}{for} \PY{n}{d}\PY{p}{,} \PY{n}{n} \PY{o+ow}{in} \PY{n}{distance\PYZus{}statistics}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
E:  43.741776338794
E:  19.847570989142
E:  19.684686490780
E:  19.681629891495
E:  19.681580175157
E:  19.681330781762
E:  19.678334046533
E:  19.675773843023
E:  19.675289007551
E:  19.675287861456
E:  19.675287861233
Warning: Desired error not necessarily achieved due to precision loss.
         Current function value: 19.675288
         Iterations: 50
         Function evaluations: 53
         Gradient evaluations: 53
[(0.0, 1.0), (1.17125, 2.0), (1.28769, 2.0), (1.65639, 1.0), (1.89689, 2.0)]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let\PYZsq{}s check how far we can take this.}
\PY{n}{opt\PYZus{}energy\PYZus{}20}\PY{p}{,} \PY{n}{opt\PYZus{}config\PYZus{}20} \PY{o}{=} \PY{n}{get\PYZus{}min\PYZus{}energy\PYZus{}config\PYZus{}n\PYZus{}electrons}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{)}
\PY{c+c1}{\PYZsh{} opt\PYZus{}energy\PYZus{}200, opt\PYZus{}config\PYZus{}200 = get\PYZus{}min\PYZus{}energy\PYZus{}config\PYZus{}n\PYZus{}electrons(200)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
E: 189.071509811076
E: 154.619941615551
E: 151.392468902219
E: 151.188079702750
E: 151.053550913645
E: 150.974088557642
E: 150.938160338286
E: 150.895697768632
E: 150.885305939055
E: 150.883374417498
E: 150.883210810365
E: 150.882725336300
E: 150.881636086169
E: 150.881574033464
E: 150.881568539280
E: 150.881568337436
E: 150.881568333757
Warning: Desired error not necessarily achieved due to precision loss.
         Current function value: 150.881568
         Iterations: 78
         Function evaluations: 84
         Gradient evaluations: 84
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} For 8 electrons, this printed:}
\PY{c+c1}{\PYZsh{} [(0.0, 1.0), (1.17125, 2.0), (1.28769, 2.0), (1.65639, 1.0), (1.89689, 2.0)]}
\PY{c+c1}{\PYZsh{} So, for any given electron,}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} \PYZhy{} there is one electron at zero\PYZhy{}distance.}
\PY{c+c1}{\PYZsh{} \PYZhy{} there are two nearest neighbors...}
\PY{c+c1}{\PYZsh{} \PYZhy{} two next\PYZhy{}nearest neighbors...}
\PY{c+c1}{\PYZsh{} \PYZhy{} one 3rd\PYZhy{}nearest\PYZhy{}neighbor,}
\PY{c+c1}{\PYZsh{} \PYZhy{} and two \PYZdq{}farthest\PYZhy{}neighbors\PYZdq{}.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} This is not compatible with electrons sitting on the corners of a cube,}
\PY{c+c1}{\PYZsh{} where there would be 3 nearest\PYZhy{}neighbors. Did we do anything wrong?}
\PY{c+c1}{\PYZsh{} Only if we assumed that for 8 electrons, cubic arrangement had lowest\PYZhy{}energy.}
\PY{c+c1}{\PYZsh{} What is the energy of a cubic arrangement?}

\PY{k+kn}{import} \PY{n+nn}{itertools}

\PY{n}{cube\PYZus{}corners} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{itertools}\PY{o}{.}\PY{n}{product}\PY{p}{(}\PY{o}{*}\PY{p}{[}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cube\PYZus{}corners}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E(cube):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{thomson\PYZus{}energy\PYZus{}for\PYZus{}visualization}\PY{p}{(}
          \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{cube\PYZus{}corners}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E\PYZus{}min:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{opt\PYZus{}energy\PYZus{}8}\PY{p}{)}

\PY{c+c1}{\PYZsh{} So... indeed, our minimal\PYZhy{}energy configuration has lower energy as the one}
\PY{c+c1}{\PYZsh{} with cubic symmetry.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[(-1, -1, -1), (-1, -1, 1), (-1, 1, -1), (-1, 1, 1), (1, -1, -1), (1, -1, 1),
(1, 1, -1), (1, 1, 1)]
E(cube): 19.7407740737628 E\_min: 19.67528786123276
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Incidentally... we still have OpenGL loaded. Let\PYZsq{}s make good use of it.}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{spatial}

\PY{n}{delaunay} \PY{o}{=} \PY{n}{scipy}\PY{o}{.}\PY{n}{spatial}\PY{o}{.}\PY{n}{Delaunay}\PY{p}{(}\PY{n}{opt\PYZus{}config\PYZus{}8}\PY{p}{)}
\PY{n}{tetrahedra} \PY{o}{=} \PY{n}{delaunay}\PY{o}{.}\PY{n}{simplices}

\PY{n}{count\PYZus{}by\PYZus{}surface} \PY{o}{=} \PY{n}{collections}\PY{o}{.}\PY{n}{Counter}\PY{p}{(}
    \PY{n+nb}{frozenset}\PY{p}{(}\PY{n}{tet}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{tet}\PY{p}{[}\PY{n}{n} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)} \PY{k}{for} \PY{n}{tet} \PY{o+ow}{in} \PY{n}{tetrahedra}\PY{p}{)}

\PY{n}{outer\PYZus{}faces} \PY{o}{=} \PY{p}{[}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{face}\PY{p}{)} \PY{k}{for} \PY{n}{face}\PY{p}{,} \PY{n}{count} \PY{o+ow}{in}
               \PY{n}{count\PYZus{}by\PYZus{}surface}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{count} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{outer\PYZus{}faces}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{gl\PYZus{}wireframe}\PY{p}{(}\PY{n}{filename}\PY{p}{,} \PY{n}{points}\PY{p}{,} \PY{n}{triangles}\PY{p}{)}\PY{p}{:}
  \PY{n}{width}\PY{p}{,} \PY{n}{height} \PY{o}{=} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{400}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutInit}\PY{p}{(}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutInitDisplayMode}\PY{p}{(}\PY{n}{glut}\PY{o}{.}\PY{n}{GLUT\PYZus{}DOUBLE} \PY{o}{|} \PY{n}{glut}\PY{o}{.}\PY{n}{GLUT\PYZus{}RGB}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutInitWindowSize}\PY{p}{(}\PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutCreateWindow}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutHideWindow}\PY{p}{(}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glClearColor}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glViewport}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glMatrixMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}PROJECTION}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glLoadIdentity}\PY{p}{(}\PY{p}{)}
  \PY{n}{glu}\PY{o}{.}\PY{n}{gluPerspective}\PY{p}{(}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{n}{width} \PY{o}{/} \PY{n}{height}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1000.0}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glMatrixMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}MODELVIEW}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glLoadIdentity}\PY{p}{(}\PY{p}{)}
  \PY{n}{glu}\PY{o}{.}\PY{n}{gluLookAt}\PY{p}{(}\PY{l+m+mf}{6.0}\PY{p}{,} \PY{l+m+mf}{40.0}\PY{p}{,} \PY{l+m+mf}{15.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glClear}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}COLOR\PYZus{}BUFFER\PYZus{}BIT} \PY{o}{|} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}DEPTH\PYZus{}BUFFER\PYZus{}BIT}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glPolygonMode}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}FRONT\PYZus{}AND\PYZus{}BACK}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}LINE}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glColor3f}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glBegin}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}TRIANGLES}\PY{p}{)}
  \PY{k}{for} \PY{n}{triangle} \PY{o+ow}{in} \PY{n}{triangles}\PY{p}{:}
    \PY{k}{for} \PY{n}{num\PYZus{}point} \PY{o+ow}{in} \PY{n}{triangle}\PY{p}{:}
      \PY{n}{gl}\PY{o}{.}\PY{n}{glVertex3f}\PY{p}{(}\PY{o}{*}\PY{n}{points}\PY{p}{[}\PY{n}{num\PYZus{}point}\PY{p}{]}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glEnd}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{n}{glut}\PY{o}{.}\PY{n}{glutSwapBuffers}\PY{p}{(}\PY{p}{)}
  \PY{n}{gl}\PY{o}{.}\PY{n}{glPixelStorei}\PY{p}{(}\PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}PACK\PYZus{}ALIGNMENT}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{data} \PY{o}{=} \PY{n}{gl}\PY{o}{.}\PY{n}{glReadPixels}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,}
                         \PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{,}
                         \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}RGB}\PY{p}{,} \PY{n}{gl}\PY{o}{.}\PY{n}{GL\PYZus{}UNSIGNED\PYZus{}BYTE}\PY{p}{)}
  \PY{n}{image} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{frombytes}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RGB}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{p}{(}\PY{n}{width}\PY{p}{,} \PY{n}{height}\PY{p}{)}\PY{p}{,} \PY{n}{data}\PY{p}{)}
  \PY{n}{image}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{filename}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PNG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{n}{gl\PYZus{}wireframe}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wire8.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{opt\PYZus{}config\PYZus{}8}\PY{p}{,} \PY{n}{outer\PYZus{}faces}\PY{p}{)}


\PY{n}{pyplot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{mpimage}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wire8.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[3, 5, 7], [0, 3, 5], [5, 6, 7], [0, 1, 5], [4, 6, 7], [1, 5, 6], [3, 4, 7],
[0, 1, 2], [0, 2, 3], [1, 2, 6], [2, 4, 6], [2, 3, 4]]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The shape we get sort-of looks like a cube where we twisted the top
square by 45 degrees relative to the bottom, and then allowed these
squares to move a bit up/down on the sphere. Here, we render triangles,
so these two squares are divided into two triangles. All the other faces
are triangles.

Compare e.g.~with:
\url{https://tracer.lcc.uma.es/problems/thomson/thomson.html}

\textbf{Participant Exercise}: Work out corresponding shapes for more
electrons. It might be helpful to tweak the energy-function with an
extra spurious term that punishes electrons moving in 3d to have
distance-from-origin-prior-to-normalization that is off of the desired
value 1 by more than 1/2.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    When writing such code, we might have to do some debugging. Some general
tips around that:

\begin{itemize}
\item
  \texttt{@tf.function} decorated functions will be processed by
  \texttt{autograph} and have their logic turned into TensorFlow graphs
  that get executed instead of the Python code after first use (there
  are some complicated rules around this). When debugging, it often
  makes sense to instead stick with pure Python code manipulating
  ``eager tensors'' (basically, TF-wrapped numpy arrays). This achieves
  these things:

  \begin{itemize}
  \tightlist
  \item
    \texttt{print()} statements will get executed on every invocation.
  \item
    We can actually call the \texttt{.numpy()} method on tensor-objects.
    (For code that gets autograph-translated, tf-tensors are ``non-eager
    tensors'', so ``abstract elements on a graph'' which do not have an
    associated numerical value.)
  \item
    We can use the Python debugger (via \texttt{pdb.pm()} -
    \href{https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.06-Errors-and-Debugging.ipynb}{details
    on how to do this in Colab}) to inspect stacktraces.
  \end{itemize}
\item
  One somewhat convenient way to ``have it both ways'' is this:

\begin{verbatim}
import os

_DEFAULT_USE_AUTOGRAPH = 0

# Identity-decorator if env var `TF_USE_AUTOGRAPH` is unset and
# default is to not use it, or if `TF_USE_AUTOGRAPH` is set to
# anything else but '1'. Otherwise, equals tf.function.
maybe_tf_function = (
  tf.function if os.getenv('TF_USE_AUTOGRAPH',
                           str(_DEFAULT_USE_AUTOGRAPH)) == '1'
  else lambda f: f)

...

@maybe_tf_function
def my_function(...):
  ...
\end{verbatim}
\end{itemize}

    \hypertarget{rendering-a-black-hole-with-tensorflow}{%
\subsection{Rendering a Black Hole with
TensorFlow}\label{rendering-a-black-hole-with-tensorflow}}

Since we now have all the bits and pieces together, let's actually build
a TensorFlow-powered geodesic renderer for arbitrary Riemannian
geometries - as an application, we want to do some basic raytracer-style
rendering of a black hole.

This is a somewhat advanced exercise in ``wiring up low-level TensorFlow
in a more complicated calculation'', which showcases many important
techniques.

While this example in some sense is an abuse of TensorFlow, since this
framework was not built to efficiently handle ``small'' tensors and
shuttle them back and forth a lot between \texttt{numpy.ndarray} and
\texttt{tf.Tensor}, it nevertheless allows us to discuss many
interesting aspects. If one wanted to go for best possible achievable
computational performance, this might not be a good approach, but as an
illustration for how ML tools offer us new pathways to quickly do
exploration on things that would be considerable effort without them,
this certainly is interesting.

The example we are discussing in this section works in two directions -
physicists who are familiar with the theory will be able to hold on to
their theoretical knowledge and see the example as an illustration for
how to accomplish rather nontrivial things with libraries that can also
power Machine Learning. In the other direction, Machine Learning
practitioners will be able to understand the wiring and hopefully be
able to get a glimpse of how physics works in curved spacetime by
studying a very concrete example in a way that is fully spelled out,
with explicit numbers. In a way, our approach here allows course
participants to experience the physics of black holes in yet another way
that does not involve having to map complicated analytic expressions to
geometric ideas.

Overall, if we wanted simply to render some specific geometry, such as
Schwarzschild spacetime, there of course would be extra tricks available
that should be exploited. There indeed by now are interactive WebGL
demos that allow live rendering of motion in a black hole geometry
directly in the browser - such as
\href{https://www.esa.int/gsp/ACT/phy/Projects/Blackholes/WebGL/}{this
one by ESA}.

Our aspirations here are different - we want to start from a simple
\texttt{@tf.function} that computationally describes the local
distance-function, then use TensorFlow to backpropagate this twice, in
order to find the spacetime metric. Then, we backpropagate once more to
get the Christoffel symbols. Next, we stick this into a very simple
ODE-solver that can trace out batches of geodesics and think about how
to then use this to render some objects.

Overall, some things are noteworthy about this approach:

\begin{itemize}
\item
  We will backpropagate through numerical matrix inversion or
  linear-equation-system-solving (since we use the inverse metric). This
  may be doubtful to do in general, since in many situations where we
  have a good ``analytic formula'' description of the metric, working
  out the analytic expression may be substantually less effort.
\item
  We have seen that sensitivity-backpropagation is generally useful for
  5+ parameters. Here, with only four spacetime coordinates,
  forward-mode AD might actually be a better choice. So, this is more
  about a ``finger exercise'' to show how we can wire up something like
  this rather than the overall best possible design.
\item
  In order to make good use of the silicon on a GPU, we will process a
  collection of geodesics all at once. So, we generally want our
  tensor-arithmetic operations to be ``batched''. We have seen
  batch-indices in earlier ML related examples.

  Here, the presence of batch-indices leads to a strange interplay
  between the notion of ``Einstein Summation'' generally used in
  mathematics, and the (at first doubtful-looking) generalization widely
  used in Machine Learning.

  The rules of the game are codified in the behavior of the
  \texttt{numpy.einsum()} and \texttt{tf.einsum()} functions: for a
  summation prescription such as `ab,ac-\textgreater bc', the rule is:
  ``Each combination of right hand side indices specifies a cell. All
  left hand side indices not present on the right hand side get summed
  over.'' This means that we can specify broken-looking ``Einstein
  Summations'' such as
  \texttt{numpy.einsum(\textquotesingle{}i-\textgreater{}\textquotesingle{},\ vector)}
  to get the sum of elements of a vector, or
  \texttt{numpy.einsum(\textquotesingle{}ba,ba-\textgreater{}b\textquotesingle{},\ vecs1,\ vecs2)}
  to compute a batch of scalar products for two batches of vectors (the
  batch-index being \texttt{b} here). Even more, there is special
  notation in such ``generalized Einstein summation'' for ``a group of
  batch indices'':
  \texttt{numpy.einsum(\textquotesingle{}...a,...a-\textgreater{}...\textquotesingle{},\ m,\ m)}
  would compute a batched-scalar-product where we may even have, say,
  two batch-indices, such as for ``image-row'' and ``image-column'',
  treating a rectangle of pixels as a doubly-indexed batch.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Below, \PYZdq{}tfb\PYZus{}\PYZdq{} is used as a short\PYZhy{}hand for a batch\PYZhy{}tf.Tensor\PYZhy{}to\PYZhy{}batch\PYZhy{}tf.Tensor}
\PY{c+c1}{\PYZsh{} graph\PYZhy{}compiled TensorFlow function.}

\PY{c+c1}{\PYZsh{} Helper.}
\PY{k}{def} \PY{n+nf}{tff64}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}


\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{tfb\PYZus{}minkowski\PYZus{}ds2}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{,} \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Batched TensorFlow Minkowski Spacetime local scalar product.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...a,...a,a\PYZhy{}\PYZgt{}...}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                   \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{,}
                   \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{,}
                   \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Let\PYZsq{}s see how this works... Batch\PYZhy{}scalar\PYZhy{}product of a batch of 3 vectors,}
\PY{c+c1}{\PYZsh{} the first timelike, the second spacelike, the third null.}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tfb\PYZus{}minkowski\PYZus{}ds2}\PY{p}{(}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Arbitrary position.}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[-1.  1.  0.]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us define Schwarzschild geometry. Slight trick here: Should we ever}
\PY{c+c1}{\PYZsh{} evaluate this for r \PYZlt{}= M, we make the result \PYZdq{}Not\PYZhy{}a\PYZhy{}Number\PYZdq{}, as a canary for}
\PY{c+c1}{\PYZsh{} \PYZdq{}this calculation accidentally entered forbidden territory due to}
\PY{c+c1}{\PYZsh{} limited\PYZhy{}accuracy effects.\PYZdq{}}

\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{tfb\PYZus{}schwarzschild\PYZus{}ds2}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{,} \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Batched TensorFlow Schwarzschild Spacetime local scalar product.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{r\PYZus{}s} \PY{o}{=} \PY{n}{tff64}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Let\PYZsq{}s use R\PYZus{}Schwarzschild = 1 here.}
  \PY{n}{tb\PYZus{}pos\PYZus{}xyz} \PY{o}{=} \PY{n}{tb\PYZus{}pos}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
  \PY{n}{tb\PYZus{}r} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{tb\PYZus{}pos\PYZus{}xyz}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} We need the radial unit vector.}
  \PY{n}{tb\PYZus{}e\PYZus{}r} \PY{o}{=} \PY{n}{tb\PYZus{}pos\PYZus{}xyz} \PY{o}{/} \PY{n}{tb\PYZus{}r}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
  \PY{n}{tb\PYZus{}dr} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...i,...i,...k\PYZhy{}\PYZgt{}...k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{tb\PYZus{}e\PYZus{}r}\PY{p}{,} \PY{n}{tb\PYZus{}e\PYZus{}r}\PY{p}{)}
  \PY{n}{db\PYZus{}d\PYZus{}rperp} \PY{o}{=} \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{tb\PYZus{}dr} \PY{c+c1}{\PYZsh{} \PYZdq{}perpendicular\PYZhy{}to\PYZhy{}radial\PYZdq{}.}
  \PY{c+c1}{\PYZsh{} Geometry is simple in spacelike\PYZhy{}perpendicular direction.}
  \PY{n}{ds2\PYZus{}rperp} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...i,...i\PYZhy{}\PYZgt{}...}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{db\PYZus{}d\PYZus{}rperp}\PY{p}{,} \PY{n}{db\PYZus{}d\PYZus{}rperp}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} The \PYZdq{}Schwarzschild factor\PYZdq{}}
  \PY{n}{s\PYZus{}factor} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{tb\PYZus{}r} \PY{o}{\PYZgt{}} \PY{n}{r\PYZus{}s}\PY{p}{,}
                      \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{r\PYZus{}s} \PY{o}{/} \PY{n}{tb\PYZus{}r}\PY{p}{,}
                      \PY{n}{tff64}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{nan}\PY{p}{)}\PY{p}{)}
  \PY{n}{ds2\PYZus{}radial} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...i,...i,...\PYZhy{}\PYZgt{}...}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dr}\PY{p}{,} \PY{n}{tb\PYZus{}dr}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{s\PYZus{}factor}\PY{p}{)}
  \PY{n}{ds2\PYZus{}time} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...,...,...\PYZhy{}\PYZgt{}...}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                        \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{tb\PYZus{}d\PYZus{}txyz}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{s\PYZus{}factor}\PY{p}{)}
  \PY{k}{return} \PY{n}{ds2\PYZus{}time} \PY{o}{+} \PY{n}{ds2\PYZus{}radial} \PY{o}{+} \PY{n}{ds2\PYZus{}rperp}


\PY{c+c1}{\PYZsh{} Let\PYZsq{}s see how this works... Batch\PYZhy{}scalar\PYZhy{}product of a batch of 3 vectors,}
\PY{c+c1}{\PYZsh{} the first timelike, the second spacelike, the third null.}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== Batch of x\PYZhy{}directed line\PYZhy{}elements at increasing x\PYZhy{}coordinate. ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}ds2}\PY{p}{(}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{1.01}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Just above R\PYZus{}s}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}   \PY{c+c1}{\PYZsh{} Inside the black hole.}
           \PY{p}{]}\PY{p}{)}\PY{p}{,}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== Batch of y\PYZhy{}directed line\PYZhy{}elements at increasing x\PYZhy{}coordinate. ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}ds2}\PY{p}{(}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{1.01}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Just above R\PYZus{}s}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}   \PY{c+c1}{\PYZsh{} Inside the black hole.}
           \PY{p}{]}\PY{p}{)}\PY{p}{,}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== Batch of t\PYZhy{}directed line\PYZhy{}elements at increasing x\PYZhy{}coordinate. ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}ds2}\PY{p}{(}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{1.01}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Just above R\PYZus{}s}
           \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}   \PY{c+c1}{\PYZsh{} Inside the black hole.}
           \PY{p}{]}\PY{p}{)}\PY{p}{,}
    \PY{n}{tff64}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
=== Batch of x-directed line-elements at increasing x-coordinate. ===
[  3.           1.25         1.11111111   1.001001   101.
          nan]
=== Batch of y-directed line-elements at increasing x-coordinate. ===
[ 1.  1.  1.  1.  1. nan]
=== Batch of t-directed line-elements at increasing x-coordinate. ===
[-0.33333333 -0.8        -0.9        -0.999      -0.00990099         nan]
    \end{Verbatim}

    Note that this last
scalar-product-of-t-directed-line-elements-with-themselves tells us
that, for one unit of coordinate-time passing, there is the less
physical time that is passing the closer we are to the black hole. (We
of course need to take the square-root-of-magnitude of these numbers
here, since we here see the ``\(dT^2\)''.)

Our next task is to obtain a ``metric field'', given the
spacetime-position-dependent distance-squared. This is doable, but
overall quite a bit messy.

Likely, we would be almost always better off here handling this step
symbolically, but let us nevertheless see how this can be accomplished.

In general, we would here want to ask the user to provide a hand-coded
metric-field-function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{\PYZus{}DEFAULT\PYZus{}USE\PYZus{}TF\PYZus{}FUNCTION} \PY{o}{=} \PY{k+kc}{True}


\PY{k}{def} \PY{n+nf}{maybe\PYZus{}tf\PYZus{}function}\PY{p}{(}\PY{n}{use\PYZus{}tf\PYZus{}function}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{function} \PY{k}{if} \PY{n}{use\PYZus{}tf\PYZus{}function} \PY{k}{else} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}


\PY{c+c1}{\PYZsh{} Slight wart: This function only allows a single batch\PYZhy{}index.}
\PY{c+c1}{\PYZsh{} This is not much of a restriction, since we can wrap functions up}
\PY{c+c1}{\PYZsh{} into reshaping\PYZhy{}to\PYZhy{}one\PYZhy{}batch\PYZhy{}index\PYZhy{}and\PYZhy{}back.}
\PY{k}{def} \PY{n+nf}{tfb\PYZus{}metric\PYZus{}fn}\PY{p}{(}\PY{n}{tfb\PYZus{}ds2}\PY{p}{,} \PY{n}{use\PYZus{}tf\PYZus{}function}\PY{o}{=}\PY{n}{\PYZus{}DEFAULT\PYZus{}USE\PYZus{}TF\PYZus{}FUNCTION}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Given a batched\PYZhy{}ds2 function, return a batched\PYZhy{}metric function.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} The metric is half the Hessian of the ds2\PYZhy{}function.}
  \PY{n}{uc\PYZus{}zero} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{UnconnectedGradients}\PY{o}{.}\PY{n}{ZERO}
  \PY{n+nd}{@maybe\PYZus{}tf\PYZus{}function}\PY{p}{(}\PY{n}{use\PYZus{}tf\PYZus{}function}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{tfb\PYZus{}metric}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} First, we compute the ds2 for a zero\PYZhy{}displacement ds=0.}
    \PY{c+c1}{\PYZsh{} We of course know this to be zero, but what matters is the}
    \PY{c+c1}{\PYZsh{} functional dependency, i.e. if we twice\PYZhy{}backprop this to get}
    \PY{c+c1}{\PYZsh{} the Hessian, we obtain the metric.}
    \PY{n}{tcb\PYZus{}ds0} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}
    \PY{n}{tape1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{p}{)}
    \PY{k}{with} \PY{n}{tape1}\PY{p}{:}
      \PY{n}{tape1}\PY{o}{.}\PY{n}{watch}\PY{p}{(}\PY{n}{tcb\PYZus{}ds0}\PY{p}{)}
      \PY{n}{tape0} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{n}{persistent}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
      \PY{k}{with} \PY{n}{tape0}\PY{p}{:}
        \PY{n}{tape0}\PY{o}{.}\PY{n}{watch}\PY{p}{(}\PY{n}{tcb\PYZus{}ds0}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Pretend tb\PYZus{}ds2 is batched and 1d\PYZhy{}vector\PYZhy{}valued,}
        \PY{c+c1}{\PYZsh{} since we need to take a Jacobian.}
        \PY{c+c1}{\PYZsh{} Unfortunately, there is no tape.batch\PYZus{}gradient,}
        \PY{c+c1}{\PYZsh{} so we have to go via tape.batch\PYZus{}jacobian.}
        \PY{n}{tb\PYZus{}ds2} \PY{o}{=} \PY{n}{tfb\PYZus{}ds2}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{,} \PY{n}{tcb\PYZus{}ds0}\PY{p}{)}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
      \PY{c+c1}{\PYZsh{} We need to take a batched\PYZhy{}gradient.}
      \PY{c+c1}{\PYZsh{} That is available via GradientTape.batch\PYZus{}jacobian.}
      \PY{c+c1}{\PYZsh{} The indexing then removes the scalar\PYZhy{}as\PYZhy{}a\PYZhy{}1d\PYZhy{}vector}
      \PY{c+c1}{\PYZsh{} dimension again.}
      \PY{n}{grad} \PY{o}{=} \PY{n}{tape0}\PY{o}{.}\PY{n}{batch\PYZus{}jacobian}\PY{p}{(}
          \PY{n}{tb\PYZus{}ds2}\PY{p}{,} \PY{n}{tcb\PYZus{}ds0}\PY{p}{,}
          \PY{n}{unconnected\PYZus{}gradients}\PY{o}{=}\PY{n}{uc\PYZus{}zero}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]}
    \PY{n}{tb\PYZus{}hessian} \PY{o}{=} \PY{n}{tape1}\PY{o}{.}\PY{n}{batch\PYZus{}jacobian}\PY{p}{(}\PY{n}{grad}\PY{p}{,} \PY{n}{tcb\PYZus{}ds0}\PY{p}{,}
                                      \PY{n}{unconnected\PYZus{}gradients}\PY{o}{=}\PY{n}{uc\PYZus{}zero}\PY{p}{)}
    \PY{k}{return} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{tb\PYZus{}hessian}
  \PY{k}{return} \PY{n}{tfb\PYZus{}metric}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}metric} \PY{o}{=} \PY{n}{tfb\PYZus{}metric\PYZus{}fn}\PY{p}{(}\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}ds2}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}metric}\PY{p}{(}\PY{n}{tff64}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} Batch of positions.}
    \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[[-0.8       0.        0.        0.      ]
  [ 0.        1.25      0.        0.      ]
  [ 0.        0.        1.        0.      ]
  [ 0.        0.        0.        1.      ]]

 [[-0.8       0.        0.        0.      ]
  [ 0.        1.25      0.        0.      ]
  [ 0.        0.        1.        0.      ]
  [ 0.        0.        0.        1.      ]]

 [[-0.999     0.        0.        0.      ]
  [ 0.        1.001001  0.        0.      ]
  [ 0.        0.        1.        0.      ]
  [ 0.        0.        0.        1.      ]]]
    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let us actually compare this with a hand-coded Schwarzschild metric.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{tfb\PYZus{}schwarzschild\PYZus{}metric\PYZus{}handcoded}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Batched TensorFlow Schwarzschild Spacetime local scalar product.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{r\PYZus{}s} \PY{o}{=} \PY{n}{tff64}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Let\PYZsq{}s use R\PYZus{}Schwarzschild = 1 here.}
  \PY{n}{tb\PYZus{}pos\PYZus{}xyz} \PY{o}{=} \PY{n}{tb\PYZus{}pos}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
  \PY{n}{tb\PYZus{}r} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{tb\PYZus{}pos\PYZus{}xyz}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{s\PYZus{}factor} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{tb\PYZus{}r} \PY{o}{\PYZgt{}} \PY{n}{r\PYZus{}s}\PY{p}{,}
                      \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{r\PYZus{}s} \PY{o}{/} \PY{n}{tb\PYZus{}r}\PY{p}{,}
                      \PY{n}{tff64}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{nan}\PY{p}{)}\PY{p}{)}
  \PY{n}{one} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{tb\PYZus{}r} \PY{o}{\PYZgt{}} \PY{n}{r\PYZus{}s}\PY{p}{,} \PY{n}{tff64}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{tff64}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{nan}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{s\PYZus{}factor}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{s\PYZus{}factor}\PY{p}{,} \PY{n}{one}\PY{p}{,} \PY{n}{one}\PY{p}{]}\PY{p}{,}
                                 \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{tfb\PYZus{}schwarzschild\PYZus{}metric\PYZus{}handcoded}\PY{p}{(}\PY{n}{tff64}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} Batch of positions.}
    \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[[-0.8       0.        0.        0.      ]
  [ 0.        1.25      0.        0.      ]
  [ 0.        0.        1.        0.      ]
  [ 0.        0.        0.        1.      ]]

 [[-0.8       0.        0.        0.      ]
  [ 0.        1.25      0.        0.      ]
  [ 0.        0.        1.        0.      ]
  [ 0.        0.        0.        1.      ]]

 [[-0.999     0.        0.        0.      ]
  [ 0.        1.001001  0.        0.      ]
  [ 0.        0.        1.        0.      ]
  [ 0.        0.        0.        1.      ]]]
    \end{Verbatim}

    Next, let us get batched Christoffel symbols from the batched
metric-function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Again, due to use of tape.batch\PYZus{}jacobian(),}
\PY{c+c1}{\PYZsh{} we actually only can do one batch\PYZhy{}index.}
\PY{k}{def} \PY{n+nf}{tfb\PYZus{}christoffel\PYZus{}fn\PYZus{}v0}\PY{p}{(}\PY{n}{tfb\PYZus{}fn\PYZus{}g}\PY{p}{,} \PY{n}{use\PYZus{}tf\PYZus{}function}\PY{o}{=}\PY{n}{\PYZus{}DEFAULT\PYZus{}USE\PYZus{}TF\PYZus{}FUNCTION}\PY{p}{)}\PY{p}{:}
  \PY{n+nd}{@maybe\PYZus{}tf\PYZus{}function}\PY{p}{(}\PY{n}{use\PYZus{}tf\PYZus{}function}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{tf\PYZus{}christoffel}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}\PY{p}{:}
    \PY{n}{tape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{p}{)}
    \PY{k}{with} \PY{n}{tape}\PY{p}{:}
      \PY{n}{tape}\PY{o}{.}\PY{n}{watch}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}
      \PY{n}{tb\PYZus{}g\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{n}{tfb\PYZus{}fn\PYZus{}g}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}
    \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{n}{tape}\PY{o}{.}\PY{n}{batch\PYZus{}jacobian}\PY{p}{(}\PY{n}{tb\PYZus{}g\PYZus{}at\PYZus{}pos}\PY{p}{,} \PY{n}{tb\PYZus{}pos}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} This variant uses the \PYZdq{}inverse of the metric tensor\PYZdq{}.}
    \PY{c+c1}{\PYZsh{} Now, our code is set up to produce a NaN matrix inside the black hole,}
    \PY{c+c1}{\PYZsh{} and tf.linalg.inv() may raise an exception when it encounters}
    \PY{c+c1}{\PYZsh{} a non\PYZhy{}invertible matrix. This is fixed in the 2nd variant below.}
    \PY{n}{tb\PYZus{}ginv\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{tb\PYZus{}g\PYZus{}at\PYZus{}pos}\PY{p}{)}
    \PY{n}{tb\PYZus{}dg3\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...abc\PYZhy{}\PYZgt{}...cab}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos}\PY{p}{)}
                     \PY{o}{+} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...abc\PYZhy{}\PYZgt{}...bca}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos}\PY{p}{)}
                     \PY{o}{+} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...abc\PYZhy{}\PYZgt{}...acb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...ab,...bcd\PYZhy{}\PYZgt{}...acd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                           \PY{n}{tb\PYZus{}ginv\PYZus{}at\PYZus{}pos}\PY{p}{,}
                           \PY{n}{tb\PYZus{}dg3\PYZus{}at\PYZus{}pos}\PY{p}{)}
  \PY{k}{return} \PY{n}{tf\PYZus{}christoffel}


\PY{k}{def} \PY{n+nf}{tfb\PYZus{}christoffel\PYZus{}fn}\PY{p}{(}\PY{n}{tfb\PYZus{}fn\PYZus{}g}\PY{p}{,} \PY{n}{use\PYZus{}tf\PYZus{}function}\PY{o}{=}\PY{n}{\PYZus{}DEFAULT\PYZus{}USE\PYZus{}TF\PYZus{}FUNCTION}\PY{p}{)}\PY{p}{:}
  \PY{n+nd}{@maybe\PYZus{}tf\PYZus{}function}\PY{p}{(}\PY{n}{use\PYZus{}tf\PYZus{}function}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{tf\PYZus{}christoffel}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}\PY{p}{:}
    \PY{n}{tape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{p}{)}
    \PY{k}{with} \PY{n}{tape}\PY{p}{:}
      \PY{n}{tape}\PY{o}{.}\PY{n}{watch}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}
      \PY{n}{tb\PYZus{}g\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{n}{tfb\PYZus{}fn\PYZus{}g}\PY{p}{(}\PY{n}{tb\PYZus{}pos}\PY{p}{)}
    \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{n}{tape}\PY{o}{.}\PY{n}{batch\PYZus{}jacobian}\PY{p}{(}\PY{n}{tb\PYZus{}g\PYZus{}at\PYZus{}pos}\PY{p}{,} \PY{n}{tb\PYZus{}pos}\PY{p}{)}
    \PY{n}{tb\PYZus{}dg3\PYZus{}at\PYZus{}pos} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...abc\PYZhy{}\PYZgt{}...cab}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos}\PY{p}{)}
                     \PY{o}{+} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...abc\PYZhy{}\PYZgt{}...bca}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos}\PY{p}{)}
                     \PY{o}{+} \PY{n}{tf}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...abc\PYZhy{}\PYZgt{}...acb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tb\PYZus{}dg\PYZus{}at\PYZus{}pos}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Use tf.linalg.solve() rather than matrix\PYZhy{}inversion.}
    \PY{c+c1}{\PYZsh{} This works with batched matrices, so we will need to reshape the}
    \PY{c+c1}{\PYZsh{} Christoffel symbols of the 1st kind}
    \PY{c+c1}{\PYZsh{} (batch\PYZus{}size, 4, 4, 4) \PYZhy{}\PYZgt{} (batch\PYZus{}size, 4, 16)}
    \PY{c+c1}{\PYZsh{} and reshape back again afterwards.}
    \PY{k}{return} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}
        \PY{n}{tf}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{solve}\PY{p}{(}\PY{n}{tb\PYZus{}g\PYZus{}at\PYZus{}pos}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{tb\PYZus{}dg3\PYZus{}at\PYZus{}pos}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{tf\PYZus{}christoffel}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{n}{tfb\PYZus{}christoffel\PYZus{}schwarzschild} \PY{o}{=} \PY{n}{tfb\PYZus{}christoffel\PYZus{}fn}\PY{p}{(}
    \PY{n}{tfb\PYZus{}schwarzschild\PYZus{}metric\PYZus{}handcoded}\PY{p}{)}

\PY{n}{christoffel\PYZus{}examples} \PY{o}{=} \PY{n}{tfb\PYZus{}christoffel\PYZus{}schwarzschild}\PY{p}{(}\PY{n}{tff64}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} Batch of positions.}
    \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} This produces lengthy output:}
\PY{c+c1}{\PYZsh{} print(christoffel\PYZus{}examples.round(6))}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Naturally, Christoffel symbols will have many zero\PYZhy{}entries.}
\PY{c+c1}{\PYZsh{} So, let us also use a helper to pretty\PYZhy{}format such tensors.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} This code is taken from the m\PYZhy{}theory Python library published alongside}
\PY{c+c1}{\PYZsh{} the current author\PYZsq{}s \PYZdq{}numerical supergravity\PYZdq{} papers,}
\PY{c+c1}{\PYZsh{} see: https://github.com/google\PYZhy{}research/google\PYZhy{}research/blob/master/m\PYZus{}theory/m\PYZus{}theory\PYZus{}lib/m\PYZus{}util.py}

\PY{k}{def} \PY{n+nf}{tformat}\PY{p}{(}\PY{n}{array}\PY{p}{,}
            \PY{n}{name}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
            \PY{n}{d}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
            \PY{n+nb}{filter}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{,}  \PY{c+c1}{\PYZsh{} pylint:disable=redefined\PYZhy{}builtin}
            \PY{n+nb}{format}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{c+c1}{\PYZsh{} pylint:disable=redefined\PYZhy{}builtin}
            \PY{n}{nmax}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{inf}\PY{p}{,}
            \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{120}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Formats a numpy\PYZhy{}array in human readable table form.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} Leading row will be replaced if caller asked for a name\PYZhy{}row.}
  \PY{k}{if} \PY{n}{d} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
    \PY{n}{array} \PY{o}{=} \PY{n}{array}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{d}\PY{p}{)}
  \PY{n}{dim\PYZus{}widths} \PY{o}{=} \PY{p}{[}
      \PY{n+nb}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{int}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{dim} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}100}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
      \PY{k}{for} \PY{n}{dim} \PY{o+ow}{in} \PY{n}{array}\PY{o}{.}\PY{n}{shape}\PY{p}{]}
  \PY{n}{format\PYZus{}str} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{: }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{w} \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{dim\PYZus{}widths}\PY{p}{)}\PY{p}{,} \PY{n+nb}{format}\PY{p}{)}
  \PY{n}{rows} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{k}{for} \PY{n}{indices} \PY{o+ow}{in} \PY{n}{itertools}\PY{o}{.}\PY{n}{product}\PY{p}{(}\PY{o}{*}\PY{p}{[}\PY{n+nb}{range}\PY{p}{(}\PY{n}{dim}\PY{p}{)} \PY{k}{for} \PY{n}{dim} \PY{o+ow}{in} \PY{n}{array}\PY{o}{.}\PY{n}{shape}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{v} \PY{o}{=} \PY{n}{array}\PY{p}{[}\PY{n}{indices}\PY{p}{]}
    \PY{k}{if} \PY{n+nb}{filter}\PY{p}{(}\PY{n}{v}\PY{p}{)}\PY{p}{:}
      \PY{n}{rows}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{format\PYZus{}str} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{indices} \PY{o}{+} \PY{p}{(}\PY{n}{v}\PY{p}{,}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{n}{num\PYZus{}entries} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{)}
  \PY{k}{if} \PY{n}{num\PYZus{}entries} \PY{o}{\PYZgt{}} \PY{n}{nmax}\PY{p}{:}
    \PY{n}{rows} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{p}{:}\PY{n}{nmax}\PY{p}{]}
  \PY{k}{if} \PY{n}{ncols} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
    \PY{n}{width} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n+nb}{len}\PY{p}{,} \PY{n}{rows}\PY{p}{)}\PY{p}{)} \PY{k}{if} \PY{n}{rows} \PY{k}{else} \PY{l+m+mi}{80}
    \PY{n}{num\PYZus{}cols} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ncols} \PY{o}{/}\PY{o}{/} \PY{p}{(}\PY{l+m+mi}{3} \PY{o}{+} \PY{n}{width}\PY{p}{)}\PY{p}{)}
    \PY{n}{num\PYZus{}xrows} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{)} \PY{o}{/} \PY{n}{num\PYZus{}cols}\PY{p}{)}\PY{p}{)}
    \PY{n}{padded} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{width}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{s}
              \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{rows} \PY{o}{+} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{num\PYZus{}cols} \PY{o}{*} \PY{n}{num\PYZus{}xrows} \PY{o}{\PYZhy{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{)}\PY{p}{)}\PY{p}{]}
    \PY{n}{table} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{padded}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{object}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{num\PYZus{}cols}\PY{p}{,} \PY{n}{num\PYZus{}xrows}\PY{p}{)}\PY{o}{.}\PY{n}{T}
    \PY{n}{xrows} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ | }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{row}\PY{p}{)} \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{table}\PY{p}{]}
  \PY{k}{else}\PY{p}{:}
    \PY{n}{xrows} \PY{o}{=} \PY{n}{rows}
  \PY{k}{if} \PY{n}{name} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
    \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}
        \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{, shape=}\PY{l+s+si}{\PYZpc{}r}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ non\PYZhy{}small entries ===}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}
            \PY{n}{name}\PY{p}{,} \PY{n}{array}\PY{o}{.}\PY{n}{shape}\PY{p}{,}
            \PY{n}{num\PYZus{}entries}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{num\PYZus{}entries} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{)} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ (}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ shown)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{)}\PY{p}{,}
            \PY{n}{array}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{]} \PY{o}{+}
        \PY{p}{[}\PY{n}{r}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{r} \PY{o+ow}{in} \PY{n}{xrows}\PY{p}{]}\PY{p}{)}
  \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{r}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{r} \PY{o+ow}{in} \PY{n}{xrows}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{tprint}\PY{p}{(}\PY{n}{array}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{file}\PY{o}{=}\PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{tformat\PYZus{}kwargs}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Prints a numpy array in human readable table form.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n+nb}{print}\PY{p}{(}\PY{n}{tformat}\PY{p}{(}\PY{n}{array}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{tformat\PYZus{}kwargs}\PY{p}{)}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{n}{sep}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{n}{end}\PY{p}{,} \PY{n}{file}\PY{o}{=}\PY{n}{file}\PY{p}{)}



\PY{n}{tprint}\PY{p}{(}\PY{n}{christoffel\PYZus{}examples}\PY{p}{,} \PY{n}{d}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0 0 1 0: 0.05       | 0 1 1 1: -0.025     | 1 1 0 0: 0.016      | 2 0 1 0:
1.001e-06  | 2 1 1 1: -5.005e-07
0 1 0 0: 0.016      | 1 0 1 0: 0.05       | 1 1 1 1: -0.025     | 2 1 0 0:
4.995e-07  |
    \end{Verbatim}

    What do we see here?

\begin{itemize}
\item
  The batch-index-0 and batch-index-1 Christoffel symbols are the same,
  as is expected since ``we merely moved forward in time''.
\item
  The batch-index-2 Christoffel symbols are small, as is expected
  ``faraway from the Schwarzschild radius''.
\end{itemize}

For batch-index-2, starting ``at rest'', i.e.~with a time-directed-only
trajectory, and moving along it, forward in time, we get a small radial
contribution pulling us inward
(\(\ddot\gamma=-\Gamma\dot\gamma\dot\gamma\)). Magnitude-wise, the force
looks as if it were \(\propto 1/r^2\).

Let us see if we can compute some first trajectories.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot}

\PY{k}{def} \PY{n+nf}{rk4\PYZus{}ode}\PY{p}{(}\PY{n}{df\PYZus{}ds}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{ds}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}(Batched) Runge\PYZhy{}Kutta RK4 for numerically solving an ODE.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} https://en.wikipedia.org/wiki/Runge\PYZpc{}E2\PYZpc{}80\PYZpc{}93Kutta\PYZus{}methods}
  \PY{n}{f1} \PY{o}{=} \PY{n}{df\PYZus{}ds}\PY{p}{(}\PY{n}{x0}\PY{p}{,} \PY{n}{s}\PY{p}{)}
  \PY{n}{f2} \PY{o}{=} \PY{n}{df\PYZus{}ds}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{ds} \PY{o}{*} \PY{n}{f1}\PY{p}{,} \PY{n}{s} \PY{o}{+} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{ds}\PY{p}{)}
  \PY{n}{f3} \PY{o}{=} \PY{n}{df\PYZus{}ds}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{ds} \PY{o}{*} \PY{n}{f2}\PY{p}{,} \PY{n}{s} \PY{o}{+} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{ds}\PY{p}{)}
  \PY{n}{f4} \PY{o}{=} \PY{n}{df\PYZus{}ds}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{n}{ds} \PY{o}{*} \PY{n}{f3}\PY{p}{,} \PY{n}{s} \PY{o}{+} \PY{n}{ds}\PY{p}{)}
  \PY{k}{return} \PY{n}{x0} \PY{o}{+} \PY{p}{(}\PY{n}{ds} \PY{o}{/} \PY{l+m+mf}{6.0}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{f1} \PY{o}{+} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{f2} \PY{o}{+} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{f3} \PY{o}{+} \PY{n}{f4}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{rewrite\PYZus{}2nd\PYZus{}order\PYZus{}as\PYZus{}1st\PYZus{}order}\PY{p}{(}\PY{n}{f\PYZus{}acceleration}\PY{p}{,} \PY{n}{debug}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
  \PY{k}{def} \PY{n+nf}{d\PYZus{}by\PYZus{}ds\PYZus{}xvs}\PY{p}{(}\PY{n}{xvs}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} `xvs` is the vector}
    \PY{c+c1}{\PYZsh{} (x0, ..., x[n\PYZhy{}1], v0, ..., v[n\PYZhy{}1], curve\PYZus{}parameter\PYZus{}s).}
    \PY{n}{dim} \PY{o}{=} \PY{p}{(}\PY{n}{xvs}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}
    \PY{k}{return} \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}
              \PY{n}{xvs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{n}{dim} \PY{p}{:} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{dim}\PY{p}{]}\PY{p}{,}  \PY{c+c1}{\PYZsh{} \PYZdq{}d/dt x = v\PYZdq{}}
              \PY{n}{f\PYZus{}acceleration}\PY{p}{(}           \PY{c+c1}{\PYZsh{} \PYZdq{}d/dt v = a\PYZdq{}}
                  \PY{n}{xvs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{p}{:}\PY{n}{dim}\PY{p}{]}\PY{p}{,}
                  \PY{n}{xvs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{n}{dim} \PY{p}{:} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{dim}\PY{p}{]}\PY{p}{)}\PY{p}{,}
              \PY{n}{numpy}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{xvs}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}
              \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{k}{return} \PY{n}{d\PYZus{}by\PYZus{}ds\PYZus{}xvs}


\PY{k}{def} \PY{n+nf}{get\PYZus{}trajectories}\PY{p}{(}\PY{n}{f\PYZus{}acceleration}\PY{p}{,} \PY{n}{x0s}\PY{p}{,} \PY{n}{v0s}\PY{p}{,} \PY{n}{ds}\PY{p}{,} \PY{n}{s0}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
  \PY{n}{x0s} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{x0s}\PY{p}{)}
  \PY{n}{v0s} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{v0s}\PY{p}{)}
  \PY{n}{dim} \PY{o}{=} \PY{n}{x0s}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
  \PY{n}{d\PYZus{}by\PYZus{}ds\PYZus{}xvs} \PY{o}{=} \PY{n}{rewrite\PYZus{}2nd\PYZus{}order\PYZus{}as\PYZus{}1st\PYZus{}order}\PY{p}{(}\PY{n}{f\PYZus{}acceleration}\PY{p}{)}
  \PY{k}{def} \PY{n+nf}{ode\PYZus{}f}\PY{p}{(}\PY{n}{xs}\PY{p}{,} \PY{n}{s}\PY{p}{)}\PY{p}{:}
    \PY{k}{del} \PY{n}{s}  \PY{c+c1}{\PYZsh{} Curve\PYZhy{}parameter actually gets ignored here.}
    \PY{k}{return} \PY{n}{d\PYZus{}by\PYZus{}ds\PYZus{}xvs}\PY{p}{(}\PY{n}{xs}\PY{p}{)}
  \PY{n}{xvs\PYZus{}now} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}
    \PY{p}{[}\PY{n}{x0s}\PY{p}{,} \PY{n}{v0s}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{x0s}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{x0s}\PY{o}{.}\PY{n}{dtype}\PY{p}{)}\PY{p}{]}\PY{p}{,}
    \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{k}{for} \PY{n}{num\PYZus{}step} \PY{o+ow}{in} \PY{n}{itertools}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{s} \PY{o}{=} \PY{n}{s0} \PY{o}{+} \PY{n}{num\PYZus{}step} \PY{o}{*} \PY{n}{ds}
    \PY{n}{xvs\PYZus{}now} \PY{o}{=} \PY{n}{rk4\PYZus{}ode}\PY{p}{(}\PY{n}{ode\PYZus{}f}\PY{p}{,} \PY{n}{xvs\PYZus{}now}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{ds}\PY{p}{)}
    \PY{k}{yield} \PY{n}{xvs\PYZus{}now}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{p}{:}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{xvs\PYZus{}now}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{n}{dim} \PY{p}{:} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{xvs\PYZus{}now}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}

\PY{k}{def} \PY{n+nf}{get\PYZus{}schwarzschild\PYZus{}renderer}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{tf\PYZus{}christoffel} \PY{o}{=} \PY{n}{tfb\PYZus{}christoffel\PYZus{}schwarzschild}
  \PY{k}{def} \PY{n+nf}{f\PYZus{}acceleration\PYZus{}einstein}\PY{p}{(}\PY{n}{xs}\PY{p}{,} \PY{n}{vs}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Christoffel symbols at current position.}
    \PY{n}{gamma\PYZus{}amn} \PY{o}{=} \PY{n}{tf\PYZus{}christoffel}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{xs}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float64}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{...amn,...m,...n\PYZhy{}\PYZgt{}...a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma\PYZus{}amn}\PY{p}{,} \PY{n}{vs}\PY{p}{,} \PY{n}{vs}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{c+c1}{\PYZsh{} The function below is a closure over f\PYZus{}acceleration\PYZus{}einstein,}
  \PY{c+c1}{\PYZsh{} so this will compile the TensorFlow machinery on first execution and then}
  \PY{c+c1}{\PYZsh{} re\PYZhy{}use it on subsequent ones.}
  \PY{k}{def} \PY{n+nf}{fn\PYZus{}renderer}\PY{p}{(}\PY{n}{start\PYZus{}txyz}\PY{p}{,} \PY{n}{start\PYZus{}d\PYZus{}txyz\PYZus{}ds}\PY{p}{,} \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{get\PYZus{}trajectories}\PY{p}{(}
      \PY{n}{f\PYZus{}acceleration\PYZus{}einstein}\PY{p}{,}
      \PY{c+c1}{\PYZsh{} All start at (t, x, y, z) = (0, 5, 0, 0)}
      \PY{n}{start\PYZus{}txyz}\PY{p}{,} \PY{n}{start\PYZus{}d\PYZus{}txyz\PYZus{}ds}\PY{p}{,} \PY{n}{ds}\PY{p}{)}
  \PY{k}{return} \PY{n}{fn\PYZus{}renderer}

\PY{k}{def} \PY{n+nf}{demo1\PYZus{}schwarzschild\PYZus{}trajectories}\PY{p}{(}\PY{n}{fn\PYZus{}renderer}\PY{p}{,}
                                     \PY{n}{vys}\PY{p}{,}
                                     \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,}
                                     \PY{n}{fn\PYZus{}style\PYZus{}by\PYZus{}num\PYZus{}traj}\PY{o}{=}\PY{k}{lambda} \PY{n}{\PYZus{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}k}\PY{l+s+s1}{\PYZsq{}}
                                     \PY{p}{)}\PY{p}{:}
  \PY{n}{trajectories} \PY{o}{=} \PY{n}{fn\PYZus{}renderer}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} All start at (t, x, y, z) = (0, \PYZhy{}5, 0, 0)}
    \PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{]} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{vys}\PY{p}{]}\PY{p}{,}
    \PY{c+c1}{\PYZsh{} This is actually a bit off and does not give us length\PYZhy{}squared \PYZhy{}1}
    \PY{c+c1}{\PYZsh{} tangents to the geodesic. See below for a discussion.}
    \PY{p}{[}\PY{p}{[}\PY{l+m+mf}{1.0} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{vy}\PY{o}{*}\PY{n}{vy}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{vy}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{]} \PY{k}{for} \PY{n}{vy} \PY{o+ow}{in} \PY{n}{vys}\PY{p}{]}\PY{p}{,}
    \PY{n}{ds}\PY{p}{)}
  \PY{n}{xs\PYZus{}vs\PYZus{}ss} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{itertools}\PY{o}{.}\PY{n}{islice}\PY{p}{(}\PY{n}{trajectories}\PY{p}{,} \PY{n}{num\PYZus{}steps}\PY{p}{)}\PY{p}{)}
  \PY{n}{fig} \PY{o}{=} \PY{n}{pyplot}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
  \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
  \PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
  \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}aspect}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{alphas} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{numpy}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{l+m+mi}{201}\PY{p}{)}
  \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{alphas}\PY{p}{)}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{alphas}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{k}{for} \PY{n}{num\PYZus{}trajectory} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{vys}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{xs}\PY{p}{[}\PY{n}{num\PYZus{}trajectory}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{xs}\PY{p}{,} \PY{n}{vs}\PY{p}{,} \PY{n}{ss} \PY{o+ow}{in} \PY{n}{xs\PYZus{}vs\PYZus{}ss}\PY{p}{]}\PY{p}{,}
            \PY{p}{[}\PY{n}{xs}\PY{p}{[}\PY{n}{num\PYZus{}trajectory}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]} \PY{k}{for} \PY{n}{xs}\PY{p}{,} \PY{n}{vs}\PY{p}{,} \PY{n}{ss} \PY{o+ow}{in} \PY{n}{xs\PYZus{}vs\PYZus{}ss}\PY{p}{]}\PY{p}{,}
            \PY{n}{fn\PYZus{}style\PYZus{}by\PYZus{}num\PYZus{}traj}\PY{p}{(}\PY{n}{num\PYZus{}trajectory}\PY{p}{)}\PY{p}{)}
  \PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
  \PY{k}{return} \PY{n}{xs\PYZus{}vs\PYZus{}ss}


\PY{k+kn}{import} \PY{n+nn}{time}

\PY{n}{fn\PYZus{}renderer} \PY{o}{=} \PY{n}{get\PYZus{}schwarzschild\PYZus{}renderer}\PY{p}{(}\PY{p}{)}
\PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{demo1\PYZus{}schwarzschild\PYZus{}trajectories}\PY{p}{(}
    \PY{n}{fn\PYZus{}renderer}\PY{p}{,}
    \PY{p}{[}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.35}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{]}\PY{p}{,}
    \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
    \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,}
    \PY{n}{fn\PYZus{}style\PYZus{}by\PYZus{}num\PYZus{}traj}\PY{o}{=}\PY{k}{lambda} \PY{n}{n}\PY{p}{:} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{)}
\PY{n}{t1} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{demo1\PYZus{}schwarzschild\PYZus{}trajectories}\PY{p}{(}
    \PY{n}{fn\PYZus{}renderer}\PY{p}{,}
    \PY{p}{[}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.35}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{]}\PY{p}{,}
    \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
    \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,}
    \PY{n}{fn\PYZus{}style\PYZus{}by\PYZus{}num\PYZus{}traj}\PY{o}{=}\PY{k}{lambda} \PY{n}{n}\PY{p}{:} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{)}
\PY{n}{t2} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{demo1\PYZus{}schwarzschild\PYZus{}trajectories}\PY{p}{(}
    \PY{n}{fn\PYZus{}renderer}\PY{p}{,}
    \PY{p}{[}\PY{l+m+mf}{0.35}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{300}\PY{p}{,}
    \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
    \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,}
    \PY{n}{fn\PYZus{}style\PYZus{}by\PYZus{}num\PYZus{}traj}\PY{o}{=}\PY{k}{lambda} \PY{n}{n}\PY{p}{:} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{n} \PY{o}{\PYZpc{}} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{t3} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Timings}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{t1}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{,} \PY{n}{t2}\PY{o}{\PYZhy{}}\PY{n}{t1}\PY{p}{,} \PY{n}{t3}\PY{o}{\PYZhy{}}\PY{n}{t2}\PY{p}{)}
\PY{c+c1}{\PYZsh{} With GPU acceleration, this prints something like:}
\PY{c+c1}{\PYZsh{} Timings 3.527270555496216 3.01137113571167 4.59316873550415}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} So, thanks to GPU hardware, producing 300 trajectories is still about}
\PY{c+c1}{\PYZsh{} the same effort as producing one trajectory.}
\PY{c+c1}{\PYZsh{} This depends of course on the capacity of the GPU units in the system.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} Overall, if we compute one trajectory on GPU, we may just as well}
\PY{c+c1}{\PYZsh{} make the extra silicon on the GPU which would only idle during that time}
\PY{c+c1}{\PYZsh{} do the same calculations for other trajectories.}

\PY{c+c1}{\PYZsh{} Participant Exercise: Redo the computation with ds=0.1 and num\PYZus{}steps=1000}
\PY{c+c1}{\PYZsh{} to validate that step size is small enough to not produce}
\PY{c+c1}{\PYZsh{} visibly bad accuracy.}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<ipython-input-22-ec64963b6829>:86: UserWarning: Matplotlib is currently using
module://matplotlib\_inline.backend\_inline, which is a non-GUI backend, so cannot
show the figure.
  fig.show()
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Timings 3.4908783435821533 3.330188274383545 5.980652332305908
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_38_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_38_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_38_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    One thing is a bit awkward here: We are doing batch-ODE-integration with
NumPy, and so end up shuffling data back and forth between GPU and CPU,
once per ODE-step.

With a bit of extra cleverness, we could translate ODE-integration also
to TensorFlow. This would ask for techniques such as ``looping and
stopping gradients'' (since we are not interested in gradients here)
that go beyond what we cover here.

Overall, with Colab kernel power as observed at the time this was
written, we can do 300 pixels in 5 seconds, so 18k pixels in 5 minutes.
This is good enough for a 128x128 image. So, let us see if we can render
a crude image of the hole - even for more generic metric fields.

Here, however, it would be a shame to not exploit rotational symmetry.
After all, if we merely shoot 1000 light rays at the hole, this should
give us suffcient numerical information about scattering to then just
rotate these sample trajectories in order to create an image.

But let us try to do this accurately. For the timelike trajectories we
had earlier, we used an initial tangent-vector for ODE-integration that
was not unit-normalized w.r.t. the geometry at the starting point. Since
we just wanted to see ``some first geodesics'' for a quick
plausibility-check, this did not matter much. This time, we need to do
better. Why not by working out a local Lorentz frame? We will use
another little helper from the author's M-Theory repository, included
here to make the notebook self-contained.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}gramian\PYZus{}onb}\PY{p}{(}\PY{n}{gramian}\PY{p}{,} \PY{n}{eigenvalue\PYZus{}threshold}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Computes orthogonalizing transform for a gramian.}
\PY{l+s+sd}{  Args:}
\PY{l+s+sd}{    gramian: [N, N]\PYZhy{}array G.}
\PY{l+s+sd}{    eigenvalue\PYZus{}threshold: Eigenvalues smaller than this}
\PY{l+s+sd}{      are considered to be equal to zero.}
\PY{l+s+sd}{  Returns:}
\PY{l+s+sd}{    A pair of matrices (R, R\PYZus{}inv) such that R @ R\PYZus{}inv = numpy.eye(N) and}
\PY{l+s+sd}{    einsum(\PYZsq{}Aa,Bb,ab\PYZhy{}\PYZgt{}AB\PYZsq{}, R, R, gramian) is diagonal with entries}
\PY{l+s+sd}{    in (0, 1, \PYZhy{}1).}
\PY{l+s+sd}{    Example: If gramian=numpy.array([[100.0, 0.1], [0.1, 1.0]])}
\PY{l+s+sd}{    then R.round(6) == numpy.array([[0.00101, \PYZhy{}1.00005], [\PYZhy{}0.1, \PYZhy{}0.000101]])}
\PY{l+s+sd}{    and R[0, :] as well as R[1, :] are orthonormal unit vectors w.r.t.}
\PY{l+s+sd}{    the scalar product given by the gramian.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{n}{gramian} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{gramian}\PY{p}{)}
  \PY{n}{sprods\PYZus{}eigvals}\PY{p}{,} \PY{n}{sprods\PYZus{}eigvecsT} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eigh}\PY{p}{(}\PY{n}{gramian}\PY{p}{)}
  \PY{n}{abs\PYZus{}evs} \PY{o}{=} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{sprods\PYZus{}eigvals}\PY{p}{)}
  \PY{n}{onbi\PYZus{}scaling} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{abs\PYZus{}evs} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{eigenvalue\PYZus{}threshold}\PY{p}{,}
                             \PY{l+m+mf}{1.0}\PY{p}{,}
                             \PY{n}{numpy}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{abs\PYZus{}evs}\PY{p}{)}\PY{p}{)}
  \PY{n}{onbi} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WZ,Z\PYZhy{}\PYZgt{}WZ}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{n}{sprods\PYZus{}eigvecsT}\PY{p}{,} \PY{n}{onbi\PYZus{}scaling}\PY{p}{)}
  \PY{n}{onb} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WZ,Z\PYZhy{}\PYZgt{}WZ}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{sprods\PYZus{}eigvecsT}\PY{p}{,} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{n}{onbi\PYZus{}scaling}\PY{p}{)}\PY{o}{.}\PY{n}{T}
  \PY{k}{assert} \PY{n}{numpy}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{onb} \PY{o}{@} \PY{n}{onbi}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{onb}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{onb}\PY{p}{,} \PY{n}{onbi}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This is the \PYZdq{}TensorFlow\PYZhy{}powered Virtual Event Horizon Telescope (TVEHT) :\PYZhy{})\PYZdq{}}

\PY{n}{TVEHT\PYZus{}CAMERA\PYZus{}POS} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Terminology: \PYZob{}name\PYZcb{}\PYZus{}DmDn means the tensor has a}
\PY{c+c1}{\PYZsh{} (D)own\PYZhy{}index m and (D)own\PYZhy{}index n. \PYZsq{}U\PYZsq{} is for an up\PYZhy{}index.}
\PY{n}{CAMERA\PYZus{}G\PYZus{}DmDn} \PY{o}{=} \PY{n}{tfb\PYZus{}schwarzschild\PYZus{}metric\PYZus{}handcoded}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} batching and unbatching}
    \PY{n}{tff64}\PY{p}{(}\PY{n}{TVEHT\PYZus{}CAMERA\PYZus{}POS}\PY{p}{)}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{tprint}\PY{p}{(}\PY{n}{CAMERA\PYZus{}G\PYZus{}DmDn}\PY{p}{,} \PY{n}{d}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Camera G\PYZus{}AB}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} a,b,... = Lorentz index, m,n,... = Manifold\PYZhy{}coordinates index.}
\PY{n}{CAMERA\PYZus{}DaUm}\PY{p}{,} \PY{n}{CAMERA\PYZus{}DmUa} \PY{o}{=} \PY{n}{get\PYZus{}gramian\PYZus{}onb}\PY{p}{(}\PY{n}{CAMERA\PYZus{}G\PYZus{}DmDn}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== Lorentz frame metric ===}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{p}{(}\PY{n}{CAMERA\PYZus{}DaUm} \PY{o}{@} \PY{n}{CAMERA\PYZus{}G\PYZus{}DmDn} \PY{o}{@} \PY{n}{CAMERA\PYZus{}DaUm}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=== Lorentz frame transform ===}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{CAMERA\PYZus{}DaUm}\PY{p}{)}

\PY{c+c1}{\PYZsh{} In general, the local Lorentz frame may be rotated/boosted in a weird way.}
\PY{c+c1}{\PYZsh{} Let us convert three relevant directions in the manifold\PYZhy{}frame}
\PY{c+c1}{\PYZsh{} to the Lorentz frame: timelike, towards\PYZhy{}the\PYZhy{}hole, perpendicular.}
\PY{c+c1}{\PYZsh{} LF == Lorentz\PYZhy{}Frame}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} G is: G\PYZus{}AB, CAMERA\PYZus{}LM is: CAMERA\PYZus{}L\PYZca{}M, CAMERA\PYZus{}ML is: CAMERA\PYZus{}M\PYZca{}L}

\PY{c+c1}{\PYZsh{} Not\PYZhy{}normalized Lorentz\PYZhy{}frame vectors which in manifold\PYZhy{}frame are:}
\PY{c+c1}{\PYZsh{} timelike, towards\PYZhy{}hole, perpendicular.}
\PY{n}{LF\PYZus{}T0} \PY{o}{=} \PY{n}{CAMERA\PYZus{}DmUa}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{LF\PYZus{}RX0} \PY{o}{=} \PY{n}{CAMERA\PYZus{}DmUa}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{LF\PYZus{}RY0} \PY{o}{=} \PY{n}{CAMERA\PYZus{}DmUa}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} We want to unit\PYZhy{}normalize these.}
\PY{n}{ETA} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{LF\PYZus{}T} \PY{o}{=} \PY{n}{LF\PYZus{}T0} \PY{o}{/} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ab,a,b\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ETA}\PY{p}{,} \PY{n}{LF\PYZus{}T0}\PY{p}{,} \PY{n}{LF\PYZus{}T0}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}
\PY{n}{LF\PYZus{}RX} \PY{o}{=} \PY{n}{LF\PYZus{}RX0} \PY{o}{/} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ab,a,b\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ETA}\PY{p}{,} \PY{n}{LF\PYZus{}RX0}\PY{p}{,} \PY{n}{LF\PYZus{}RX0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}
\PY{n}{LF\PYZus{}RY} \PY{o}{=} \PY{n}{LF\PYZus{}RY0} \PY{o}{/} \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ab,a,b\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ETA}\PY{p}{,} \PY{n}{LF\PYZus{}RY0}\PY{p}{,} \PY{n}{LF\PYZus{}RY0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}


\PY{k}{def} \PY{n+nf}{get\PYZus{}initial\PYZus{}lightlike\PYZus{}geodesic\PYZus{}tangent}\PY{p}{(}\PY{o}{*}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
  \PY{n}{v\PYZus{}xy} \PY{o}{=} \PY{n}{x} \PY{o}{*} \PY{n}{LF\PYZus{}RX} \PY{o}{+} \PY{n}{y} \PY{o}{*} \PY{n}{LF\PYZus{}RY}
  \PY{n}{norm} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{v\PYZus{}xy}\PY{p}{)}
  \PY{n}{tangent\PYZus{}Ua} \PY{o}{=} \PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{norm}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n}{v\PYZus{}xy}\PY{p}{)} \PY{o}{/} \PY{n}{norm}
  \PY{n}{tangent\PYZus{}Um} \PY{o}{=} \PY{n}{CAMERA\PYZus{}DaUm}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{tangent\PYZus{}Ua}\PY{p}{)}
  \PY{k}{return} \PY{p}{(}\PY{n}{tangent\PYZus{}Ua}\PY{p}{,} \PY{n}{tangent\PYZus{}Um}\PY{p}{)}


\PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DEBUG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
 \PY{n}{v0\PYZus{}Ua}\PY{p}{,} \PY{n}{v0\PYZus{}Um} \PY{o}{=} \PY{n}{get\PYZus{}initial\PYZus{}lightlike\PYZus{}geodesic\PYZus{}tangent}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}
 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{=== mostly\PYZhy{}towards\PYZhy{}hole slightly\PYZhy{}y\PYZhy{}tilted light ray ===}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ddd\PYZus{}v0\PYZus{}Ua:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{v0\PYZus{}Ua}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ddd\PYZus{}v0\PYZus{}Um:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{v0\PYZus{}Um}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Manifold\PYZhy{}frame ds\PYZca{}2:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
       \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ab,a,b\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{CAMERA\PYZus{}G\PYZus{}DmDn}\PY{p}{,} \PY{n}{v0\PYZus{}Um}\PY{p}{,} \PY{n}{v0\PYZus{}Um}\PY{p}{)}\PY{p}{)}
 \PY{n}{v0\PYZus{}Um\PYZus{}time} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{pad}\PY{p}{(}\PY{n}{v0\PYZus{}Um}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}\PY{p}{)}
 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Manifold\PYZhy{}frame dt\PYZca{}2:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
       \PY{n}{numpy}\PY{o}{.}\PY{n}{einsum}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ab,a,b\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{CAMERA\PYZus{}G\PYZus{}DmDn}\PY{p}{,} \PY{n}{v0\PYZus{}Um\PYZus{}time}\PY{p}{,} \PY{n}{v0\PYZus{}Um\PYZus{}time}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
=== Camera G\_AB, shape=(4, 4), 4 / 16 non-small entries ===
0 0: -0.95    | 1 1: 1.052632 | 2 2: 1.0      | 3 3: 1.0      |               |
|
=== Lorentz frame metric ===
 [[-1.  0.  0.  0.]
 [ 0.  1.  0.  0.]
 [ 0.  0.  1.  0.]
 [ 0.  0.  0.  1.]]
=== Lorentz frame transform ===
 [[1.02597835 0.         0.         0.        ]
 [0.         0.         1.         0.        ]
 [0.         0.         0.         1.        ]
 [0.         0.97467943 0.         0.        ]]


=== mostly-towards-hole slightly-y-tilted light ray ===
ddd\_v0\_Ua: [1.0, 0.0099995, 0.0, 0.9999500037]
ddd\_v0\_Um: [1.0259783521, 0.9746307042, 0.0099995, 0.0]
Manifold-frame ds\^{}2: 3.164921666756748e-16
Manifold-frame dt\^{}2: -1.0000000000000002
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Let us use this to render some geodesics...}

\PY{n}{light\PYZus{}ys} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{get\PYZus{}rays}\PY{p}{(}\PY{n}{light\PYZus{}ys}\PY{p}{,} \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{:}
  \PY{n}{trajectories} \PY{o}{=} \PY{n}{fn\PYZus{}renderer}\PY{p}{(}
      \PY{p}{[}\PY{n}{TVEHT\PYZus{}CAMERA\PYZus{}POS} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{light\PYZus{}ys}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{n}{get\PYZus{}initial\PYZus{}lightlike\PYZus{}geodesic\PYZus{}tangent}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n}{light\PYZus{}ys}\PY{p}{]}\PY{p}{,}
      \PY{n}{ds}\PY{o}{=}\PY{n}{ds}\PY{p}{)}
  \PY{n}{xs\PYZus{}vs\PYZus{}ss} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{itertools}\PY{o}{.}\PY{n}{islice}\PY{p}{(}\PY{n}{trajectories}\PY{p}{,} \PY{n}{num\PYZus{}steps}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} We want the result to be indexed in the form:}
  \PY{c+c1}{\PYZsh{} result[num\PYZus{}trajectory, num\PYZus{}step, xv\PYZus{}selector, xyzt\PYZus{}coord]}
  \PY{n}{the\PYZus{}xs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{xs} \PY{k}{for} \PY{n}{xs}\PY{p}{,} \PY{n}{vs}\PY{p}{,} \PY{n}{ss} \PY{o+ow}{in} \PY{n}{xs\PYZus{}vs\PYZus{}ss}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{n}{the\PYZus{}vs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{vs} \PY{k}{for} \PY{n}{xs}\PY{p}{,} \PY{n}{vs}\PY{p}{,} \PY{n}{ss} \PY{o+ow}{in} \PY{n}{xs\PYZus{}vs\PYZus{}ss}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
  \PY{k}{return} \PY{n}{numpy}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{the\PYZus{}xs}\PY{p}{,} \PY{n}{the\PYZus{}vs}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}

\PY{n}{demo\PYZus{}rays} \PY{o}{=} \PY{n}{get\PYZus{}rays}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{pyplot}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}aspect}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{alphas} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{numpy}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{l+m+mi}{201}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{alphas}\PY{p}{)}\PY{p}{,} \PY{n}{numpy}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{alphas}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
\PY{k}{for} \PY{n}{geodesic} \PY{o+ow}{in} \PY{n}{demo\PYZus{}rays}\PY{p}{:}
  \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{geodesic}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{geodesic}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<ipython-input-25-5241dd76335d>:30: UserWarning: Matplotlib is currently using
module://matplotlib\_inline.backend\_inline, which is a non-GUI backend, so cannot
show the figure.
  fig.show()
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_42_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The above figure suggests that camera-placement and field-of-view
choices are somewhat reasonable. Let us hence proceed to rendering.

We will need a collection of trajectories more finely spaced than what
we had before. The y-range we used will correspond to the
largest-angle-from-center pixels of the image, which will sit at the
corners. Let us obtain 1024 trajectories.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This takes about 3 minutes of computation}
\PY{c+c1}{\PYZsh{} (at the time this course material was written).}
\PY{n}{NUM\PYZus{}RAYS\PYZus{}FOR\PYZus{}RENDERING} \PY{o}{=} \PY{l+m+mi}{1024}
\PY{n}{SCATTERING\PYZus{}B\PYZus{}MAX} \PY{o}{=} \PY{l+m+mf}{0.7}  \PY{c+c1}{\PYZsh{} Somewhat larger field\PYZhy{}of\PYZhy{}view than above.}
\PY{n}{rendering\PYZus{}ys} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{SCATTERING\PYZus{}B\PYZus{}MAX}\PY{p}{,} \PY{n}{NUM\PYZus{}RAYS\PYZus{}FOR\PYZus{}RENDERING}\PY{p}{)}
\PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{} One\PYZhy{}sided only, y increasing from 0 out.}
\PY{c+c1}{\PYZsh{}}
\PY{n}{rendering\PYZus{}rays} \PY{o}{=} \PY{n}{get\PYZus{}rays}\PY{p}{(}\PY{n}{rendering\PYZus{}ys}\PY{p}{,} \PY{n}{ds}\PY{o}{=}\PY{l+m+mf}{0.02}\PY{p}{,} \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{10\PYZus{}000}\PY{p}{)}
\PY{n}{t1} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Obtained }\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}RAYS\PYZus{}FOR\PYZus{}RENDERING}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ geodesics in }\PY{l+s+si}{\PYZob{}}\PY{n}{t1}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{l+s+si}{:}\PY{l+s+s1}{.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ sec.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Obtained 1024 geodesics in 129.186 sec.
    \end{Verbatim}

    We are only interested in the direction of the final velocity-vector, if
this is far-out. Let us imagine there is a checkerboard at large
distance behind our black hole. We are interested in light rays that
passed well past the hole, i.e.~where x-final is ``large'' (say,
\(>50\)), and the direction these rays are then traveling.

We want to associate the following colors to pixels: * Black for rays
that have NaN-values in the final tangent-vector. These have hit the
event horizon. * Dark Gray for ``stray rays'' that end up not with
\(x>50, v_x>0\). * A lighter or darker color of gray, depending on which
checkerboard-cell the \(v_x\)-scaled-to-\(1\) final velocity-vector
hits.

We have our set of geodesics, but will interpolate in between them and
rotate them around, exploiting some of the symmetry of the Schwarzschild
solution.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{interpolate}

\PY{c+c1}{\PYZsh{} First, let us play a bit with the rendering\PYZhy{}geodesics we have, and}
\PY{c+c1}{\PYZsh{} check a few end\PYZhy{}states.}
\PY{k}{for} \PY{n}{index} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{390}\PY{p}{,} \PY{l+m+mi}{380}\PY{p}{,} \PY{l+m+mi}{375}\PY{p}{,} \PY{l+m+mi}{370}\PY{p}{]}\PY{p}{:}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{N=}\PY{l+s+si}{\PYZob{}}\PY{n}{index}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}}\PY{n}{rendering\PYZus{}rays}\PY{p}{[}\PY{n}{index}\PY{p}{,}\PY{+w}{ }\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{fn\PYZus{}final\PYZus{}xv} \PY{o}{=} \PY{n}{scipy}\PY{o}{.}\PY{n}{interpolate}\PY{o}{.}\PY{n}{interp1d}\PY{p}{(}
    \PY{n}{rendering\PYZus{}ys}\PY{p}{,}
    \PY{n}{rendering\PYZus{}rays}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{,}
    \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
    \PY{c+c1}{\PYZsh{} If we used higher\PYZhy{}order interpolation, this would increase}
    \PY{c+c1}{\PYZsh{} the \PYZsq{}blast radius\PYZsq{} of Not\PYZhy{}a\PYZhy{}Number values.}
    \PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
N=1000:
[[199.87556006 155.44916702  85.65445942   0.        ]
 [  0.98020215   0.88676353   0.40568178   0.        ]]
N=500:
[[ 2.01676029e+02  1.73408854e+02 -8.62193292e+00  0.00000000e+00]
 [ 9.80325722e-01  9.70715958e-01 -8.80635927e-02  0.00000000e+00]]
N=400:
[[202.62184955 164.58482327 -53.49140948   0.        ]
 [  0.98034422   0.91856822  -0.32687894   0.        ]]
N=390:
[[202.75571734 162.4114513  -59.67900416   0.        ]
 [  0.98034522   0.90618773  -0.35996201   0.        ]]
N=380:
[[202.90155791 159.77697873 -66.35801079   0.        ]
 [  0.98034588   0.89124621  -0.39570856   0.        ]]
N=375:
[[202.97966947 158.25089089 -69.90565055   0.        ]
 [  0.98034607   0.88261491  -0.41471078   0.        ]]
N=370:
[[203.06169927 156.56141582 -73.60693908   0.        ]
 [  0.98034613   0.87307526  -0.434547     0.        ]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{NUM\PYZus{}PIXELS} \PY{o}{=} \PY{l+m+mi}{800}
\PY{n}{R\PYZus{}MAX} \PY{o}{=} \PY{n}{SCATTERING\PYZus{}B\PYZus{}MAX} \PY{o}{*} \PY{l+m+mf}{0.999}
\PY{n}{X\PYZus{}MAX} \PY{o}{=} \PY{n}{R\PYZus{}MAX} \PY{o}{/} \PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{.5}

\PY{n}{xcoords}\PY{p}{,} \PY{n}{ycoords} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{X\PYZus{}MAX}\PY{p}{,} \PY{n}{X\PYZus{}MAX}\PY{p}{,} \PY{n}{NUM\PYZus{}PIXELS}\PY{p}{)}\PY{p}{,}
                                  \PY{n}{numpy}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{X\PYZus{}MAX}\PY{p}{,} \PY{n}{X\PYZus{}MAX}\PY{p}{,} \PY{n}{NUM\PYZus{}PIXELS}\PY{p}{)}\PY{p}{)}

\PY{n}{img\PYZus{}bs} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{hypot}\PY{p}{(}\PY{n}{xcoords}\PY{p}{,} \PY{n}{ycoords}\PY{p}{)}
\PY{c+c1}{\PYZsh{} For checkerboard\PYZhy{}cell checking, we need the x\PYZhy{} and y\PYZhy{}component of a radial}
\PY{c+c1}{\PYZsh{} unit vector from the image\PYZhy{}center to the given image\PYZhy{}pixel.}
\PY{n}{img\PYZus{}uxs} \PY{o}{=} \PY{n}{xcoords} \PY{o}{/} \PY{n}{img\PYZus{}bs}
\PY{n}{img\PYZus{}uys} \PY{o}{=} \PY{n}{ycoords} \PY{o}{/} \PY{n}{img\PYZus{}bs}
\PY{n}{img\PYZus{}final\PYZus{}xvs} \PY{o}{=} \PY{n}{fn\PYZus{}final\PYZus{}xv}\PY{p}{(}\PY{n}{img\PYZus{}bs}\PY{p}{)}
\PY{n}{img\PYZus{}scattering\PYZus{}tan\PYZus{}angle} \PY{o}{=} \PY{n}{img\PYZus{}final\PYZus{}xvs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]} \PY{o}{/} \PY{n}{img\PYZus{}final\PYZus{}xvs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}


\PY{n}{img\PYZus{}blackhole} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}
    \PY{n}{numpy}\PY{o}{.}\PY{n}{isfinite}\PY{p}{(}\PY{n}{img\PYZus{}scattering\PYZus{}tan\PYZus{}angle}\PY{p}{)}\PY{p}{,}
    \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}
        \PY{c+c1}{\PYZsh{} Final x\PYZhy{}coordinate is \PYZgt{}50, i.e. we passed the hole.}
        \PY{p}{(}\PY{n}{img\PYZus{}final\PYZus{}xvs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{50}\PY{p}{)} \PY{o}{\PYZam{}}
        \PY{c+c1}{\PYZsh{} Final x\PYZhy{}velocity is \PYZgt{}0, i.e. we are looking}
        \PY{c+c1}{\PYZsh{} \PYZdq{}towards the sky in front of us\PYZdq{}.}
        \PY{p}{(}\PY{n}{img\PYZus{}final\PYZus{}xvs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Case: We are looking at the checkerboard.}
        \PY{c+c1}{\PYZsh{} We have to work out the cell.}
        \PY{l+m+mf}{0.75} \PY{o}{+} \PY{p}{(}
            \PY{n}{numpy}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{l+m+mi}{15} \PY{o}{*} \PY{n}{img\PYZus{}scattering\PYZus{}tan\PYZus{}angle} \PY{o}{*} \PY{n}{img\PYZus{}uxs}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{int32}\PY{p}{)} \PY{o}{+}
            \PY{n}{numpy}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{l+m+mi}{15} \PY{o}{*} \PY{n}{img\PYZus{}scattering\PYZus{}tan\PYZus{}angle} \PY{o}{*} \PY{n}{img\PYZus{}uys}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{int32}\PY{p}{)}\PY{p}{)}
        \PY{o}{\PYZpc{}} \PY{l+m+mi}{2} \PY{o}{*} \PY{l+m+mf}{0.125}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Case: \PYZdq{}Stray ray, such as backscattered or still\PYZhy{}orbiting\PYZdq{}.}
        \PY{l+m+mf}{0.25}
    \PY{p}{)}\PY{p}{,}
    \PY{c+c1}{\PYZsh{} Case: NaN angle}
    \PY{l+m+mi}{0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print(img\PYZus{}final\PYZus{}xvs[:5, :5, 0, 1:].round(3))}
\PY{n}{fig} \PY{o}{=} \PY{n}{pyplot}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img\PYZus{}blackhole}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<ipython-input-29-2fb71661f656>:41: UserWarning: Matplotlib is currently using
module://matplotlib\_inline.backend\_inline, which is a non-GUI backend, so cannot
show the figure.
  fig.show()
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ML_05_TF_Physics_files/ML_05_TF_Physics_47_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    What have we accomplished? We now have a way to do raytracing starting
from an arbitrary geometry - which we might have given not even as a
formula, but simply as code - in the form of a TensorFlow function that
implements the spacetime-dependent \(ds^2\).

Here, we actually only demonstrated that TensorFlow can derive a
Schwarzschild-metric-function from a \(ds^2\)-function, but then (for
better performance) used our hand-written Schwarzschild-metric-function
afterwards (for performance reasons - since that had a simpler graph).
We could of course simply swap it out.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Participant Exercise}: Check the code and its output for
correctness.

(The course author quickly cobbled it all together, there still might be
bugs in V1 of this course! One thing that looks suspicious is that the
ODE-integrator might have ``jumped over the hole'' for some angles, and
this might potentially explain the inner rings, which likely are
artefacts. Another issue is that RK4 is not really that well-suited for
this kind of ODE-integration.)


    % Add a bibliography block to the postdoc
    
    
    

    
    
    
    

    
    \hypertarget{advanced-topics-tying-up-some-ends}{%
\section{Advanced Topics (\ldots tying up some
ends\ldots)}\label{advanced-topics-tying-up-some-ends}}

This unit discusses some useful-to-know concepts that round off the
course.

    \hypertarget{extending-keras}{%
\subsection{Extending Keras}\label{extending-keras}}

So far, we have encountered TensorFlow in two different guises:

\begin{itemize}
\tightlist
\item
  As a RM-AD tool for GPU-enhanced computation of fast good-quality
  gradients (such as: for optimization).
\item
  As a ML toolkit to quickly wire up some DNN architectures.
\end{itemize}

What we have not seen is how these things fit together, specifically:
how can we wrap up some of our own (possibly quite sophisticated)
function designs as a Keras layer?

Let us explore this by means of an example. We start with a simple
convolutional architecture for CIFAR-10 one-out-of-10 image
classification.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\PY{k+kn}{import} \PY{n+nn}{tensorflow\PYZus{}datasets} \PY{k}{as} \PY{n+nn}{tfds}

\PY{c+c1}{\PYZsh{} Let us load the CIFAR\PYZhy{}10 dataset rather than MNIST}
\PY{c+c1}{\PYZsh{} (10 classes, 32x32 pixels, RGB).}
\PY{c+c1}{\PYZsh{} Details: http://www.cs.toronto.edu/\PYZti{}kriz/cifar.html}

\PY{p}{(}\PY{n}{ds\PYZus{}train\PYZus{}raw}\PY{p}{,} \PY{n}{ds\PYZus{}validation\PYZus{}raw}\PY{p}{,} \PY{n}{ds\PYZus{}test\PYZus{}raw}\PY{p}{)}\PY{p}{,} \PY{n}{ds\PYZus{}info} \PY{o}{=} \PY{n}{tfds}\PY{o}{.}\PY{n}{load}\PY{p}{(}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cifar10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{split}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train[:75}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train[75}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{:]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{shuffle\PYZus{}files}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{as\PYZus{}supervised}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{with\PYZus{}info}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{normalize\PYZus{}image}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{label}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Normalizes images.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{cast}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{255.}\PY{p}{,} \PY{n}{label}


\PY{n}{ds\PYZus{}train} \PY{o}{=} \PY{p}{(}
    \PY{n}{ds\PYZus{}train\PYZus{}raw}
    \PY{c+c1}{\PYZsh{} Details: see https://www.tensorflow.org/guide/data\PYZus{}performance}
    \PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{normalize\PYZus{}image}\PY{p}{,} \PY{n}{num\PYZus{}parallel\PYZus{}calls}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}
    \PY{o}{.}\PY{n}{cache}\PY{p}{(}\PY{p}{)}
    \PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{ds\PYZus{}info}\PY{o}{.}\PY{n}{splits}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{num\PYZus{}examples}\PY{p}{)}
    \PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}
    \PY{o}{.}\PY{n}{prefetch}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}\PY{p}{)}

\PY{n}{ds\PYZus{}validation} \PY{o}{=} \PY{p}{(}
    \PY{n}{ds\PYZus{}validation\PYZus{}raw}
    \PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{normalize\PYZus{}image}\PY{p}{,} \PY{n}{num\PYZus{}parallel\PYZus{}calls}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}
    \PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}
    \PY{o}{.}\PY{n}{cache}\PY{p}{(}\PY{p}{)}
    \PY{o}{.}\PY{n}{prefetch}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}\PY{p}{)}

\PY{n}{ds\PYZus{}test} \PY{o}{=} \PY{p}{(}
    \PY{n}{ds\PYZus{}test\PYZus{}raw}
    \PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{normalize\PYZus{}image}\PY{p}{,} \PY{n}{num\PYZus{}parallel\PYZus{}calls}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}
    \PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}
    \PY{o}{.}\PY{n}{cache}\PY{p}{(}\PY{p}{)}
    \PY{o}{.}\PY{n}{prefetch}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AUTOTUNE}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} The model.}

\PY{n}{cnn\PYZus{}model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}
    \PY{p}{]}
\PY{p}{)}

\PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}\PY{p}{,}
    \PY{n}{loss}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{SparseCategoricalCrossentropy}\PY{p}{(}\PY{n}{from\PYZus{}logits}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{metrics}\PY{o}{.}\PY{n}{SparseCategoricalAccuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,}
\PY{p}{)}

\PY{n}{cnn\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{n}{ds\PYZus{}train}\PY{p}{,}
    \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,}
    \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{ds\PYZus{}validation}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    As we have seen, modern DNN architectures tend to develop calibration
problems more readily than earlier architectures.

One plausible hypothesis here might be that earlier architectures used
rather simple features weakly indicative of the target class, which then
also can be regarded as reasonably independent - and we rarely would
encounter a situation where we accumulated ``really strong evidence''
where the ``independency of (Bayesian) votes'' would have been violated
badly. With DNNs, we can have a situation where hierarchical extraction
of features might lead to ``tell-tale sign'' type of evidence which (by
optimization) gets attributed some large logit-value(s), but is not
independent of other such evidence, which we do however implicitly
assume by summing.

So, one could have the idea that, perhaps, large accumulated evidence
(as input to softmax) should generally be distrusted, and the model is
driven to use large evidence-contributions in some cases since, overall,
this still improves predictions. So, how about introducing a layer that
uniformly ``punishes'' large evidence before we feed it into softmax?
Rather than trying to come up with a principled way to do this, let us
here try an ad-hoc approach and attenuate all ``large evidence'' the
same way with a nonlinear function that involves two parameters that are
learnable - one determining a ``length scale'' below which behavior is
mostly linear, and one for overall rescaling. We will go with the
function \texttt{f(x)\ =\ A\ *\ asinh(B\ *\ x)}, with learnable
\texttt{A} and \texttt{B}.

Also, for the sake of this example, we will ignore the question how to
perhaps achieve this via creative use of some existing Keras layers
(specifically, a ``1D'' convolution across a hidden feature-vector with
an \(1\times 1\) kernel and nonlinear activation). Rather, we want to
implement our own layer.

For this, we need to do a little bit of OO programming, which we so far
did not discuss in this course - implementing a subclass of the
\texttt{tf.keras.layers.Layer} class. This will have a tiny amount of
state - just two parameters - but show all the relevant bits and pieces.

Also mostly for illustration purposes, we give our layer an extra tuning
parameter which allows selecting a nonlinearity, with (here) two
possible choices.

We mostly follow the
\href{https://g3doc.corp.google.com/eng/doc/devguide/py/style/index.md?cl=head}{Google/Alphabet
Python style guide} for this code.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Introducing a new type of Keras layer.}
\PY{c+c1}{\PYZsh{} Base class documentation:}
\PY{c+c1}{\PYZsh{}   https://www.tensorflow.org/api\PYZus{}docs/python/tf/keras/layers/Layer}


\PY{c+c1}{\PYZsh{} Here, we are moving the computational body to separate @tf.function\PYZhy{}s.}
\PY{c+c1}{\PYZsh{} For this particular example, the body is so simple that this makes}
\PY{c+c1}{\PYZsh{} little sense \PYZhy{} we could just have done the computation in\PYZhy{}place.}
\PY{c+c1}{\PYZsh{}}
\PY{c+c1}{\PYZsh{} In general, it is useful to have nontrivial transformations which we}
\PY{c+c1}{\PYZsh{} put into Keras layers also available directly as @tf.function functions}
\PY{c+c1}{\PYZsh{} that do not depend on Keras. A typical design would then use one module}
\PY{c+c1}{\PYZsh{} with such @tf.function definitions that can be used independently, plus}
\PY{c+c1}{\PYZsh{} a Keras wrapper that defines a layer using these functions on top of that.}

\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{squash\PYZus{}evidence\PYZus{}asinh}\PY{p}{(}\PY{n}{t\PYZus{}evidence}\PY{p}{,} \PY{n}{t\PYZus{}param\PYZus{}a}\PY{p}{,} \PY{n}{t\PYZus{}param\PYZus{}b}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Squashes evidence `E` to `a * asinh(b * E)`.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{n}{t\PYZus{}param\PYZus{}a} \PY{o}{*} \PY{n}{tf}\PY{o}{.}\PY{n}{math}\PY{o}{.}\PY{n}{asinh}\PY{p}{(}\PY{n}{t\PYZus{}param\PYZus{}b} \PY{o}{*} \PY{n}{t\PYZus{}evidence}\PY{p}{)}


\PY{n+nd}{@tf}\PY{o}{.}\PY{n}{function}
\PY{k}{def} \PY{n+nf}{squash\PYZus{}evidence\PYZus{}atan}\PY{p}{(}\PY{n}{t\PYZus{}evidence}\PY{p}{,} \PY{n}{t\PYZus{}param\PYZus{}a}\PY{p}{,} \PY{n}{t\PYZus{}param\PYZus{}b}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Squashes evidence `E` to `a * atan(b * E)`.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{n}{t\PYZus{}param\PYZus{}a} \PY{o}{*} \PY{n}{tf}\PY{o}{.}\PY{n}{math}\PY{o}{.}\PY{n}{atan}\PY{p}{(}\PY{n}{t\PYZus{}param\PYZus{}b} \PY{o}{*} \PY{n}{t\PYZus{}evidence}\PY{p}{)}


\PY{k}{class} \PY{n+nc}{EvidenceTweakingLayer}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Layer for uniform nonlinear total\PYZhy{}evidence\PYZhy{}adjustment.}

\PY{l+s+sd}{  Based on the hypothesis that seeing large accumulated evidence may generally}
\PY{l+s+sd}{  have violated the Bayesian \PYZdq{}independence\PYZdq{} assumption beyond what we would}
\PY{l+s+sd}{  be comfortable with.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{c+c1}{\PYZsh{} Class attributes.}
  \PY{n}{\PYZus{}NONLINEARITY\PYZus{}BY\PYZus{}TAG} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{asinh}\PY{o}{=}\PY{n}{squash\PYZus{}evidence\PYZus{}asinh}\PY{p}{,}
                              \PY{n}{atan}\PY{o}{=}\PY{n}{squash\PYZus{}evidence\PYZus{}atan}\PY{p}{)}

  \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{o}{*}\PY{p}{,} \PY{n}{nonlinearity}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{asinh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initializes the instance.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} Forward parent\PYZhy{}class keyword args to parent\PYZhy{}class \PYZus{}\PYZus{}init\PYZus{}\PYZus{}, so that}
    \PY{c+c1}{\PYZsh{} `name=...` etc. args work as for a generic Keras `Layer`.}
    \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
    \PY{n}{nonlinearity\PYZus{}func} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}NONLINEARITY\PYZus{}BY\PYZus{}TAG}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{nonlinearity}\PY{p}{)}
    \PY{k}{if} \PY{n}{nonlinearity\PYZus{}func} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
      \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unknown nonlinearity: }\PY{l+s+si}{\PYZob{}}\PY{n}{nonlinearity}\PY{l+s+si}{!r\PYZcb{}}\PY{l+s+s1}{ \PYZhy{} }\PY{l+s+s1}{\PYZsq{}}
                       \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{known: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{set}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}NONLINEARITY\PYZus{}BY\PYZus{}TAG}\PY{p}{)}\PY{l+s+si}{!r\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} We need to store the \PYZus{}\PYZus{}init\PYZus{}\PYZus{}() parameters for .get\PYZus{}config(), so that}
    \PY{c+c1}{\PYZsh{} serialization and deserialization of the layer works.}
    \PY{n}{config} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{nonlinearity}\PY{o}{=}\PY{n}{nonlinearity}\PY{p}{)}
    \PY{n}{config}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}config} \PY{o}{=} \PY{n}{config}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}nonlinearity\PYZus{}func} \PY{o}{=} \PY{n}{nonlinearity\PYZus{}func}
    \PY{c+c1}{\PYZsh{} It is generally good practice to make sure that inspection of the}
    \PY{c+c1}{\PYZsh{} \PYZus{}\PYZus{}init\PYZus{}\PYZus{} method\PYZsq{}s body gives clarity about all (public and private)}
    \PY{c+c1}{\PYZsh{} instance attributes.}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}param\PYZus{}ab} \PY{o}{=} \PY{k+kc}{None}

  \PY{k}{def} \PY{n+nf}{build}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Sets up layer\PYZhy{}state.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{del} \PY{n}{input\PYZus{}shape}  \PY{c+c1}{\PYZsh{} Unused by this layer.}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}param\PYZus{}ab} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}weight}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)}\PY{p}{,}
                                     \PY{n}{initializer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}normal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

  \PY{k}{def} \PY{n+nf}{call}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Evaluates the layer.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} Note that `inputs` may in general have batch\PYZhy{}indices. The code in this}
    \PY{c+c1}{\PYZsh{} method needs to be able to handle data in such a form.}
    \PY{c+c1}{\PYZsh{} Here, the calculation is rather simple.}
    \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}nonlinearity\PYZus{}func}\PY{p}{(}\PY{n}{inputs}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}param\PYZus{}ab}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}param\PYZus{}ab}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

  \PY{k}{def} \PY{n+nf}{get\PYZus{}config}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the layer\PYZsq{}s configuration.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} This method is important to ensure that saving and loading a layer}
    \PY{c+c1}{\PYZsh{} (such as: as part of a trained model) can re\PYZhy{}create the given layer}
    \PY{c+c1}{\PYZsh{} in the expected form. Here, we need this since we have}
    \PY{c+c1}{\PYZsh{} instantiation\PYZhy{}time tweaking parameters.}
    \PY{c+c1}{\PYZsh{}}
    \PY{c+c1}{\PYZsh{} The default .from\PYZus{}config() classmethod works for our use case,}
    \PY{c+c1}{\PYZsh{} since all our config is JSON\PYZhy{}serializable.}
    \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}config}
\end{Verbatim}
\end{tcolorbox}

    Let us see if adding our new layer improves things.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cnn\PYZus{}model\PYZus{}tweaked} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}
        \PY{n}{EvidenceTweakingLayer}\PY{p}{(}\PY{n}{nonlinearity}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{asinh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{]}
\PY{p}{)}

\PY{n}{cnn\PYZus{}model\PYZus{}tweaked}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}\PY{p}{,}
    \PY{n}{loss}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{SparseCategoricalCrossentropy}\PY{p}{(}\PY{n}{from\PYZus{}logits}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{metrics}\PY{o}{.}\PY{n}{SparseCategoricalAccuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,}
\PY{p}{)}

\PY{n}{cnn\PYZus{}model\PYZus{}tweaked}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{n}{ds\PYZus{}train}\PY{p}{,}
    \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,}
    \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{ds\PYZus{}validation}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let us try saving and loading our model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cnn\PYZus{}model\PYZus{}tweaked}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnn\PYZus{}model\PYZus{}tweaked.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{reloaded\PYZus{}cnn\PYZus{}model\PYZus{}tweaked} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{load\PYZus{}model}\PY{p}{(}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnn\PYZus{}model\PYZus{}tweaked.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{custom\PYZus{}objects}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{EvidenceTweakingLayer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{EvidenceTweakingLayer}\PY{p}{\PYZcb{}}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test set accuracy (orig): }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{cnn\PYZus{}model\PYZus{}tweaked}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{ds\PYZus{}test}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{l+m+mi}{100}\PY{l+s+si}{:}\PY{l+s+s1}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test set accuracy (reloaded): }\PY{l+s+s1}{\PYZsq{}}
      \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{reloaded\PYZus{}cnn\PYZus{}model\PYZus{}tweaked}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{ds\PYZus{}test}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{l+m+mi}{100}\PY{l+s+si}{:}\PY{l+s+s1}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
313/313 [==============================] - 2s 7ms/step - loss: 1.1486 -
sparse\_categorical\_accuracy: 0.6017
Test set accuracy (orig): 60.17\%
313/313 [==============================] - 1s 3ms/step - loss: 1.1486 -
sparse\_categorical\_accuracy: 0.6017
Test set accuracy (reloaded): 60.17\%
    \end{Verbatim}

    Here, the classifier performance impact of this little idea was
unconvincing - but this is often how things turn out when exploring
ideas. Two things matter:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Having developed some useful intuition about what might work (via
  experimenting).
\item
  Being able to quickly try out ideas with little effort.
\end{enumerate}

    \hypertarget{tensorflow-and-jax}{%
\subsection{TensorFlow and JAX}\label{tensorflow-and-jax}}

TensorFlow is Google's ML flagship library. This however does not mean
that everybody at Google would do ML exclusively with TensorFlow.

One particularly interesting ``not officially supported,
currently-under-research'' tool which Google also open sourced is
\href{https://github.com/google/jax}{JAX}.

Tony Hoare is claimed to have said that ``inside every large program is
a small program struggling to get out'', and JAX can be thought of as
being such a ``small program'': On its github page, it describes itself
as ``Autograd and XLA'' (where
``\href{https://www.tensorflow.org/xla}{XLA}'' is the ``accelerated
linear algebra'' also powering TensorFlow). JAX looks a lot like ``numpy
with automatic differentiation capabilities''.

The Google colab kernel has JAX pre-installed, so let us explore it a
bit.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{jax}
\PY{k+kn}{from} \PY{n+nn}{jax} \PY{k+kn}{import} \PY{n}{numpy} \PY{k}{as} \PY{n}{jnp}


\PY{k}{def} \PY{n+nf}{box\PYZus{}volume}\PY{p}{(}\PY{n}{sides}\PY{p}{)}\PY{p}{:}
  \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{prod}\PY{p}{(}\PY{n}{sides}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{box\PYZus{}volume}\PY{p}{(}\PY{n}{jnp}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{jnp}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{n}{grad\PYZus{}vol} \PY{o}{=} \PY{n}{jax}\PY{o}{.}\PY{n}{jit}\PY{p}{(}\PY{n}{jax}\PY{o}{.}\PY{n}{grad}\PY{p}{(}\PY{n}{box\PYZus{}volume}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Grad(volume) at [2, 4, 8]:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
      \PY{n}{grad\PYZus{}vol}\PY{p}{(}\PY{n}{jnp}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{jnp}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
120.0
Grad(volume) at [2, 4, 8]: [32. 16.  8.]
    \end{Verbatim}

    While this certainly looks useful, please note the JAX documentation (at
the time of this writing) says:

\begin{verbatim}
This is a research project, not an official Google product.
Expect bugs and sharp edges.
Please help by trying it out, reporting bugs, and letting us know what you think!
\end{verbatim}

    \hypertarget{recap}{%
\section{Recap}\label{recap}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We looked into the structure of the Python language.

  \begin{itemize}
  \tightlist
  \item
    This was a lot of material.
  \item
    Further on, we used nested functions a lot, used numpy vectorization
    left and right, and having a good model of evaluation semantics was
    relevant for understanding how \texttt{@tf.function} sees code
    differently.
  \end{itemize}
\item
  We looked into the basic principles underlying fast gradients - and
  their use in numerical optimization.

  \begin{itemize}
  \tightlist
  \item
    This is the basis for many ML and ML-related frameworks (other than
    TensorFlow and JAX).
  \item
    This also explains the design limitations one would face when trying
    to implement similar such ML-supporting infrastructure on top of a
    framework such as Matlab, Octave, Mathematica, Maple, etc.
  \item
    ML frameworks have their limitations, such as having little reason
    to support better-than-float64 numerical accuracy. So, when we
    cannot use them, it is good to see how we can do this on our own.
    Also, a solid understanding of sensitivity backpropagation opens up
    new perspectives on other ideas, including Hamiltonian mechanics.
  \item
    We have seen that ``solving non-malicious 1000-parameter
    optimization problems numerically'' is generally ``easy'' these
    days.
  \end{itemize}
\item
  We formulated a first ML problem (``digit-8-recognizer'') purely as a
  high-dimensional optimization problem, and from there went on to ML.

  \begin{itemize}
  \tightlist
  \item
    Mental model of supervised ML: ``infinite-examples limit of
    k-Nearest-Neighbors classifiers''.
  \item
    Major concepts we encountered: ``Estimating gradients on batches''
    (``stochastic gradient descent''), some ``standard constructions''
    (loss functions, softmax, embeddings),
    generalization-performance-enhancing tricks (early stopping, L2
    regularization, dropout).
  \item
    We looked into (often entropy-based) explanations of the design of
    some standard constructions.
  \item
    We discussed what generally happens if inference is done on examples
    drawn on a different distribution than the one used to get the
    training set.
  \item
    We briefly discussed ``vanishing gradients'' and how information
    propagation in a DNN can be seen as a percolation problem (via MFA).
  \item
    We observed that TensorFlow can indeed make our life much easier as
    long as we are within the confines of what we can build with
    commonly used ``lego bricks''.
  \end{itemize}
\item
  We took a deeper look at TensorFlow

  \begin{itemize}
  \tightlist
  \item
    We explored a bit of the inner mechanics.
  \item
    We saw how to use it to conveniently formulate high dimensional
    optimization problems in a device-agnostic way (and do physics with
    that).
  \item
    We saw how to wrap up ``unusual'' computations to make them
    available as Keras layers.
  \end{itemize}
\end{enumerate}

{[}Author's note: Overall, I may occasionally have talked a bit of
nonsense, but this was less than 10\% of the time, and physics course
material rarely is right more than 90\% of the time anyhow. This was the
1st iteration of teaching this material. We might want to refine this
for future iterations.{]}

    \hypertarget{addendum-connecting-mathematica-and-tensorflow}{%
\subsection{Addendum: Connecting Mathematica and
TensorFlow}\label{addendum-connecting-mathematica-and-tensorflow}}

Given the large popularity of symbolic algebra packages such as in
particular Mathematica in theoretical physics, it makes sense to address
some obvious questions around ML and Mathematica.

As we have seen, the changes required to make a major programming
language support backpropagation go rather deep, and with both
TensorFlow and JAX, there are still some sharp edges that are related to
the attempt to blend object-language (for tensor-arithmetic computations
- in TensorFlow1, these enter the scene very explicitly as computational
graphs) and meta-language (for setting up and manipulating
object-language entities - so, Python). Design-wise, it is perhaps
debateable if this is the best possible design approach. While a clearer
separation between these roles would be conceptually more elegant, there
is also a large human element here. In any case, it is clear that
bringing backpropagation to any major language is most feasible if the
language is designed with simplicity and minimality in mind (such as the
Lisp dialect Scheme, where this has indeed been accomplished), or
started out with the design idea of properly supporting backpropagation.
Retrofitting this into some existent design of a large language is hard.
Also, compiling linear algebra to machine code that can execute fast on
a range of very different hardware architectures (CPUs and also GPUs)
requires major effort to build.

As such, it is not clear when - or even if at all - data manipulation
packages that are popular with physicists will implement advanced
numerical tensor backpropagation capabilities roughly on par with
TensorFlow or JAX. Clearly, major symbolic algebra packages these days
do support some form of ML - but typically not in the general-purpose
way that would allow us to design and shape rather freely what our
models look like. Still, one clearly would want to be able to
\emph{utilize} what such advanced capabilities have to offer in a
setting where one would not want the ML library to dictate the
programming language to use. This raises the question: if we might not
get backpropagation with fast compiled linear algebra in the near future
in popular symbolic algebra packages, is there perhaps at least a way to
utilize such functionality in such a way that we can still use these
packages as we are used to, but have them delegate parts of a problem to
some other code behind-the-scenes? So, to the user of a Mathematica
notebook, it looks as if there simply were a few functions to perform
specialized data analysis, but behind the scenes, mostly invisible to
the user, these functions exchange data with some other component
running TensorFlow code.

This is indeed feasible - and we will look in detail into one concrete
possible realization here. Before we do that, we should briefly ponder
the solution landscape. There are efforts to introduce file format
standards for serialized computation graphs, and modern versions of
Mathematica have experimental support for loading and then running
inference with models saved in the
\href{https://onnx.ai/about.html}{ONNX format} - we produced such a
serialized model earlier for classifying MNIST digits. Since this file
format is rather new and new versions are currently introduced in short
succession that expand the set of supported basic numerical operators,
it is quite possible that the dust has to settle a bit first before this
file format can be used widely without hassle. If this is available,
this might be the best option.

More generally, symbolic algebra packages, like just about every
programming language, usually come with a Foreign Function Interface
(FFI) that allows code authors to have the runtime kernel utilize other
libraries - perhaps call functions from C or Fortran libraries. In
principle, it might be possible to integrate TensorFlow(-Lite) into
Mathematica at this level. The advantage would be very efficient data
exchange between these components. However, given that both projects are
large and complex and have their own unique approaches to handling some
deep technical problems, it may well be that this causes major friction.
Another idea might be to go for less tight coupling and have Mathematica
exchange data with another process (perhaps on the same machine) that
runs TensorFlow. In Mathematica, connecting to an external process
either for one-off or session-based evaluation is supported via the
\href{https://reference.wolfram.com/language/guide/ExternalInterpretedLanguageInterfaces.html}{External
Interpreted Languages Interfaces}. Using this - or also a more
bare-bones approach such as one based on Mathematica's
\href{https://reference.wolfram.com/language/ref/RunThrough.html}{RunThrough}
command - would typically require first serializing data into some
textual form, and then communicating requests and responses
back-and-forth. Overall, this requires both data-conversion and
interprocess communication, so is perhaps less efficient and also more
brittle (with two running processes) than a more tightly integrated
solution, but also - in principle - would be much easier to build.

Since setting up a ML model is major computational effort that typically
then is amortized over all the queries to that model, it makes little
sense to use an approach where every query would start a new process and
set up the model from saved form first - we want to go with a background
server that loads the model at start time and then can be queried as
needed.

A general possible concern when starting processes from Mathematica,
such as via \texttt{RunThrough{[}{]}}, is that these processes generally
would inherit the Mathematica kernel's process environment, which may
well have adjustments to e.g.~\texttt{LD\_LIBRARY\_PATH} (on Unix
systems), making launched processes by default use a very different
collection of common libraries (such as for example \texttt{libz.so})
than the system-default - and this might clash with the needs libraries
needed by other components in such a set-up. Here, an extra step (such
as one more indirection) would likely be needed to switch over to using
a less customized library environment.

One basic fallback approach that should nowadays always be available,
irrespective of which symbolic algebra system (and version) one uses, is
to wire up a simple web server that allows querying ML models via HTTP
requests. This is spelled out in detail in the following - and may be a
useful basis for trying very similar approaches for other combinations
than Mathematica and TFLite.

    \hypertarget{having-mathematica-use-an-external-http-server-to-access-ml-capabilities}{%
\subsubsection{Having Mathematica use an external HTTP server to access
ML
capabilities}\label{having-mathematica-use-an-external-http-server-to-access-ml-capabilities}}

Let us look into a basic solution for having Mathematica handle ML tasks
via calling a dedicated function that delegates work to a webserver, via
a HTTP request. Here, we want to consider running the webserver and the
Mathematica process(es) on the same computer, which we assume to be a
Unix system - such as a researcher's personal workstation. We would
still want to have at least some protection against unrelated users
simply accessing the webserver and submitting their own requests without
any form of authorization. If the computer is the researcher's private
machine, no one else can log in to it, and also all programs running on
that computer can be trusted, a lightweight approach would be to simply
bind the webserver port only to the loopback interface - making it
unreachable from the wider network. If there are other
assumed-unprivileged users (i.e.~users who cannot read all files or
sniff network traffic), they still could in principle access the web
server socket (since web services in general cannot be run over Unix
domain sockets, only TCP sockets). Here, a very simple authentication
mechanism that is similar to what is used by the X Window System might
make sense. The idea is that every request needs to include a ``secret''
which can only be learned by accessing a file on the filesystem. This
way, filesystem-based access control can be transmuted into access
control for services offered by a server to which every user on the
local machine can connect. For X11, the \texttt{MIT-MAGIC-COOKIE-1}
mechanism shall be our guideline - whatever program can read the
\texttt{\$HOME/.XAuthority} file (or whatever the \texttt{XAUTHORITY}
environment variable has been set to) and extract the secret within can
access and interact with the X display of the logged in user.

We will do something very similar with our own secrets-file: We want our
webserver to write a secret to an agreed-upon location in the user's
home directory at start up time, and the Mathematica kernel (which needs
to be started afterwards) read that secret file and communicate the
secret alongside every HTTP request to our ``ML Server''. For actually
exchanging data, we cannot use the \texttt{HTTP\ GET} method for
multiple reasons, an important one being URL length restrictions.
Instead, we want to use \texttt{HTTP\ POST} requests that then transport
a payload. Let us implement a bare-bones server on the basis of Python's
\texttt{http} module. We want this server to - at start up time - load
one (or multiple) ML models and use them to process requests,
differentiating models by URL. The following code-cell is not intended
to be run in a colab notebook, but is a copy-pasteable complete
executable basic web server implemented in Python that can use
\texttt{tflite}. We want to serve the \texttt{MNIST} tflite model we
trained earlier on this course. The code comments indicate some little
tricks here and there that were required to make this work despite some
software packages having minor issues.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+ch}{\PYZsh{}!/usr/bin/env python3.10}

\PY{k+kn}{import} \PY{n+nn}{ast}
\PY{k+kn}{import} \PY{n+nn}{base64}
\PY{k+kn}{import} \PY{n+nn}{contextlib}
\PY{k+kn}{import} \PY{n+nn}{http}\PY{n+nn}{.}\PY{n+nn}{server}
\PY{k+kn}{import} \PY{n+nn}{threading}

\PY{k+kn}{import} \PY{n+nn}{io}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{import} \PY{n+nn}{sys}

\PY{c+c1}{\PYZsh{} Hack: at the time of this writing, tflite was only available for}
\PY{c+c1}{\PYZsh{} Python3.10, which however could not use Python3.11\PYZsq{}s numpy, so I did a:}
\PY{c+c1}{\PYZsh{} python3.10 \PYZhy{}m pip install numpy \PYZhy{}t \PYZti{}/TensorFlow\PYZhy{}ML/python3.10/site\PYZhy{}packages}
\PY{c+c1}{\PYZsh{} ...and we adjust the path here.}
\PY{c+c1}{\PYZsh{} Note: This is for demo purposes only, and an unsound technique.}
\PY{c+c1}{\PYZsh{} Done properly, this would use a Python \PYZdq{}virtual environment\PYZdq{} (venv).}
\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{insert}\PY{p}{(}
    \PY{l+m+mi}{0}\PY{p}{,}
    \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{getenv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HOME}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow\PYZhy{}ML/python3.10/site\PYZhy{}packages}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{numpy}
\PY{k+kn}{import} \PY{n+nn}{tflite\PYZus{}runtime}\PY{n+nn}{.}\PY{n+nn}{interpreter} \PY{k}{as} \PY{n+nn}{tflite}


\PY{k}{class} \PY{n+nc}{ServerError}\PY{p}{(}\PY{n+ne}{Exception}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Generic ML HTTP Server error.\PYZdq{}\PYZdq{}\PYZdq{}}


\PY{c+c1}{\PYZsh{} This is somewhat slow and perhaps overkill for simply\PYZhy{}structured data.}
\PY{k}{def} \PY{n+nf}{parse\PYZus{}mathematica}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Parse mathematica via sympy.parsing.\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{c+c1}{\PYZsh{} These imports only do heavy lifting upon first evaluation of the body;}
  \PY{c+c1}{\PYZsh{} subsequent evaluations (which however we do not have here) could}
  \PY{c+c1}{\PYZsh{} re\PYZhy{}use this. If we use the below \PYZdq{}fast\PYZdq{} alternative, this also}
  \PY{c+c1}{\PYZsh{} avoids the `sympy` dependency.}
  \PY{k+kn}{import} \PY{n+nn}{sympy}
  \PY{k+kn}{from} \PY{n+nn}{sympy}\PY{n+nn}{.}\PY{n+nn}{parsing} \PY{k+kn}{import} \PY{n}{mathematica}
  \PY{k}{return} \PY{n}{sympy}\PY{o}{.}\PY{n}{parsing}\PY{o}{.}\PY{n}{mathematica}\PY{o}{.}\PY{n}{parse\PYZus{}mathematica}\PY{p}{(}\PY{n}{data}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{parse\PYZus{}mathematica\PYZus{}simple\PYZus{}fast}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Ad\PYZhy{}hoc parse simple Mathematica data (fast, lightweight).\PYZdq{}\PYZdq{}\PYZdq{}}
  \PY{k}{return} \PY{n}{ast}\PY{o}{.}\PY{n}{literal\PYZus{}eval}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{translate}\PY{p}{(}\PY{n+nb}{str}\PY{o}{.}\PY{n}{maketrans}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{get\PYZus{}mnist\PYZus{}tflite\PYZus{}predictor}\PY{p}{(}\PY{n}{model\PYZus{}path}\PY{p}{)}\PY{p}{:}
  \PY{n}{interpreter} \PY{o}{=} \PY{n}{tflite}\PY{o}{.}\PY{n}{Interpreter}\PY{p}{(}\PY{n}{model\PYZus{}path}\PY{o}{=}\PY{n}{model\PYZus{}path}\PY{p}{)}
  \PY{n}{interpreter}\PY{o}{.}\PY{n}{allocate\PYZus{}tensors}\PY{p}{(}\PY{p}{)}
  \PY{n}{input\PYZus{}details} \PY{o}{=} \PY{n}{interpreter}\PY{o}{.}\PY{n}{get\PYZus{}input\PYZus{}details}\PY{p}{(}\PY{p}{)}
  \PY{n}{output\PYZus{}details} \PY{o}{=} \PY{n}{interpreter}\PY{o}{.}\PY{n}{get\PYZus{}output\PYZus{}details}\PY{p}{(}\PY{p}{)}
  \PY{n}{interpreter\PYZus{}lock} \PY{o}{=} \PY{n}{threading}\PY{o}{.}\PY{n}{Lock}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{def} \PY{n+nf}{fn\PYZus{}predict}\PY{p}{(}\PY{n}{in\PYZus{}data}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Note that this is setting interpreter\PYZhy{}state, making}
    \PY{c+c1}{\PYZsh{} this function non\PYZhy{}reentrant unless we ensure that}
    \PY{c+c1}{\PYZsh{} interpreter.invoke() calls cannot get mangled by}
    \PY{c+c1}{\PYZsh{} interwoven concurrent set\PYZhy{}up / read\PYZhy{}out operations.}
    \PY{c+c1}{\PYZsh{} This would matter a lot if we were to use e.g.}
    \PY{c+c1}{\PYZsh{} a http.server.ThreadingHTTPServer \PYZhy{} but let\PYZsq{}s make this}
    \PY{c+c1}{\PYZsh{} robust.}
    \PY{k}{try}\PY{p}{:}
      \PY{n}{interpreter\PYZus{}lock}\PY{o}{.}\PY{n}{acquire}\PY{p}{(}\PY{p}{)}
      \PY{c+c1}{\PYZsh{} Here, we are extra\PYZhy{}permissive:}
      \PY{c+c1}{\PYZsh{} Any numerical data matrix/vector that comes in gets zero\PYZhy{}padded}
      \PY{c+c1}{\PYZsh{} to at least 28x28 elements, then trimmed to 28x28 elements,}
      \PY{c+c1}{\PYZsh{} then reshaped. This allows us to easily hand\PYZhy{}feed data such as}
      \PY{c+c1}{\PYZsh{} \PYZob{}1,2,3\PYZcb{} for debugging. No harm in trying to predict}
      \PY{c+c1}{\PYZsh{} from bad\PYZhy{}size data here.}
      \PY{n}{in\PYZus{}array} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{pad}\PY{p}{(}
          \PY{n}{numpy}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{in\PYZus{}data}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{numpy}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,}
          \PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{28}\PY{o}{*}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{28}\PY{o}{*}\PY{l+m+mi}{28}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}
      \PY{n}{interpreter}\PY{o}{.}\PY{n}{set\PYZus{}tensor}\PY{p}{(}
          \PY{n}{input\PYZus{}details}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{in\PYZus{}array}\PY{p}{)}
      \PY{n}{interpreter}\PY{o}{.}\PY{n}{invoke}\PY{p}{(}\PY{p}{)}
      \PY{n}{output\PYZus{}data} \PY{o}{=} \PY{n}{interpreter}\PY{o}{.}\PY{n}{get\PYZus{}tensor}\PY{p}{(}\PY{n}{output\PYZus{}details}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
      \PY{k}{return} \PY{n}{output\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
    \PY{k}{finally}\PY{p}{:}
      \PY{n}{interpreter\PYZus{}lock}\PY{o}{.}\PY{n}{release}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}}
  \PY{k}{return} \PY{n}{fn\PYZus{}predict}


\PY{k}{class} \PY{n+nc}{ML\PYZus{}HTTPServer}\PY{p}{(}\PY{n}{http}\PY{o}{.}\PY{n}{server}\PY{o}{.}\PY{n}{HTTPServer}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Server subclass that has access to a ML model.}

\PY{l+s+sd}{  Attributes:}
\PY{l+s+sd}{    (base class attributes plus...):}
\PY{l+s+sd}{    ml\PYZus{}fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath: Mapping of url\PYZhy{}path to predictor\PYZhy{}function,}
\PY{l+s+sd}{      as documented in `\PYZus{}\PYZus{}init\PYZus{}\PYZus{}`.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{server\PYZus{}address}\PY{p}{,} \PY{n}{request\PYZus{}handler\PYZus{}class}\PY{p}{,} \PY{o}{*}\PY{p}{,}
               \PY{n}{fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath}\PY{o}{=}\PY{p}{(}\PY{p}{)}\PY{p}{,}
               \PY{n}{token\PYZus{}file}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.ml\PYZus{}server\PYZus{}token}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initializes the instance.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      server\PYZus{}address: The server address, passed on to base class `\PYZus{}\PYZus{}init\PYZus{}\PYZus{}`.}
\PY{l+s+sd}{      request\PYZus{}handler\PYZus{}class: The request handler class, passed on to}
\PY{l+s+sd}{        base class `\PYZus{}\PYZus{}init\PYZus{}\PYZus{}`.}
\PY{l+s+sd}{      fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath: data which when passed to `dict()` produces}
\PY{l+s+sd}{        a key\PYZhy{}value dictionary that maps a URL\PYZhy{}path key to a predictor\PYZhy{}function;}
\PY{l+s+sd}{        Each predictor function is expected to map a single numpy.ndarray\PYZhy{}like}
\PY{l+s+sd}{        ML\PYZhy{}data argument to numerical output data.}
\PY{l+s+sd}{      token\PYZus{}file: Path (implicitly\PYZhy{}relative to web\PYZhy{}user\PYZsq{}s \PYZdl{}HOME env\PYZhy{}var)}
\PY{l+s+sd}{        to a file where upon webserver startup the server writes a random}
\PY{l+s+sd}{        secret. POST web requests must include the secret as the 1st line of}
\PY{l+s+sd}{        the payload, proving that the requestor could read the token\PYZhy{}file.}
\PY{l+s+sd}{        This loosely resembles MIT\PYZhy{}MAGIC\PYZhy{}COOKIE\PYZhy{}1 X Window Authorization and}
\PY{l+s+sd}{        protects against independent users on the same machine (who see the}
\PY{l+s+sd}{        TCP port bound to the loopback interface) connecting to the webserver}
\PY{l+s+sd}{        and issuing their own requests. Does not protect against local}
\PY{l+s+sd}{        users with elevated privileges, such as root access to files or}
\PY{l+s+sd}{        packet sniffing. If web requests are to be routed from a different}
\PY{l+s+sd}{        machine, TLS encryption should be used additionally.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} Secret handling goes first. Overall, this is a primitive mechanism}
    \PY{c+c1}{\PYZsh{} that one might want to replace with something more advanced,}
    \PY{c+c1}{\PYZsh{} such as HMAC, but the client also needs to support this.}
    \PY{n}{secret} \PY{o}{=} \PY{n}{base64}\PY{o}{.}\PY{n}{b64encode}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{getrandom}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{rstrip}\PY{p}{(}\PY{l+s+sa}{b}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{token\PYZus{}path} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{getenv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HOME}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{token\PYZus{}file}\PY{p}{)}
    \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{token\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h\PYZus{}token}\PY{p}{:}
      \PY{n}{os}\PY{o}{.}\PY{n}{fchmod}\PY{p}{(}\PY{n}{h\PYZus{}token}\PY{o}{.}\PY{n}{fileno}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mo}{0o700}\PY{p}{)}
      \PY{n}{h\PYZus{}token}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{secret}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Check if writing the secret succeeded.}
    \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{token\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{h\PYZus{}token\PYZus{}re}\PY{p}{:}
      \PY{n}{re\PYZus{}secret} \PY{o}{=} \PY{n}{h\PYZus{}token\PYZus{}re}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}
    \PY{k}{if} \PY{o+ow}{not} \PY{n}{re\PYZus{}secret} \PY{o}{==} \PY{n}{secret}\PY{p}{:}
      \PY{k}{raise} \PY{n}{ServerError}\PY{p}{(}
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Could not align on\PYZhy{}filesystem secret }\PY{l+s+s1}{\PYZsq{}}
          \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{with internal secret \PYZhy{} path: }\PY{l+s+si}{\PYZob{}}\PY{n}{token\PYZus{}path}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Initialize base class instance. We have not yet touched instance\PYZhy{}state.}
    \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{server\PYZus{}address}\PY{p}{,} \PY{n}{request\PYZus{}handler\PYZus{}class}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{ml\PYZus{}fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}secret} \PY{o}{=} \PY{n}{secret}

  \PY{k}{def} \PY{n+nf}{check\PYZus{}secret}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{user\PYZus{}provided}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Checks if the user\PYZhy{}provided string equals the secret.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}secret} \PY{o}{==} \PY{n}{user\PYZus{}provided}


\PY{k}{class} \PY{n+nc}{ML\PYZus{}HTTPRequestHandler}\PY{p}{(}\PY{n}{http}\PY{o}{.}\PY{n}{server}\PY{o}{.}\PY{n}{BaseHTTPRequestHandler}\PY{p}{)}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}HTTP Request handler that delegates POST to specialist functions.}

\PY{l+s+sd}{  Also performs secret\PYZhy{}checking on the server.}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{k}{def} \PY{n+nf}{\PYZus{}send\PYZus{}text\PYZus{}plain\PYZus{}response}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{status\PYZus{}code}\PY{p}{,} \PY{n}{response}\PY{p}{)}\PY{p}{:}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{send\PYZus{}response}\PY{p}{(}\PY{n}{status\PYZus{}code}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{send\PYZus{}header}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Content\PYZhy{}Type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text/plain}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{send\PYZus{}header}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Content\PYZhy{}Length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{response}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{end\PYZus{}headers}\PY{p}{(}\PY{p}{)}
    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{wfile}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{response}\PY{p}{)}

  \PY{k}{def} \PY{n+nf}{do\PYZus{}POST}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
    \PY{n}{req\PYZus{}urlpath} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{path}
    \PY{n}{content\PYZus{}length} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{headers}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{content\PYZhy{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
    \PY{n}{request\PYZus{}data} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rfile}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{n}{content\PYZus{}length}\PY{p}{)}
    \PY{k}{try}\PY{p}{:}
      \PY{c+c1}{\PYZsh{} If any of these operations fail, such as due to `req\PYZus{}urlpath`}
      \PY{c+c1}{\PYZsh{} not being in the mapping, the payload not having a b\PYZsq{}\PYZbs{}n\PYZsq{}, etc.,}
      \PY{c+c1}{\PYZsh{} the violation of the implicit code\PYZhy{}expectation raises an exception,}
      \PY{c+c1}{\PYZsh{} and the `except`\PYZhy{}section just returns a `Not Implemented` response.}
      \PY{n}{user\PYZus{}provided\PYZus{}secret}\PY{p}{,} \PY{n}{payload} \PY{o}{=} \PY{n}{request\PYZus{}data}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+sa}{b}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
      \PY{k}{if} \PY{o+ow}{not} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{server}\PY{o}{.}\PY{n}{check\PYZus{}secret}\PY{p}{(}\PY{n}{user\PYZus{}provided\PYZus{}secret}\PY{p}{)}\PY{p}{:}
          \PY{c+c1}{\PYZsh{} Handled right below.}
          \PY{c+c1}{\PYZsh{} We provide a message since we stderr\PYZhy{}output text.}
          \PY{k}{raise} \PY{n}{ServerError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bad secret.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
      \PY{n}{parsed\PYZus{}payload} \PY{o}{=} \PY{n}{parse\PYZus{}mathematica\PYZus{}simple\PYZus{}fast}\PY{p}{(}\PY{n}{payload}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{utf\PYZhy{}8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
      \PY{n}{fn\PYZus{}predict} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{server}\PY{o}{.}\PY{n}{ml\PYZus{}fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath}\PY{p}{[}\PY{n}{req\PYZus{}urlpath}\PY{p}{]}
      \PY{n}{prediction} \PY{o}{=} \PY{n}{fn\PYZus{}predict}\PY{p}{(}\PY{n}{parsed\PYZus{}payload}\PY{p}{)}
      \PY{n}{response} \PY{o}{=} \PY{n+nb}{repr}\PY{p}{(}\PY{n}{prediction}\PY{p}{)}\PY{o}{.}\PY{n}{translate}\PY{p}{(}
          \PY{n+nb}{str}\PY{o}{.}\PY{n}{maketrans}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{utf\PYZhy{}8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}send\PYZus{}text\PYZus{}plain\PYZus{}response}\PY{p}{(}\PY{n}{http}\PY{o}{.}\PY{n}{HTTPStatus}\PY{o}{.}\PY{n}{OK}\PY{p}{,} \PY{n}{response}\PY{p}{)}
    \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{exn}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ERROR:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{repr}\PY{p}{(}\PY{n}{exn}\PY{p}{)}\PY{p}{,} \PY{n}{file}\PY{o}{=}\PY{n}{sys}\PY{o}{.}\PY{n}{stderr}\PY{p}{)}
      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}send\PYZus{}text\PYZus{}plain\PYZus{}response}\PY{p}{(}\PY{n}{http}\PY{o}{.}\PY{n}{HTTPStatus}\PY{o}{.}\PY{n}{NOT\PYZus{}IMPLEMENTED}\PY{p}{,}
                                     \PY{l+s+sa}{b}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{501 \PYZhy{} Not Implemented}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{run\PYZus{}server}\PY{p}{(}\PY{n}{server\PYZus{}address}\PY{o}{=}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{localhost}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{8000}\PY{p}{)}\PY{p}{,}
               \PY{n}{mnist\PYZus{}model}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.tflite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
  \PY{n}{fn\PYZus{}predict\PYZus{}mnist} \PY{o}{=} \PY{n}{get\PYZus{}mnist\PYZus{}tflite\PYZus{}predictor}\PY{p}{(}\PY{n}{mnist\PYZus{}model}\PY{p}{)}
  \PY{n}{httpd} \PY{o}{=} \PY{n}{ML\PYZus{}HTTPServer}\PY{p}{(}\PY{n}{server\PYZus{}address}\PY{p}{,}
                        \PY{n}{ML\PYZus{}HTTPRequestHandler}\PY{p}{,}
                        \PY{n}{fn\PYZus{}predict\PYZus{}by\PYZus{}urlpath}\PY{o}{=}\PY{p}{\PYZob{}}
                            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/mnist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{fn\PYZus{}predict\PYZus{}mnist}\PY{p}{,}
                            \PY{p}{\PYZcb{}}\PY{p}{)}
  \PY{n}{httpd}\PY{o}{.}\PY{n}{serve\PYZus{}forever}\PY{p}{(}\PY{p}{)}


\PY{k}{if} \PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}\PYZus{}main\PYZus{}\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
  \PY{n}{run\PYZus{}server}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    If we start this webserver, the following Mathematica code illustrates
how to then wrap up delegation of a ML inference task to this external
server.

\begin{verbatim}
(* Server URL Authentication Secret *)
mlServerURL := "http://localhost:8000/mnist";
mlServerTokenPath:=FileNameJoin[{$HomeDirectory,".ml_server_token"}];
mlServerAuthSecret = Import[mlServerTokenPath, "Text"];
mlServerAuthSecretLength := {"Secret Length", StringLength[mlServerAuthSecret]};
Print[mlServerAuthSecretLength];


(* Running external classification *)
TFClassifyMNIST[numdata_]:=Module[{body, result},
body=mlServerAuthSecret<>"\n"<>ExportString[numdata,"String"];
result=URLFetch[mlServerURL,"Method"->"POST","Body"->body];
result]


(* Demo - Example Input *)

(* For illustration: Create 28x28 input from 7x7 text. *)

as28x28[textImage7x7_]:=KroneckerProduct[
ToExpression[#/.{"#"->1.0,"."->0.0}]&/@Characters[textImage7x7],
ConstantArray[1, {4,4}]]

demoDigit := as28x28[{
".......",
"..####.",
"..#..#.",
"..####.",
"..#..#.",
"..####.",
"......."}]

(* Running Classification *)

MLResponse:=TFClassifyMNIST[demoDigit];
Print[MLResponse]
(* Produces:
{-0.12768815457820892, -3.9151949882507324,
 -1.174180269241333, -2.3871653079986572,
 0.34610337018966675, 3.6387715339660645,
 2.232551097869873, -4.550597667694092,
 4.761049270629883, 0.7428076267242432}
*)

\end{verbatim}

    This basic template can (perhaps with a bit of help from a Unix wizard)
be adjusted to other, similar tasks.


    % Add a bibliography block to the postdoc
    
    
    
\begin{thebibliography}{10}

\bibitem{brock2016neural}
Andrew Brock, Theodore Lim, James~M Ritchie, and Nick Weston.
\newblock Neural photo editing with introspective adversarial networks.
\newblock {\em arXiv preprint arXiv:1609.07093}, 2016.

\bibitem{fischbacher2009many}
Thomas Fischbacher.
\newblock The many vacua of gauged extended supergravities.
\newblock {\em General Relativity and Gravitation}, 41(2):315--411, 2009.

\bibitem{fischbacher2020intelligent}
Thomas Fischbacher, Iulia~M Comsa, Krzysztof Potempa, Moritz Firsching, Luca
  Versari, and Jyrki Alakuijala.
\newblock Intelligent matrix exponentiation.
\newblock {\em arXiv preprint arXiv:2008.03936}, 2020.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{goguen1996algebraic}
Joseph Goguen and Grant Malcolm.
\newblock {\em Algebraic semantics of imperative programs}.
\newblock MIT press, 1996.

\bibitem{goh2021multimodal}
Gabriel Goh, Nick~Cammarata †, Chelsea~Voss †, Shan Carter, Michael Petrov,
  Ludwig Schubert, Alec Radford, and Chris Olah.
\newblock Multimodal neurons in artificial neural networks.
\newblock {\em Distill}, 2021.
\newblock https://distill.pub/2021/multimodal-neurons.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{graham2002plan}
Paul Graham.
\newblock A plan for spam.
\newblock {\em http://paulgraham. com/spam. html}, 2002.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International conference on machine learning}, pages
  1321--1330. PMLR, 2017.

\bibitem{hascoet2013tapenade}
Laurent Hascoet and Val{\'e}rie Pascual.
\newblock The tapenade automatic differentiation tool: principles, model, and
  specification.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)}, 39(3):1--43,
  2013.

\bibitem{klambauer2017self}
G{\"u}nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
\newblock Self-normalizing neural networks.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{mallick2020can}
Ankur Mallick, Chaitanya Dwivedi, Bhavya Kailkhura, Gauri Joshi, and T~Yong-Jin
  Han.
\newblock Can your ai differentiate cats from covid-19? sample efficient
  uncertainty estimation for deep learning safety.
\newblock {\em choice}, 50(6), 2020.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533--536, 1986.

\bibitem{schoenholz2016deep}
Samuel~S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock {\em arXiv preprint arXiv:1611.01232}, 2016.

\bibitem{speelpenning1980compiling}
Bert Speelpenning.
\newblock {\em Compiling fast partial derivatives of functions given by
  algorithms}.
\newblock University of Illinois at Urbana-Champaign, 1980.

\bibitem{turing2004intelligent}
Alan Turing.
\newblock Intelligent machinery (1948).
\newblock {\em B. Jack Copeland}, page 395, 2004.

\end{thebibliography}
\end{document}
